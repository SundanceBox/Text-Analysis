{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram Twitter Wordcloud\n",
    "This notebook takes you through the steps to create a wordcloud of ngrams (unigrams, bigrams, or trigrams) from tweets in either .csv or .json format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "Before we start, you will need to have set up a [Carbonate account](https://kb.iu.edu/d/aolp) in order to access [Research Desktop (ReD)](https://kb.iu.edu/d/apum). You will also need to have access to ReD through the [thinlinc client](https://kb.iu.edu/d/aput). If you have not done any of this, or have only done some of this, but not all, you should go to our [textPrep-Py.ipynb](https://github.com/cyberdh/Text-Analysis/blob/master/textPrep-Py.ipynb) before you proceed further. The textPrep-Py notebook provides information and resources on how to get a Carbonate account, how to set up ReD, and how to get started using the Jupyter Notebook on ReD.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CyberDH environment\n",
    "The code in the cell below points to a Python environment specifically for use with the Python Jupyter Notebooks created by Cyberinfrastructure for Digital Humanities. It allows for the use of the different packages in our notebooks and their subsequent data sets.\n",
    "\n",
    "##### Packages\n",
    "- **sys:** Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "#### NOTE: This cell is only for use with Research Desktop. You will get an error if you try to run this cell on your personal device!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,\"/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages\")\n",
    "os.environ[\"NLTK_DATA\"] = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary packages for notebook \n",
    "\n",
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others created by Python users are available for download.\n",
    "\n",
    "In your terminal, packages can be installed by simply typing `pip install nameofpackage --user`. However, since you are using ReD and our Python environment, you will not need to install any of the packages below to use this notebook. Anytime you need to make use of a package, however, you need to import it so that Python knows to look in these packages for any functions or commands you use. Below is a brief description of the packages we are using in this notebook:   \n",
    "\n",
    "- **textblob:** Library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more. \n",
    "\n",
    "- **nltk:** Platform for building Python programs to work with human language data.\n",
    "\n",
    "- **re:** Provides regular expression matching operations similar to those found in Perl.\n",
    "\n",
    "- **string:** contains a number of useful constants and classes, as well as some deprecated legacy functions that are also available as methods on strings.\n",
    "\n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "- **wordcloud:** A simple wordcloud generator in Python.\n",
    "\n",
    "- **glob:** Finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order.\n",
    "\n",
    "- **zipfile:** Allows for handling of zipfiles.\n",
    "\n",
    "- **matplotlib:** A Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n",
    "\n",
    "Notice we import some of the packages differently. In some cases we just import the entire package when we say `import XYZ`. For some packages which are small, or, from which we are going to use a lot of the functionality it provides, this is fine. \n",
    "\n",
    "Sometimes when we import the package directly we say `import XYZ as X`. All this does is allow us to type `X` instead of `XYZ` when we use certain functions from the package. So we can now say `X.function()` instead of `XYZ.function()`. This saves time typing and eliminates errors from having to type out longer package names. I could just as easily type `import XYZ as potato` and whenever I use a function from the `XYZ` package I would need to type `potato.function()`. What we import the package as is up to you, but some commonly used packages have abbreviations that are standard amongst Python users such as `import pandas as pd` or `import matplotlib.pyplot as plt`. You do not need to us `pd` or `plt`, however, these are widely used and using something else could confuse other users and is generally considered bad practice. \n",
    "\n",
    "Other times we import only specific elements or functions from a package. This is common with packages that are very large and provide a lot of functionality, but from which we are only using a couple functions or a specific subset of the package that contains the functionality we need. This is seen when we say `from XYZ import ABC`. This is saying I only want the `ABC` function from the `XYZ` package. Sometimes we need to point to the specific location where a function is located within the package. We do this by adding periods in between the directory names, so it would look like `from XYZ.123.A1B2 import LMN`. This says we want the `LMN` function which is located in the `XYZ` package and then the `123` and `A1B2` directory in that package. \n",
    "\n",
    "You can also import more than one function from a package by separating the functions with commas like this `from XYZ import ABC, LMN, QRS`. This imports the `ABC`, `LMN` and `QRS` functions from the `XYZ` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import wordcloud\n",
    "import glob\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set needed variables\n",
    "This is where you will make some decisions about your data and set the necessary variables. We are doing this so you will not need to make changes in the code further down.\n",
    "\n",
    "First, we need to decide if we want our code to read all the files in a directory or just a single file. If we want all the files in a directory then we set `source` equal to `\"*\"`. This means 'all' and will be added to the file type later in the code. If you want a single file change `\"*\"` to the file name without the \".csv\" or \".json\" at the end. So if you have a file named \"myFile.csv\" you would set `source` equal to `\"myFile\"` without the \".csv\".\n",
    "\n",
    "Next we assign the file type our data comes in to a variable. At the moment the only options are \".csv\" or \".json\" as these are the most popular twitter data formats. We assign the format to the `fileType` variable. It should look like this: `fileType = \".json\"`.\n",
    "\n",
    "Your data can be contained in one file or many. Therefore, you need to state if you want to read in a single document or an entire directory. If you want to read in a single document, then set `singleDoc` equal to **True**. If you want to read in an entire directory of documents then set `singleDoc` equal to **False**.\n",
    "\n",
    "The `nltkStop` is where you determine if you want to use the built in stopword list provided by the NLTK package. They provide stopword lists in multiple languages. If you wish to use this then set `nltkStop` equal to **True**. If you do not, then set `nltkStop` equal to **False**.\n",
    "\n",
    "The `customStop` variable is for if you have a dataset that contains additional stopwords that you would like to read in and have added to the existing `stopWords` list. You do **NOT** need to use the NLTK stopwords list in order to add your own custom list of stopwords. **NOTE: Your custom stopwords file needs to have one word per line as it reads in a line at a time and the full contents of the line is read in and added to the existing stopwords list.** If you have a list of your own then set `customStop` equal to **True**. If you do not have your own custom stopwords list then set `customStop` equal to **False**.\n",
    "\n",
    "The `ng` variable (short for ngram) is where you determine if you want a unigram (basically a word frequency count), a bigram (most common 2 word pairs), or a trigram (most common three word phrases). This will be used in the code further down.\n",
    "\n",
    "The `stopLang` variable is to choose the language of the nltk stopword list you wish to use. It is currently set to `stopwords.fileids()` which will use the stopwords list for every available language from the nltk package. This was done because twitter data often contains multiple languages. If you need a single language, simply change `stopwords.fileids()` to `\"english\"` or the anglicized name of the language you wish to use (e.g. \"spanish\" instead of \"espanol\" or \"german\" instead of \"deutsch\").\n",
    "\n",
    "The variable `encoding` is where you determine what type encoding to use (ascii, ISO-8850-1, utf-8, etc...). We have it set to utf-8 at the moment as we have found it is less likely to have any problems. However, errors do occur, but the encoding errors rarely impact our results and it causes the Python code to exit. So instead of dealing with unhelpful errors we ignore the ones dealing with encoding by assigning `'ignore'` to the `errors` variable. If you want to see any encoding errors then change `'ignore'` to `None` without the quotes.\n",
    "\n",
    "The `stopWords = []` is an empty list that will contain the final list of stop words to be removed form your dataset. What ends up in the list depends on whether you set `nltkStop` and/or `customStop` equal to **True** or **False** and if you add any additional words to the list.\n",
    "\n",
    "The `cleanText` variable is another empty list. This is where your document(s) will reside later.\n",
    "\n",
    "The `ngramList` variable is another empty list and is where your resulting ngrams will reside. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = \"coronaVirus01-21Jan2020\"\n",
    "fileType = \".json\"\n",
    "singleDoc = True\n",
    "nltkStop = True\n",
    "customStop = True\n",
    "ng = 2\n",
    "textColIndex = \"text\"\n",
    "stopLang = stopwords.fileids()\n",
    "encoding = \"utf-8\"\n",
    "errors = \"ignore\"\n",
    "stopWords = []\n",
    "cleanText = []\n",
    "ngramList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File paths\n",
    "Here we are saving different file paths as variables that we need in our code. We again do this so you will not need to make as many changes to the code later. \n",
    "\n",
    "First we use the `os` package above to find our `['HOME']` directory using the `environ` function. This will work for any operating system, so if you decide to try this out on your personal computer instead of ReD, the `homePath` variable will still be the path to your 'home' directory, so no changes are needed.\n",
    "\n",
    "Next, we combine the `homePath` file path with the folder names that lead to where our data is stored. Note that we do not use any file names yet, just the path to the folder. This is because we may want to read in all the files in the directory, or just one, or we may need to access a file in this directory, but need to navigate to another folder to access other data. There are options below for doing both. We save the path as a variable named `dataHome`.\n",
    "\n",
    "Now we add the `homePath` file path to other folder names that lead to a folder where we will want to save any output generated by this code. We again will change the file names for the output in the appropriate cells down below. We save this file path as the variable `dataResults`.\n",
    "\n",
    "Finally, we use an 'if...else' statement to determine what file path we assign to the variable `dataRoot`. If we chose \".csv\" for our `fileType` above then `dataRoot` points to where our \".csv\" data is stored. If we chose \".json\" for `fileType`, then it points to where our \".json\" data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homePath = os.environ['HOME']\n",
    "dataHome = os.path.join(homePath, \"Text-Analysis-master\", \"data\")\n",
    "dataResults = os.path.join(homePath, \"Text-Analysis-master\", \"Output\")\n",
    "if fileType == \".csv\":\n",
    "    dataRoot = os.path.join(dataHome, \"twitter\", \"CSV\")\n",
    "else:\n",
    "    dataRoot = os.path.join(dataHome, \"twitter\", \"JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "If you set `nltkStop` equal to **True** above then this will add the nltk stopwords list to the empty list named `stopWords`. You should already have chosen your language above, so there is no need to do that here.\n",
    "\n",
    "If you wish to add additional words to the `stopWords` list, add the word in quotes to the list in `stopWords.extend(['the', 'words', 'you', 'want', 'to', 'add'])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "if nltkStop is True:\n",
    "    stopWords.extend(stopwords.words(stopLang))\n",
    "    \n",
    "    stopWords.extend(['pic', 'com', 'coronavirus', 'twitter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add own stopword list\n",
    "\n",
    "Here is where your own stopwords list is added if you selected **True** in \"customStop\" above. Here you will need to change the folder names and file name to match your folders and file. Remember to put each folder name in quotes and in the correct path order, always putting the file name including the file extension ('.txt') last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if customStop is True:\n",
    "    stopWordsFilepath = os.path.join(dataHome, \"twitterStopword.txt\")\n",
    "\n",
    "    with open(stopWordsFilepath, \"r\",encoding = encoding) as stopfile:\n",
    "        stopWordsCustom = [x.strip() for x in stopfile.readlines()]\n",
    "\n",
    "    stopWords.extend(stopWordsCustom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "We need to create a few functions in order to calculate and create a wordcloud. Any time you see `def` that means we are *DE*claring a *F*unction. The `def` is usually followed by the name of the function being created and then in parentheses are the parameters the function requires. After the parentheses is a colon, which closes the declaration, then a bunch of code below which is indented. The indented code is the program statement or statements to be executed. Once you have created your function all you need to do in order to run it is call the function by name and make sure you have included all the required parameters in the parentheses. This allows you to do what the function does without having to write out all the code in the function every time you wish to perform that task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Cleaning\n",
    "This function does some text cleaning for us and requires the parameter: text (as in what text are you cleaning).\n",
    "\n",
    "Now we come to the statements to be executed. First we lowercase the text or else 'Love' and 'love' will be counted as two different words, so we make them all 'love'. Next we remove URLs by removing any text that starts with 'http' and ending with a space. Then we split the text into individual words. Next we remove any empty space, digits, stopwords, and punctuation. Finally, we return a list of cleaned words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textClean(text):\n",
    "    \n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    tweets = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    tokens = re.split(r'\\W+', tweets )\n",
    "    \n",
    "    # remove empty string\n",
    "    tokens = [t for t in tokens if t]\n",
    "    \n",
    "    # remove digits\n",
    "    tokens = [t for t in tokens if not t.isdigit()]\n",
    "    \n",
    "    # built-in stop words list\n",
    "    tokens = [t for t in tokens if t not in stopWords]\n",
    "        \n",
    "    # remove punctuation\n",
    "    puncts = list(string.punctuation)\n",
    "    puncts.append('--')\n",
    "\n",
    "    tokens = [t for t in tokens if t not in puncts]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Unzip files\n",
    "\n",
    "Here we are unzipping files. Since twitter data can be rather large it is often necessary to compress it into a '.zip' file in order to upload it to places such as GitHub. For this reason, we have setup some code to go in and automatically extract all the items in a compressed '.zip' file so you don't have to and so you don't get errors later. If the data is not in a '.zip' file there is no need to worry, it will not give an error if there are no files ending in '.zip' in your directory.\n",
    "\n",
    "You should not need to make any changes as we use the same variables containing our file paths as above, so if you need to make adjustments to the file paths, you need to make them there, specifically to the `dataRoot` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "direct = dataRoot\n",
    "allZipFiles = glob.glob(os.path.join(dataRoot, \"*.zip\"))\n",
    "for item in allZipFiles:\n",
    "    fileName = os.path.splitext(direct)[0]\n",
    "    zipRef = zipfile.ZipFile(item, \"r\")\n",
    "    zipRef.extractall(fileName)\n",
    "    zipRef.close()\n",
    "    os.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in files\n",
    "\n",
    "If you chose `\".csv\"` as your `fileType` up above, then the first `if` statement in the code below reads in \".csv\" files and saves the contents to a dataframe using the Pandas package. It will read in either an entire directory or a single \".csv\" file depending on what you chose for `source` above. \n",
    "\n",
    "Once we have read in the \".csv\" file using the Pandas `read_csv` function, we need to concatenate the \".csv\" files if there are multiple. Because of this it is important that your \".csv\" files have an identical column count and each column has identical header names or you will get errors. If you have a single \".csv\" file then you should be fine for this step. We assign this process to the variable `cdf` so we can use it later.\n",
    "\n",
    "Now we convert our `cdf` to a pandas dataframe. This allows for easier manipulation of the data in the next step.\n",
    "\n",
    "Finally, we pull in the column containing the data we are interested in which we assigned to the variable `textColIndex` earlier and turn it into a list assigned to the variable `tweets`.\n",
    "\n",
    "If you chose `\".json\"` for your fileType, then the second `if` statement will read in \".json\" files and save the content to a dataframe using the Pandas package much like the \".csv\" file process described above. The only difference is that we use the Pandas function `read_json` instead of `read_csv`. Everything else is exactly the same as what is described above in the \"csv\" section. \n",
    "\n",
    "Finally, whether your data was in \".csv\" or \".json\" we join our content to together as a string using `'\\n'.join(tweets)` and we use our cleaning function from above to remove punctuation and lowercase all the words.\n",
    "\n",
    "The last line of code outputs the filepath to each file read. This helps you to see if the code was applied to the correct file(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing text ['/N/u/klosteda/Carbonate/Text-Analysis-master/data/twitter/JSON/coronaVirus01-21Jan2020.json']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if fileType == \".csv\":\n",
    "    filenames = glob.glob(os.path.join(dataRoot, source + fileType))     \n",
    "    dfAll = (pd.read_csv(file, engine = \"python\") for file in filenames)\n",
    "    cdf = pd.concat(dfAll, ignore_index=True)\n",
    "    cdf = pd.DataFrame(cdf, dtype = 'str')\n",
    "    tweets = cdf[textColIndex].values.tolist()\n",
    "if fileType == \".json\":\n",
    "    filenames = glob.glob(os.path.join(dataRoot, source+fileType))\n",
    "    dfAll = (pd.read_json(file, encoding = \"utf-8\", lines = True) for file in filenames)\n",
    "    cdf = pd.concat(dfAll, ignore_index=True)\n",
    "    cdf = pd.DataFrame(cdf, dtype = 'str')\n",
    "    tweets = cdf[textColIndex].values.tolist()\n",
    "    \n",
    "    \n",
    "content = '\\n'.join(tweets)\n",
    "cleanTokens = textClean(content)\n",
    "\n",
    "print('Finished tokenizing text {}\\n'.format(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No changes needed here. This just converts our text to a str object so we can find ngrams later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanText = ' '.join(cleanTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Ngrams\n",
    "\n",
    "We use the textblob package to get ngrams. We use the `ng` variable we created earlier to determine if we are interested in unigrams, bigrams, or trigrams. There should be no reason to make changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(cleanText)\n",
    "\n",
    "if ng == 1: \n",
    "    nGrams = blob.ngrams(n=1)\n",
    "if ng == 2:\n",
    "    nGrams = blob.ngrams(n=2)\n",
    "if ng == 3:\n",
    "    nGrams = blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are converting our ngrams to a list which we can then put into a dataframe to be turned into a wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for wlist in nGrams:\n",
    "   ngramList.append(' '.join(wlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our dataframe. You won't need to make changes to this code. Just note that we had to go through the dataframe and replace the space between our ngrams with an underscore. This is because the Python wordcloud package has trouble handling the space, so we connect our ngrams with an underscore so that the wordcloud package will see it as one word, but humans can see it as two or three, depending on if you chose bigrams or trigrams above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ngramList)\n",
    "df = df.replace(' ', '_', regex=True)\n",
    "dfCounts = df[0].value_counts()\n",
    "countsDF = pd.DataFrame(dfCounts)\n",
    "countsDF.reset_index(inplace = True)\n",
    "df_C = countsDF.rename(columns={'index':'ngrams',0:'freq'})\n",
    "df_C.set_index(df_C['ngrams'], inplace = True)\n",
    "df_C['ngrams'] = df_C['ngrams'].astype(str)\n",
    "dfNG = df_C.sort_values('freq', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see what our dataframe looks like. If you want to see more, just change the number in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngrams</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngrams</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>outbreak_china</th>\n",
       "      <td>outbreak_china</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breaking_news</th>\n",
       "      <td>breaking_news</td>\n",
       "      <td>1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>china_sars</th>\n",
       "      <td>china_sars</td>\n",
       "      <td>1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie_outbreak</th>\n",
       "      <td>movie_outbreak</td>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scene_apocalyptic</th>\n",
       "      <td>scene_apocalyptic</td>\n",
       "      <td>1811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_scene</th>\n",
       "      <td>news_scene</td>\n",
       "      <td>1811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horror_movie</th>\n",
       "      <td>horror_movie</td>\n",
       "      <td>1811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apocalyptic_horror</th>\n",
       "      <td>apocalyptic_horror</td>\n",
       "      <td>1811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>howroute_breaking</th>\n",
       "      <td>howroute_breaking</td>\n",
       "      <td>1810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sars_lik</th>\n",
       "      <td>sars_lik</td>\n",
       "      <td>1810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ngrams  freq\n",
       "ngrams                                      \n",
       "outbreak_china          outbreak_china  1922\n",
       "breaking_news            breaking_news  1831\n",
       "china_sars                  china_sars  1831\n",
       "movie_outbreak          movie_outbreak  1813\n",
       "scene_apocalyptic    scene_apocalyptic  1811\n",
       "news_scene                  news_scene  1811\n",
       "horror_movie              horror_movie  1811\n",
       "apocalyptic_horror  apocalyptic_horror  1811\n",
       "howroute_breaking    howroute_breaking  1810\n",
       "sars_lik                      sars_lik  1810"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNG.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot our wordcloud\n",
    "\n",
    "First we choose the maximum number of ngrams we want in our wordcloud by assigning the number to the variable `maxWrdCnt`.\n",
    "\n",
    "Then we choose a background color and assign it the variable `bgColor`. \n",
    "\n",
    "Next we choose the color of the words in the wordcloud and assign it to the variable `color`. The current color is from the RColorBrewer palette of colors. You can find other color options [here](https://www.nceas.ucsb.edu/~frazier/RSpatialGuides/colorPaletteCheatsheet.pdf). Just put the name of the color selection you want in quotes.\n",
    "\n",
    "Next we choose the minimum font size for the words in the wordcloud and assign the font size to the variable `minFont`.\n",
    "\n",
    "Now we decide the width and height of the output image and assign those integers to the `width` and `height` variables. This mixed with the `figureSz` variable (discussed below) help produce a sharper and crisper looking result, so feel free to play with these numbers.\n",
    "\n",
    "Now we choose the figure size. The first number is the width and the second number is the height. Feel free to make changes as needed. We assign the figure size to the variable `figureSz`.\n",
    "\n",
    "Then we name the output '.png' file our wordcloud will be saved as and assign it to the variable `wcOutputFile`. \n",
    "\n",
    "Next we determine the format of the output file. We already named our file above with a '.png' file type. So we need to make sure this matches that file type. We assign the format to the variable `imgFmt`.\n",
    "\n",
    "Then we choose the resolution of our output image by assigning the dpi resolution we want to the variable `dpi`.\n",
    "\n",
    "Then we create an additional list of stopwords that will remove problematic ngrams. Just remember to type the ngram with an underscore between the two words. Then we remove the row containing the ngram from the database. This keeps you from having to go up and remove single words (and potentially other interesting ngrams) as you can now just remove the specific ngram.\n",
    "\n",
    "Next we give parameters for our wordcloud and save them as `wc`. Any changes to these lines of code were already decided in the variable at the beginning of the cell.\n",
    "\n",
    "Then are the statements for how the wordcloud is displayed. We already chose the figure size above, so we can leave `plt.figure(figsize = figureSz)` alone.  Then we want it to look like what we described in our `wc` variable which is what the `plt.imshow(wc, interpolation = 'bilinear')` statement does. Then we need to state that we are not using an x or y axis by using the `plt.axis(\"off\")` statement. Next we want the layout to be tight instead of spread out so we use the `plt.tight_layout()` statement. Then we state arguments for how we want the wordcloud saved to file (`plt.savefig(os.path.join(dataResults, wcOutputFile), format = imgFmt, dpi = dpi, bbox_inches = 'tight')`), and finally that we want to see the final result displayed in the notebook which is what `plt.show()` does.\n",
    "\n",
    "Run the code and generate your ngram wordcloud!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "maxWrdCnt = 500\n",
    "bgColor = \"black\"\n",
    "color = \"Dark2\"\n",
    "minFont = 10\n",
    "width = 800\n",
    "height = 400\n",
    "figureSz = (20,10)\n",
    "wcOutputFile = \"twitterNgramWordCloud.png\"\n",
    "imgFmt = \"png\"\n",
    "dpi = 600\n",
    "\n",
    "# Ngram Stopwords\n",
    "stopwords = [\"via_youtube\"]\n",
    "text = dfNG[~dfNG['ngrams'].isin(stopwords)]\n",
    "\n",
    "# Wordcloud aesthetics\n",
    "wc = wordcloud.WordCloud(font_path = \"/usr/share/fonts/thai-scalable/Waree.ttf\",background_color = bgColor, width = width, height = height, max_words = maxWrdCnt, colormap = color, min_font_size = minFont).generate_from_frequencies(text['freq'])\n",
    "\n",
    "# show\n",
    "plt.figure(dpi = dpi, figsize = figureSz)\n",
    "plt.imshow(wc, interpolation = 'bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "    \n",
    "# save graph as an image to file\n",
    "plt.savefig(os.path.join(dataResults, wcOutputFile), format = imgFmt, dpi = dpi, bbox_inches = 'tight')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## VOILA!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
