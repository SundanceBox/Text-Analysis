{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader Sentiment Analysis\n",
    "This notebook takes you through doing sentiment analysis of tweets and exporting the data. You can then visualize the results as a bar graph using the \"vaderSentimentResults\" notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about how Vader works behind the scenes see here: https://github.com/cjhutto/vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "Before we start, you will need to have set up a [Carbonate account](https://kb.iu.edu/d/aolp) in order to access [Research Desktop (ReD)](https://kb.iu.edu/d/apum). You will also need to have access to ReD through the [thinlinc client](https://kb.iu.edu/d/aput). If you have not done any of this, or have only done some of this, but not all, you should go to our [textPrep-Py.ipynb](https://github.com/cyberdh/Text-Analysis/blob/drafts/textPrep-Py.ipynb) before you proceed further. The textPrep-Py notebook provides information and resources on how to get a Carbonate account, how to set up ReD, and how to get started using the Jupyter Notebook on ReD.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CyberDH environment\n",
    "The code in the cell below points to a Python environment specifically for use with the Python Jupyter Notebooks created by Cyberinfrastructure for Digital Humanities. It allows for the use of the different packages in our notebooks and their subsequent data sets.\n",
    "\n",
    "##### Packages\n",
    "- **sys:** Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "#### NOTE: This cell is only for use with Research Desktop. You will get an error if you try to run this cell on your personal device!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,\"/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages\")\n",
    "os.environ[\"NLTK_DATA\"] = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary packages for notebook \n",
    "\n",
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others created by Python users are available for download.\n",
    "\n",
    "In your terminal, packages can be installed by simply typing `pip install nameofpackage --user`. However, since you are using ReD and our Python environment, you will not need to install any of the packages below to use this notebook. Anytime you need to make use of a package, however, you need to import it so that Python knows to look in these packages for any functions or commands you use. Below is a brief description of the packages we are using in this notebook:     \n",
    "\n",
    "- **nltk:** Platform for building Python programs to work with human language data. Here we bring in the VADER sentiment analysis tool which is now a part of the nltk package.\n",
    "\n",
    "- **pickle:** Implements binary protocols for serializing and de-serializing a Python object structure. \"Pickling\" is the process whereby a Python object hierarchy is converted into a byte stream, and \"unpickling\" is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy.\n",
    "\n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "- **os:** This module provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "- **glob:** Finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order.\n",
    "\n",
    "- **zipfile:** Allows for handling of zipfiles.\n",
    "\n",
    "Notice we import some of the packages differently. In some cases we just import the entire package when we say `import XYZ`. For some packages which are small, or, from which we are going to use a lot of the functionality it provides, this is fine. \n",
    "\n",
    "Sometimes when we import the package directly we say `import XYZ as X`. All this does is allow us to type `X` instead of `XYZ` when we use certain functions from the package. So we can now say `X.function()` instead of `XYZ.function()`. This saves time typing and eliminates errors from having to type out longer package names. I could just as easily type `import XYZ as potato` and whenever I use a function from the `XYZ` package I would need to type `potato.function()`. What we import the package as is up to you, but some commonly used packages have abbreviations that are standard amongst Python users such as `import pandas as pd` or `import matplotlib.pyplot as plt`. You do not need to us `pd` or `plt`, however, these are widely used and using something else could confuse other users and is generally considered bad practice. \n",
    "\n",
    "Other times we import only specific elements or functions from a package. This is common with packages that are very large and provide a lot of functionality, but from which we are only using a couple functions or a specific subset of the package that contains the functionality we need. This is seen when we say `from XYZ import ABC`. This is saying I only want the `ABC` function from the `XYZ` package. Sometimes we need to point to the specific location where a function is located within the package. We do this by adding periods in between the directory names, so it would look like `from XYZ.123.A1B2 import LMN`. This says we want the `LMN` function which is located in the `XYZ` package and then the `123` and `A1B2` directory in that package. \n",
    "\n",
    "You can also import more than one function from a package by separating the functions with commas like this `from XYZ import ABC, LMN, QRS`. This imports the `ABC`, `LMN` and `QRS` functions from the `XYZ` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "Here we create some variables for use later in our code. We do this to minimize the number and complexity of the changes you will need to make later.\n",
    "\n",
    "First we need to decide if we want to read in one file or a whole directory of files. If you want to read in a whole directory then set `source` equal to `\"*\"` as this is what is called a regular expression that means 'all'. So we are reading in 'all' the files in a directory. If you wish to read in a single file then set `source` equal to the name of the desired file in quotes, but leaving the '.json' or '.csv' off at the end. So a single file should look like this: `source = \"myFileName\"`.\n",
    "\n",
    "Next we assign the file type our data comes in to a variable. At the moment the only options are '.csv' or '.json' as these are the most popular twitter data formats. We assign the format to the `fileType` variable. It should look like this: `fileType = \".json\"`.\n",
    "\n",
    "The `textColIndex` variable is where we put the header name of the dataframe column that will contain the content we are interested in from our tweets. Generally the content of the tweets are labeled as \"text\" since this is the label given to the tweet content when it is pulled directly from the Twitter API. For this reason our default value assigned to the `textColIndex` is `\"text\"`. If for some reason the tweet content has a different label or header, and you need to change this, remember to keep the quotes around the new label.\n",
    "\n",
    "The `remove` variable is assigned a boolean of either **True** or **False**. If it is **True** it means that you want to \"remove\" the terms in the `remWords` list below from the vader lexicon. If you set it to **False**, you do not have any words to remove.\n",
    "\n",
    "The `add` variable is assigned a boolean of either **True** or **False**. If it is **True** it means you want add the key/value pairs in the dictionary `newWords` to the vader lexicon. The `newWords` dcitionary (just below the `add` boolean variable) contains **\"word\": vader polarity score** for words you would like to add to the vader lexicon. The scores in the example dictionary were made up (did not follow vader protocol), however, if you wanted to add terms you would need to follow a similar protocol to vader and find ten people to score the word between -4 (most negative) and 4 (most positive) including 0 as a possible score, and then get the average score (which is the number after each word in the `newWords` dictionary) and also determine the standard deviation as the creators of vader did not include words that had a standard deviation of over 2.5.   \n",
    "\n",
    "The variable `encoding` is where you determine what type of encoding to use (ascii, ISO-8850-1, utf-8, etc...). We have it set to utf-8 at the moment as we have found it is less likely to have any problems.\n",
    "\n",
    "The remaining variables should not need to be changed. The variable `scores` is an empty list that will have data added to it further down in the code. The variables `total`, `numberOfTweets`, and `totalSquared` are variables used for counting which is why 0 is assigned to them since we generally start with nothing when counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = \"coronaVirus01-21Jan2020\"\n",
    "fileType = \".json\"\n",
    "textColIndex = \"text\"\n",
    "remove = True\n",
    "remWords = [\"novel\", \"ha\", \"l\", \"gt\"]\n",
    "add = True\n",
    "newWords = {\"virus\": -1.7, \"outbreak\": -0.6, \"epidemic\": -2.3, \"pandemic\": -3.1, \"quarantine\": -2.6}\n",
    "encoding = \"utf-8\"\n",
    "scores = []\n",
    "total = 0\n",
    "numberOfTweets = 0\n",
    "totalSquared = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths\n",
    "Here we assign file paths we will need throughout this notebook to variables. This way we only need to make changes here and they will be implemeneted throughout the code. The `homePath` variable uses the `environ` function from the `os` package. This function points to your home directory no matter your operating system (Linux, osX, Windows).\n",
    "\n",
    "Then we join the `homePath` variable to folders that point to where our data is stored and we assign this file path to the variable `dataHome`. The folder names are in quotes and separated by a comma. \n",
    "\n",
    "Finally, we again use the `homePath` variable and join it with a file path that points to a folder where we can save our data output. We assign this file path to a variable called `dataClean`.\n",
    "\n",
    "You can change any of these to better match where your data can be found (`dataHome`) and where you want any output such as '.csv' files or images to be saved (`dataClean`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "homePath = os.environ['HOME']\n",
    "vaderPath = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data/sentiment\"\n",
    "dataHome = os.path.join(homePath, \"Text-Analysis-master\",\"data\",\"twitter\")\n",
    "newVaderWords = os.path.join(dataHome, \"sentiment\", \"newVaderWords.txt\")\n",
    "dataClean = os.path.join(homePath,\"Text-Analysis-master\",\"VADERSentimentAnalysis\", \"cleanedData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip files\n",
    "\n",
    "Here we are unzipping files. Since twitter data can be rather large it is often necessary to compress it into a '.zip' file in order to upload it to places such as GitHub. For this reason, we have setup some code to go in and automatically extract all the items in a compressed '.zip' file so you don't have to and so you don't get errors later. If the data is not in a '.zip' file there is no need to worry, it will not give an error if there are no files ending in '.zip' in your directory.\n",
    "\n",
    "The only changes you may wish to make are in the first two lines after `if fileType == \".csv\":` (if you set `fileType` equal to `\".csv\"` above) or the first two lines after `else:` (if you set `fileType` equal to `\".json\"` up above. These are the lines that point to the file paths where your '.zip' files are stored. If you have '.zip' files stored in another folder you will want to change the path. Note that the first line points to the directory and the second line points to the files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if fileType == \".csv\":\n",
    "    direct = os.path.join(dataHome, \"CSV\")\n",
    "    allZipFiles = glob.glob(os.path.join(dataHome, \"CSV\",\"*.zip\"))\n",
    "    for item in allZipFiles:\n",
    "            fileName = os.path.splitext(direct)[0]\n",
    "            zipRef = zipfile.ZipFile(item, \"r\")\n",
    "            zipRef.extractall(fileName)\n",
    "            zipRef.close()\n",
    "            os.remove(item)\n",
    "else:\n",
    "    direct = os.path.join(dataHome, \"JSON\")\n",
    "    allZipFiles = glob.glob(os.path.join(dataHome, \"JSON\",\"*.zip\"))\n",
    "    for item in allZipFiles:\n",
    "            fileName = os.path.splitext(direct)[0]\n",
    "            zipRef = zipfile.ZipFile(item, \"r\")\n",
    "            zipRef.extractall(fileName)\n",
    "            zipRef.close()\n",
    "            os.remove(item)\n",
    "if add == True:\n",
    "    vDirect = vaderPath\n",
    "    vaderZipFile = glob.glob(os.path.join(vaderPath, \"*.zip\"))\n",
    "    for item in vaderZipFile:\n",
    "        vaderFile = os.path.splitext(vDirect)[0]\n",
    "        vaderRef = zipfile.ZipFile(item, \"r\")\n",
    "        vaderRef.extractall(vaderFile)\n",
    "        vaderRef.close()\n",
    "        os.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in .csv and .json files\n",
    "\n",
    "If you chose `\".csv\"` as your `fileType` up above, then the first `if` statement in the code below reads in \".csv\" files and saves the contents to a dataframe using the Pandas package. It will read in either an entire directory or a single \".csv\" file depending on what you chose for `source` above. \n",
    "\n",
    "Once we have read in the \".csv\" file using the Pandas `read_csv` function, we need to concatenate the \".csv\" files if there are multiple. Because of this it is important that your \".csv\" files have an identical column count and each column has identical header names or you will get errors. If you have a single \".csv\" file then you should be fine for this step. We assign this process to the variable `cdf` so we can use it later.\n",
    "\n",
    "Now we convert our `cdf` to a pandas dataframe. This allows for easier manipulation of the data in the next line.\n",
    "\n",
    "Finally, we pull in the column containing the data we are interested in which we assigned to the variable `textColIndex` earlier and turn it into a list assigned to the variable `tweets`.\n",
    "\n",
    "If you chose `\".json\"` for your fileType, then the second `if` statement will read in \".json\" files and save the content to a dataframe using the Pandas package much like the \".csv\" file process described above. The only difference is that we use the Pandas function `read_json` instead of `read_csv`. Everything else is exactly the same as what is described above in the \".csv\" section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if fileType == \".csv\":\n",
    "    allFiles = glob.glob(os.path.join(dataHome, \"CSV\", source + fileType))     \n",
    "    df = (pd.read_csv(f, engine = \"python\") for f in allFiles)\n",
    "    cdf = pd.concat(df, ignore_index=True)\n",
    "    cdf = pd.DataFrame(cdf)\n",
    "    tweets = cdf[textColIndex].values.tolist()\n",
    "if fileType == \".json\":\n",
    "    allFiles = glob.glob(os.path.join(dataHome, \"JSON\", source + fileType))     \n",
    "    df = (pd.read_json(f, encoding = encoding, lines = True) for f in allFiles)\n",
    "    cdf = pd.concat(df, ignore_index=True)\n",
    "    cdf = pd.DataFrame(cdf)\n",
    "    tweets = cdf[textColIndex].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check to see if we have pulled our tweets from our dataset. We are just checking the first 10 tweets. If you wish to see more change the 10 in the parantheses to the number of tweets you wish to see. If you wish to see the last 10 tweets then change `rtDF.head(10)` to `rtDF.tail(10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simply...si no és #coronavirus, NO hi ha #SARS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @XavierAbadMdG: Simply...si no és #coronavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Update on cluster of #Patients infected with #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#HK, Two suspected #MERS #Coronavirus cases re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suspicion de Syndrome Respiratoire du Moyen-Or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#HK, Suspected #MERS #Coronavirus case reporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @ironorehopper: #HK, Suspected #MERS #Coron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5th Jan Wuhan official report\\n\\n1. 59 cases, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Latest reports are of a possible #SARS-#MERS h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#HK, Suspected #MERS #Coronavirus case reporte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Simply...si no és #coronavirus, NO hi ha #SARS...\n",
       "1  RT @XavierAbadMdG: Simply...si no és #coronavi...\n",
       "2  Update on cluster of #Patients infected with #...\n",
       "3  #HK, Two suspected #MERS #Coronavirus cases re...\n",
       "4  Suspicion de Syndrome Respiratoire du Moyen-Or...\n",
       "5  #HK, Suspected #MERS #Coronavirus case reporte...\n",
       "6  RT @ironorehopper: #HK, Suspected #MERS #Coron...\n",
       "7  5th Jan Wuhan official report\\n\\n1. 59 cases, ...\n",
       "8  Latest reports are of a possible #SARS-#MERS h...\n",
       "9  #HK, Suspected #MERS #Coronavirus case reporte..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtDF = pd.DataFrame(tweets)\n",
    "rtDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create labels for different sentiment values and start the count for each value at zero\n",
    "Here we create the labels for our different sentiment values by creating a dictionary. The number in quotes is the \"key\" and the integer after the colon is the value. Our sentiment scores can be anywhere from -1 to 1 with -1 having a very negative sentiment and 1 having a very positive sentiment. We assign this dictionary to the variable `res`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = {\"-1\":0, \"-.9\":0, \"-.8\":0, \"-.7\":0, \"-.6\":0, \"-.5\":0, \"-.4\":0, \"-.3\":0, \"-.2\":0, \"-.1\":0, \"0\":0, \".1\":0, \".2\":0,\".3\":0, \".4\":0, \".5\":0, \".6\":0, \".7\":0, \".8\":0, \".9\":0, \"1\":0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shorten SentimentIntensityAnalyzer Function\n",
    "\n",
    "We shorten the `SentimentIntensityAnalyzer()` by assigning it to the variable `vader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove words\n",
    "Here we have an \"if...else\" statement. If we assigned **True** to the variable `remove` above then we take each word in the `remWords` list and apply (`map`) the `.pop` function from vader to each word in the list. \n",
    "\n",
    "If we assigned **False** to `remove` then we do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if remove == True:\n",
    "    map(vader.lexicon.pop, remWords)\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add words\n",
    "Here we have another \"if...else\" statement. If we assigned **True** to `add` then we use the `.update` function from vader to add each `{\"key\": value}` pair from our `newWords` dictionary above.\n",
    "\n",
    "If we assigned **False** to `add` then we do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if add == True:\n",
    "    vader.lexicon.update(newWords)\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go through and apply the Vader sentiment analyzer to all tweets and count them\n",
    "\n",
    "Here we apply VADER to our tweets and start adding the results to the `res` dictionary as well as the `total`, `numberOfTweets`, `totalSquared`, and `scores` variables. No changes should be needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-1': 42, '-.9': 1996, '-.8': 218, '-.7': 230, '-.6': 582, '-.5': 1606, '-.4': 196, '-.3': 586, '-.2': 348, '-.1': 641, '0': 5107, '.1': 325, '.2': 364, '.3': 233, '.4': 1400, '.5': 342, '.6': 227, '.7': 185, '.8': 199, '.9': 50, '1': 10}\n"
     ]
    }
   ],
   "source": [
    "for index, row in rtDF.iterrows():\n",
    "    vs = vader.polarity_scores(str(rtDF.iloc[:,0][index]))\n",
    "    scores.append(vs['compound'])\n",
    "    total += vs[\"compound\"]\n",
    "    numberOfTweets += 1\n",
    "    totalSquared += vs[\"compound\"]**2\n",
    "    if vs[\"compound\"]==0.0:\n",
    "        res[\"0\"] +=1\n",
    "    elif 0 < vs[\"compound\"] <= 0.1:\n",
    "        res[\".1\"] +=1\n",
    "    elif 0.1 <= vs[\"compound\"] <= 0.2:\n",
    "        res[\".2\"] +=1\n",
    "    elif 0.2 < vs[\"compound\"] <= 0.3:\n",
    "        res[\".3\"] +=1\n",
    "    elif 0.3 < vs[\"compound\"] <= 0.4:\n",
    "        res[\".4\"] +=1\n",
    "    elif 0.4 < vs[\"compound\"] <= 0.5:\n",
    "        res[\".5\"] +=1\n",
    "    elif 0.5 < vs[\"compound\"] <= 0.6:\n",
    "        res[\".6\"] +=1\n",
    "    elif 0.6 < vs[\"compound\"] <= 0.7:\n",
    "        res[\".7\"] +=1\n",
    "    elif 0.7 < vs[\"compound\"] <= 0.8:\n",
    "        res[\".8\"] +=1\n",
    "    elif 0.8 < vs[\"compound\"] <= 0.9:\n",
    "        res[\".9\"] +=1\n",
    "    elif 0.9 < vs[\"compound\"] <= 1:\n",
    "        res[\"1\"] +=1\n",
    "    elif 0 > vs[\"compound\"] >= -0.1:\n",
    "        res[\"-.1\"] +=1\n",
    "    elif -0.1 > vs[\"compound\"] >= -0.2:\n",
    "        res[\"-.2\"] +=1\n",
    "    elif -0.2 > vs[\"compound\"] >= -0.3:\n",
    "        res[\"-.3\"] +=1\n",
    "    elif -0.3 > vs[\"compound\"] >= -0.4:\n",
    "        res[\"-.4\"] +=1\n",
    "    elif -0.4 > vs[\"compound\"] >= -0.5:\n",
    "        res[\"-.5\"] +=1\n",
    "    elif -0.5 > vs[\"compound\"] >= -0.6:\n",
    "        res[\"-.6\"] +=1\n",
    "    elif -0.6 > vs[\"compound\"] >= -0.7:\n",
    "        res[\"-.7\"] +=1\n",
    "    elif -0.7 > vs[\"compound\"] >= -0.8:\n",
    "        res[\"-.8\"] +=1\n",
    "    elif -0.8 > vs[\"compound\"] >= -0.9:\n",
    "        res[\"-.9\"] +=1\n",
    "    elif -0.9 > vs[\"compound\"] >= -1:\n",
    "        res[\"-1\"] +=1\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data\n",
    "\n",
    "Now we use the pickle package to export the data needed by the \"vaderSentimentResults\" notebook to create the bar graph visualization. We save the data to the folder in the file path assigned to the `dataClean` variable. To change the names of the output files change the contents in the quotes such as `\"vaderScores\"`, `\"total\"`, `\"scores\"`, `\"numberOfText\"`, and `\"squared\"`. DO NOT change `\"wb\"` as this states that we are writing a bytes object to file which is what we want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(dataClean, \"vaderScores\"), \"wb\") as vaderScore:\n",
    "    pickle.dump(res, vaderScore)\n",
    "with open(os.path.join(dataClean, \"total\"), \"wb\") as vaderTotal:\n",
    "    pickle.dump(total, vaderTotal)\n",
    "with open(os.path.join(dataClean, \"scores\"), \"wb\") as s:\n",
    "    pickle.dump(scores, s)\n",
    "with open(os.path.join(dataClean, \"numberOfText\"), \"wb\") as nt:\n",
    "    pickle.dump(numberOfTweets, nt)\n",
    "with open(os.path.join(dataClean, \"squared\"), \"wb\") as squared:\n",
    "    pickle.dump(totalSquared, squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
