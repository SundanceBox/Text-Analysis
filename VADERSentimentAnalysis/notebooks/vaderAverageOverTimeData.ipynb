{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader Sentiment Analysis Average Over Time Data Prep\n",
    "This notebook prepares tweets to give an average sentiment over time as well as a count of tweets over time. You can group tweets by day, week, month, or year depending on your data and needs. To view the output as a line graph, use the \"vaderAverageOverTimeResults\" notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about how Vader works behind the scenes see here: https://github.com/cjhutto/vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "Before we start, you will need to have set up a [Carbonate account](https://kb.iu.edu/d/aolp) in order to access [Research Desktop (ReD)](https://kb.iu.edu/d/apum). You will also need to have access to ReD through the [thinlinc client](https://kb.iu.edu/d/aput). If you have not done any of this, or have only done some of this, but not all, you should go to our [textPrep-Py.ipynb](https://github.com/cyberdh/Text-Analysis/blob/master/textPrep-Py.ipynb) before you proceed further. The textPrep-Py notebook provides information and resources on how to get a Carbonate account, how to set up ReD, and how to get started using the Jupyter Notebook on ReD.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CyberDH environment\n",
    "The code in the cell below points to a Python environment specificaly for use with the Python Jupyter Notebooks created by Cyberinfrastructure for Digital Humanities. It allows for the use of the different packages in our notebooks and their subsequent data sets.\n",
    "\n",
    "##### Packages\n",
    "- **sys:** Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "#### NOTE: This cell is only for use with Research Desktop. You will get an error if you try to run this cell on your personal device!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,\"/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages\")\n",
    "os.environ[\"NLTK_DATA\"] = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary packages for notebook \n",
    "\n",
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others created by Python users are available for download.\n",
    "\n",
    "In your terminal, packages can be installed by simply typing `pip install nameofpackage --user`. However, since you are using ReD and our Python environment, you will not need to install any of the packages below to use this notebook. Anytime you need to make use of a package, however, you need to import it so that Python knows to look in these packages for any functions or commands you use. Below is a brief description of the packages we are using in this notebook:    \n",
    "\n",
    "- **nltk:** Platform for building Python programs to work with human language data. Here we bring in the VADER sentiment analysis tool which is now a part of the nltk package.\n",
    "\n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "- **glob:** Finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order.\n",
    "\n",
    "- **zipfile:** Allows for handling of zipfiles.\n",
    "\n",
    "- **warnings:** Allows for the manipulation of warning messages in Python.\n",
    "\n",
    "Notice we import some of the packages differently. In some cases we just import the entire package when we say `import XYZ`. For some packages which are small, or, from which we are going to use a lot of the functionality it provides, this is fine. \n",
    "\n",
    "Sometimes when we import the package directly we say `import XYZ as X`. All this does is allow us to type `X` instead of `XYZ` when we use certain functions from the package. So we can now say `X.function()` instead of `XYZ.function()`. This saves time typing and eliminates errors from having to type out longer package names. I could just as easily type `import XYZ as potato` and whenever I use a function from the `XYZ` package I would need to type `potato.function()`. What we import the package as is up to you, but some commonly used packages have abbreviations that are standard amongst Python users such as `import pandas as pd` or `import matplotlib.pyplot as plt`. You do not need to us `pd` or `plt`, however, these are widely used and using something else could confuse other users and is generally considered bad practice. \n",
    "\n",
    "Other times we import only specific elements or functions from a package. This is common with packages that are very large and provide a lot of functionality, but from which we are only using a couple functions or a specific subset of the package that contains the functionality we need. This is seen when we say `from XYZ import ABC`. This is saying I only want the `ABC` function from the `XYZ` package. Sometimes we need to point to the specific location where a function is located within the package. We do this by adding periods in between the directory names, so it would look like `from XYZ.123.A1B2 import LMN`. This says we want the `LMN` function which is located in the `XYZ` package and then the `123` and `A1B2` directory in that package. \n",
    "\n",
    "You can also import more than one function from a package by separating the functions with commas like this `from XYZ import ABC, LMN, QRS`. This imports the `ABC`, `LMN` and `QRS` functions from the `XYZ` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import glob\n",
    "import zipfile\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will ignore future warnings. All the warnings in this code are not concerning and will not break the code or cause errors in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "Here we create some variables for use later in our code. We do this to minimize the number and complexity of the changes you will need to make later.\n",
    "\n",
    "First we need to decide if we want to read in one '.json' file or a whole directory of '.json' files. If you want to read in a whole directory then set `source` equal to `\"*\"`. This is what is called a regular expression that means 'all'. So we are reading in 'all' the files in a directory. If you wish to read in a single file then set `source` equal to the name of the desired file in quotes, but leaving the '.json' or '.csv' off at the end. So a single file should look like this: `source = \"myFileName\"`.\n",
    "\n",
    "Next we assign the file type our data comes in to a variable. At the moment the only options are '.csv' or '.json' as these are the most popular twitter data formats. We assign the format to the `fileType` variable. It should look like this: `fileType = \".json\"`.\n",
    "\n",
    "The `textColIndex` variable is where we put the header name of the dataframe column that will contain the content we are interested in from our tweets. Generally the content of the tweets are labeled as \"text\" since this is the label given to the tweet content when it is pulled directly from the Twitter API. For this reason our default value assigned to the `textColIndex` is `\"text\"`. If for some reason the tweet content has a different label or header, and you need to change this, remember to keep the quotes around the new label.\n",
    "\n",
    "We assign a unit of time to our `timeLength` variable. This is used later to get our average sentiment and tweet counts by day, week, month, or year. To signify a per day mean/count we use `\"D\"` (with the quotes). For a per week average we use `\"W\"`, followed by `\"M\"` for month, and `\"Y\"` for year if your data happens to span multiple years and that is what you are most interested in. Note that this will affect the graphical output in the \"vaderAverageOverTimeResults\" notebook as it will show the means/counts based on the setting here.\n",
    "\n",
    "The `remove` variable is assigned a boolean of either **True** or **False**. If it is **True** it means that you want to \"remove\" the terms in the `remWords` list below from the vader lexicon. If you set it to **False**, you do not have any words to remove.\n",
    "\n",
    "The `add` variable is assigned a boolean of either **True** or **False**. If it is **True** it means you want add the key/value pairs in the dictionary `newWords` to the vader lexicon. The `newWords` dcitionary (just below the `add` boolean variable) contains **\"word\": vader polarity score** for words you would like to add to the vader lexicon. The scores in the example dictionary were made up (did not follow vader protocol), however, if you wanted to add terms you would need to follow a similar protocol to vader and find ten people to score the word between -4 (most negative) and 4 (most positive) including 0 as a possible score, and then get the average score (which is the number after each word in the `newWords` dictionary) and also determine the standard deviation as the creators of vader did not include words that had a standard deviation of over 2.5.\n",
    "\n",
    "**NOTE:** If you want to change the score/polarity of an existing word in the dictionary, first remove the word by including it in the `remWords` list, then add the word with a new polarity score in the `newWords` dictionary. This removes the current word and score and adds the word with a new score in the algorithm. \n",
    "\n",
    "The variable `encoding` is where you determine what type of encoding to use (ascii, ISO-8850-1, utf-8, etc...). We have it set to utf-8 at the moment as we have found it is less likely to have any problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source = \"coronaVirus01-21Jan2020\"\n",
    "fileType = \".json\"\n",
    "textColIndex = \"text\"\n",
    "timeLength = \"D\"\n",
    "remove = True\n",
    "remWords = [\"novel\", \"ha\", \"l\", \"gt\", \"positive\"]\n",
    "add = True\n",
    "newWords = {\"virus\": -1.7, \"outbreak\": -0.6, \"epidemic\": -2.3, \"pandemic\": -3.1, \"quarantine\": -2.6, \"positive\": -2.6}\n",
    "encoding = \"utf-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File paths\n",
    "Here we assign file paths we will need throughout this notebook to variables. This way we only need to make changes here and they will be implemeneted throughout the code. The `homePath` variable uses the `environ` function from the `os` package. This function points to your home directory no matter your operating system (Linux, osX, Windows).\n",
    "\n",
    "Then we join the `homePath` variable to folders that point to where our data is stored and we assign this file path to the variable `dataHome`. The folder names are in quotes and separated by a comma. \n",
    "\n",
    "Finally, we again use the `homePath` variable and join it with a file path that points to a folder where we can save cleaned and prepped data to be used in the \"vaderAverageOverTimeResults\" notebook. We assign this file path to a variable called `dataClean`.\n",
    "\n",
    "You can change any of these to better match where your data can be found (`dataHome`) and where you want your cleaned/prepped data stored (`dataClean`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homePath = os.environ['HOME']\n",
    "dataHome = os.path.join(homePath,'Text-Analysis-master', 'data', 'twitter')\n",
    "dataClean = os.path.join(homePath,\"Text-Analysis-master\",\"VADERSentimentAnalysis\", \"cleanedData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shorten SentimentIntensityAnalyzer Function\n",
    "We shorten the `SentimentIntensityAnalyzer()` to the variable `vader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove words\n",
    "Here we have an \"if...else\" statement. If we assigned **True** to the variable `remove` above then we apply the `.pop` function from vader to each word in the list. \n",
    "\n",
    "If we assigned **False** to `remove` then we do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if remove == True:\n",
    "    [vader.lexicon.pop(x) for x in remWords]\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add words\n",
    "Here we have another \"if...else\" statement. If we assigned **True** to `add` then we use the `.update` function from vader to add each `{\"key\": value}` pair from our `newWords` dictionary above.\n",
    "\n",
    "If we assigned **False** to `add` then we do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if add == True:\n",
    "    vader.lexicon.update(newWords)\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip files\n",
    "\n",
    "Here we are unzipping files. Since twitter data can be rather large it is often necessary to compress it into a '.zip' file in order to upload it to places such as GitHub. For this reason, we have setup some code to go in and automatically extract all the items in a compressed '.zip' file so you don't have to and so you don't get errors later. If the data is not in a '.zip' file there is no need to worry, it will not give an error if there are no files ending in '.zip' in your directory.\n",
    "\n",
    "The only changes you may wish to make are in the first two lines. These are the lines that point to the file paths where your '.zip' files are stored. If you have '.zip' files stored in another folder you will want to change the path. Note that the first line points to the directory and the second line points to the files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if fileType == \".json\":\n",
    "    direct = os.path.join(dataHome, \"JSON\")\n",
    "    allZipFiles = glob.glob(os.path.join(dataHome, \"JSON\",\"*.zip\"))\n",
    "    for item in allZipFiles:\n",
    "            fileName = os.path.splitext(direct)[0]\n",
    "            zipRef = zipfile.ZipFile(item, \"r\")\n",
    "            zipRef.extractall(fileName)\n",
    "            zipRef.close()\n",
    "            os.remove(item)    \n",
    "else:\n",
    "    direct = os.path.join(dataHome, \"CSV\")\n",
    "    allZipFiles = glob.glob(os.path.join(dataHome, \"CSV\",\"*.zip\"))\n",
    "    for item in allZipFiles:\n",
    "            fileName = os.path.splitext(direct)[0]\n",
    "            zipRef = zipfile.ZipFile(item, \"r\")\n",
    "            zipRef.extractall(fileName)\n",
    "            zipRef.close()\n",
    "            os.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in .csv and .json files\n",
    "\n",
    "If you chose `\".json\"` as your `fileType` up above, then the first `if` statement in the code below reads in \".json\" files and saves the contents to a dataframe using the Pandas package. It will read in either an entire directory or a single \".json\" file depending on what you chose for `source` above. \n",
    "\n",
    "Once we have read in the \".json\" file using the Pandas `read_json` function, we need to concatenate the \".json\" files if there are multiple. Because of this it is important that your \".json\" files have an identical key count and each key has identical names or you will get errors. If you have a single \".json\" file then you should be fine for this step. We assign this process to the variable `cdf` so we can use it later.\n",
    "\n",
    "Now we convert our `cdf` to a pandas dataframe. This allows for easier manipulation of the data in the next line.\n",
    "\n",
    "Finally, we pull in the key containing the data we are interested in which we assigned to the variable `textColIndex` earlier and turn it into a list assigned to the variable `tweets`.\n",
    "\n",
    "If you chose `\".csv\"` for your fileType, then the second `if` statement will read in \".csv\" files and save the content to a dataframe using the Pandas package much like the \".json\" file process described above. The only difference is that we use the Pandas function `read_csv` instead of `read_json`. Everything else is exactly the same as what is described above in the \".json\" section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14887\n"
     ]
    }
   ],
   "source": [
    "if fileType == \".json\":\n",
    "    allFiles = glob.glob(os.path.join(dataHome,\"JSON\",source + fileType))     \n",
    "    df = (pd.read_json(f, encoding = encoding, lines = True) for f in allFiles)\n",
    "    cdf = pd.concat(df, ignore_index=True)\n",
    "    cdf = pd.DataFrame(cdf)\n",
    "if fileType == \".csv\":\n",
    "    allFiles = glob.glob(os.path.join(dataHome, \"CSV\", source + fileType))     \n",
    "    df = (pd.read_csv(f, engine = \"python\") for f in allFiles)\n",
    "    cdf = pd.concat(df, ignore_index=True)\n",
    "    cdf = pd.DataFrame(cdf)\n",
    "cdf[\"text\"] = cdf[\"text\"].astype(str)\n",
    "print(len(cdf[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run VADER\n",
    "\n",
    "Now we run vader over our tweets. We do this by \"applying\" `vader.polarity_scores` to each tweet in the \"text\" column of our \"cdf\" data frame.\n",
    "\n",
    "Then we concatenate the scores to the end of the data frame.\n",
    "\n",
    "Lastly we display the first five rows so we can check and make sure we have what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>favorited</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>otherfields</th>\n",
       "      <th>place</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01 11:10:10</td>\n",
       "      <td>{'hashtags': [{'text': 'coronavirus', 'indices...</td>\n",
       "      <td>False</td>\n",
       "      <td>1212330167269515264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'display_text_range': '[0,140]', 'favorite_co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Simply...si no és #coronavirus, NO hi ha #SARS...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'created_at': 'Mon Aug 29 16:48:30 -0400 2011...</td>\n",
       "      <td>364476368</td>\n",
       "      <td>XavierAbadMdG</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01 12:08:45</td>\n",
       "      <td>{'hashtags': [{'text': 'coronavirus', 'indices...</td>\n",
       "      <td>False</td>\n",
       "      <td>1212344909665046528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'favorite_count': '0', 'filter_level': 'low',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>RT @XavierAbadMdG: Simply...si no és #coronavi...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Sat Nov 24 15:43:45 -0500 2012...</td>\n",
       "      <td>968761746</td>\n",
       "      <td>RossellRos</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-03 09:42:22</td>\n",
       "      <td>{'hashtags': [{'text': 'Patients', 'indices': ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1213032848086773760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'favorite_count': '0', 'filter_level': 'low',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;a href=\"http://www.agenparl.com/\" rel=\"nofoll...</td>\n",
       "      <td>Update on cluster of #Patients infected with #...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'created_at': 'Mon Jul 19 04:38:21 -0400 2010...</td>\n",
       "      <td>168425731</td>\n",
       "      <td>Agenparl</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-03 17:08:04</td>\n",
       "      <td>{'hashtags': [{'text': 'HK', 'indices': [0, 3]...</td>\n",
       "      <td>False</td>\n",
       "      <td>1213145011774218240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'favorite_count': '0', 'filter_level': 'low',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;a href=\"https://www.bloglovin.com\" rel=\"nofol...</td>\n",
       "      <td>#HK, Two suspected #MERS #Coronavirus cases re...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Wed Aug 06 15:51:01 -0400 2008...</td>\n",
       "      <td>15754217</td>\n",
       "      <td>ironorehopper</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-04 08:23:03</td>\n",
       "      <td>{'hashtags': [{'text': 'Coronavirus', 'indices...</td>\n",
       "      <td>False</td>\n",
       "      <td>1213375273590251520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'favorite_count': '0', 'filter_level': 'low',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Suspicion de Syndrome Respiratoire du Moyen-Or...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Sun May 01 11:50:43 -0400 2016...</td>\n",
       "      <td>726801029963108352</td>\n",
       "      <td>Azeria64Azeria</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  coordinates          created_at  \\\n",
       "0         NaN 2020-01-01 11:10:10   \n",
       "1         NaN 2020-01-01 12:08:45   \n",
       "2         NaN 2020-01-03 09:42:22   \n",
       "3         NaN 2020-01-03 17:08:04   \n",
       "4         NaN 2020-01-04 08:23:03   \n",
       "\n",
       "                                            entities  favorited  \\\n",
       "0  {'hashtags': [{'text': 'coronavirus', 'indices...      False   \n",
       "1  {'hashtags': [{'text': 'coronavirus', 'indices...      False   \n",
       "2  {'hashtags': [{'text': 'Patients', 'indices': ...      False   \n",
       "3  {'hashtags': [{'text': 'HK', 'indices': [0, 3]...      False   \n",
       "4  {'hashtags': [{'text': 'Coronavirus', 'indices...      False   \n",
       "\n",
       "                id_str in_reply_to_screen_name  in_reply_to_status_id_str  \\\n",
       "0  1212330167269515264                     NaN                        NaN   \n",
       "1  1212344909665046528                     NaN                        NaN   \n",
       "2  1213032848086773760                     NaN                        NaN   \n",
       "3  1213145011774218240                     NaN                        NaN   \n",
       "4  1213375273590251520                     NaN                        NaN   \n",
       "\n",
       "                                         otherfields place  retweet_count  \\\n",
       "0  {'display_text_range': '[0,140]', 'favorite_co...   NaN              1   \n",
       "1  {'favorite_count': '0', 'filter_level': 'low',...   NaN              0   \n",
       "2  {'favorite_count': '0', 'filter_level': 'low',...   NaN              0   \n",
       "3  {'favorite_count': '0', 'filter_level': 'low',...   NaN              0   \n",
       "4  {'favorite_count': '0', 'filter_level': 'low',...   NaN              0   \n",
       "\n",
       "   ...                                             source  \\\n",
       "0  ...  <a href=\"http://twitter.com/download/android\" ...   \n",
       "1  ...  <a href=\"http://twitter.com/#!/download/ipad\" ...   \n",
       "2  ...  <a href=\"http://www.agenparl.com/\" rel=\"nofoll...   \n",
       "3  ...  <a href=\"https://www.bloglovin.com\" rel=\"nofol...   \n",
       "4  ...  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                                text  truncated  \\\n",
       "0  Simply...si no és #coronavirus, NO hi ha #SARS...       True   \n",
       "1  RT @XavierAbadMdG: Simply...si no és #coronavi...      False   \n",
       "2  Update on cluster of #Patients infected with #...       True   \n",
       "3  #HK, Two suspected #MERS #Coronavirus cases re...      False   \n",
       "4  Suspicion de Syndrome Respiratoire du Moyen-Or...      False   \n",
       "\n",
       "                                                user         user_id_str  \\\n",
       "0  {'created_at': 'Mon Aug 29 16:48:30 -0400 2011...           364476368   \n",
       "1  {'created_at': 'Sat Nov 24 15:43:45 -0500 2012...           968761746   \n",
       "2  {'created_at': 'Mon Jul 19 04:38:21 -0400 2010...           168425731   \n",
       "3  {'created_at': 'Wed Aug 06 15:51:01 -0400 2008...            15754217   \n",
       "4  {'created_at': 'Sun May 01 11:50:43 -0400 2016...  726801029963108352   \n",
       "\n",
       "  user_screen_name    neg    neu  pos compound  \n",
       "0    XavierAbadMdG  0.165  0.835  0.0  -0.6289  \n",
       "1       RossellRos  0.204  0.796  0.0  -0.6289  \n",
       "2         Agenparl  0.158  0.842  0.0  -0.4939  \n",
       "3    ironorehopper  0.137  0.863  0.0  -0.2263  \n",
       "4   Azeria64Azeria  0.224  0.776  0.0  -0.3818  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = cdf['text'].apply(lambda x: vader.polarity_scores(x))\n",
    "cdf = pd.concat([cdf,sentiment.apply(pd.Series)],1)\n",
    "cdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get dates\n",
    "\n",
    "Now we need to get the dates for the tweets so we can organize the tweets chronologically. \n",
    "\n",
    "We start by sorting the tweets in order based on the dates in our \"created_at\" column in our data frame. If your dates are labeled by another name then change \"created_at\" to the name of the key (.json) or column header (.csv) containing your dates in your original dataset.\n",
    "\n",
    "Then we have an \"if...else\" statement to remove the timezone info if our data was in .csv format as it did not do this automatically like it does for the \".json\" format. Having the timezone info causes issues with the graphical output later, but it does not affect the results.\n",
    "\n",
    "Next we convert the dates in the \"created_at\" column to the date/time format in the `to_datetime` function from the pandas package which is a YYYY-MM-DD format.\n",
    "\n",
    "Then we remove rows that do not contain a date as we have no way of knowing where to group the tweet. This means we will only get the average sentiment for tweets with dates.\n",
    "\n",
    "Now we make the column containing our dates the index column as this is necessary for getting the mean for the dates.\n",
    "\n",
    "Next we change the name of the index so we don't have two columns with the same name as this would cause errors later.\n",
    "\n",
    "Lastly, we take a look at the first five rows to see our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>favorited</th>\n",
       "      <th>id_str</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>otherfields</th>\n",
       "      <th>place</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 11:10:10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01 11:10:10</td>\n",
       "      <td>{'hashtags': [{'text': 'coronavirus', 'indices...</td>\n",
       "      <td>False</td>\n",
       "      <td>1212330167269515264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'display_text_range': '[0,140]', 'favorite_co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Simply...si no és #coronavirus, NO hi ha #SARS...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'created_at': 'Mon Aug 29 16:48:30 -0400 2011...</td>\n",
       "      <td>364476368</td>\n",
       "      <td>XavierAbadMdG</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 12:08:45</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-01 12:08:45</td>\n",
       "      <td>{'hashtags': [{'text': 'coronavirus', 'indices...</td>\n",
       "      <td>False</td>\n",
       "      <td>1212344909665046528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'favorite_count': '0', 'filter_level': 'low',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>RT @XavierAbadMdG: Simply...si no és #coronavi...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Sat Nov 24 15:43:45 -0500 2012...</td>\n",
       "      <td>968761746</td>\n",
       "      <td>RossellRos</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03 09:42:22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-03 09:42:22</td>\n",
       "      <td>{'hashtags': [{'text': 'Patients', 'indices': ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1213032848086773760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'favorite_count': '0', 'filter_level': 'low',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;a href=\"http://www.agenparl.com/\" rel=\"nofoll...</td>\n",
       "      <td>Update on cluster of #Patients infected with #...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'created_at': 'Mon Jul 19 04:38:21 -0400 2010...</td>\n",
       "      <td>168425731</td>\n",
       "      <td>Agenparl</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03 17:08:04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-03 17:08:04</td>\n",
       "      <td>{'hashtags': [{'text': 'HK', 'indices': [0, 3]...</td>\n",
       "      <td>False</td>\n",
       "      <td>1213145011774218240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'favorite_count': '0', 'filter_level': 'low',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;a href=\"https://www.bloglovin.com\" rel=\"nofol...</td>\n",
       "      <td>#HK, Two suspected #MERS #Coronavirus cases re...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Wed Aug 06 15:51:01 -0400 2008...</td>\n",
       "      <td>15754217</td>\n",
       "      <td>ironorehopper</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04 08:23:03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-04 08:23:03</td>\n",
       "      <td>{'hashtags': [{'text': 'Coronavirus', 'indices...</td>\n",
       "      <td>False</td>\n",
       "      <td>1213375273590251520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'favorite_count': '0', 'filter_level': 'low',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Suspicion de Syndrome Respiratoire du Moyen-Or...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Sun May 01 11:50:43 -0400 2016...</td>\n",
       "      <td>726801029963108352</td>\n",
       "      <td>Azeria64Azeria</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    coordinates          created_at  \\\n",
       "dateTime                                              \n",
       "2020-01-01 11:10:10         NaN 2020-01-01 11:10:10   \n",
       "2020-01-01 12:08:45         NaN 2020-01-01 12:08:45   \n",
       "2020-01-03 09:42:22         NaN 2020-01-03 09:42:22   \n",
       "2020-01-03 17:08:04         NaN 2020-01-03 17:08:04   \n",
       "2020-01-04 08:23:03         NaN 2020-01-04 08:23:03   \n",
       "\n",
       "                                                              entities  \\\n",
       "dateTime                                                                 \n",
       "2020-01-01 11:10:10  {'hashtags': [{'text': 'coronavirus', 'indices...   \n",
       "2020-01-01 12:08:45  {'hashtags': [{'text': 'coronavirus', 'indices...   \n",
       "2020-01-03 09:42:22  {'hashtags': [{'text': 'Patients', 'indices': ...   \n",
       "2020-01-03 17:08:04  {'hashtags': [{'text': 'HK', 'indices': [0, 3]...   \n",
       "2020-01-04 08:23:03  {'hashtags': [{'text': 'Coronavirus', 'indices...   \n",
       "\n",
       "                     favorited               id_str in_reply_to_screen_name  \\\n",
       "dateTime                                                                      \n",
       "2020-01-01 11:10:10      False  1212330167269515264                     NaN   \n",
       "2020-01-01 12:08:45      False  1212344909665046528                     NaN   \n",
       "2020-01-03 09:42:22      False  1213032848086773760                     NaN   \n",
       "2020-01-03 17:08:04      False  1213145011774218240                     NaN   \n",
       "2020-01-04 08:23:03      False  1213375273590251520                     NaN   \n",
       "\n",
       "                     in_reply_to_status_id_str  \\\n",
       "dateTime                                         \n",
       "2020-01-01 11:10:10                        NaN   \n",
       "2020-01-01 12:08:45                        NaN   \n",
       "2020-01-03 09:42:22                        NaN   \n",
       "2020-01-03 17:08:04                        NaN   \n",
       "2020-01-04 08:23:03                        NaN   \n",
       "\n",
       "                                                           otherfields place  \\\n",
       "dateTime                                                                       \n",
       "2020-01-01 11:10:10  {'display_text_range': '[0,140]', 'favorite_co...   NaN   \n",
       "2020-01-01 12:08:45  {'favorite_count': '0', 'filter_level': 'low',...   NaN   \n",
       "2020-01-03 09:42:22  {'favorite_count': '0', 'filter_level': 'low',...   NaN   \n",
       "2020-01-03 17:08:04  {'favorite_count': '0', 'filter_level': 'low',...   NaN   \n",
       "2020-01-04 08:23:03  {'favorite_count': '0', 'filter_level': 'low',...   NaN   \n",
       "\n",
       "                     retweet_count  ...  \\\n",
       "dateTime                            ...   \n",
       "2020-01-01 11:10:10              1  ...   \n",
       "2020-01-01 12:08:45              0  ...   \n",
       "2020-01-03 09:42:22              0  ...   \n",
       "2020-01-03 17:08:04              0  ...   \n",
       "2020-01-04 08:23:03              0  ...   \n",
       "\n",
       "                                                                source  \\\n",
       "dateTime                                                                 \n",
       "2020-01-01 11:10:10  <a href=\"http://twitter.com/download/android\" ...   \n",
       "2020-01-01 12:08:45  <a href=\"http://twitter.com/#!/download/ipad\" ...   \n",
       "2020-01-03 09:42:22  <a href=\"http://www.agenparl.com/\" rel=\"nofoll...   \n",
       "2020-01-03 17:08:04  <a href=\"https://www.bloglovin.com\" rel=\"nofol...   \n",
       "2020-01-04 08:23:03  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                                                  text  \\\n",
       "dateTime                                                                 \n",
       "2020-01-01 11:10:10  Simply...si no és #coronavirus, NO hi ha #SARS...   \n",
       "2020-01-01 12:08:45  RT @XavierAbadMdG: Simply...si no és #coronavi...   \n",
       "2020-01-03 09:42:22  Update on cluster of #Patients infected with #...   \n",
       "2020-01-03 17:08:04  #HK, Two suspected #MERS #Coronavirus cases re...   \n",
       "2020-01-04 08:23:03  Suspicion de Syndrome Respiratoire du Moyen-Or...   \n",
       "\n",
       "                     truncated  \\\n",
       "dateTime                         \n",
       "2020-01-01 11:10:10       True   \n",
       "2020-01-01 12:08:45      False   \n",
       "2020-01-03 09:42:22       True   \n",
       "2020-01-03 17:08:04      False   \n",
       "2020-01-04 08:23:03      False   \n",
       "\n",
       "                                                                  user  \\\n",
       "dateTime                                                                 \n",
       "2020-01-01 11:10:10  {'created_at': 'Mon Aug 29 16:48:30 -0400 2011...   \n",
       "2020-01-01 12:08:45  {'created_at': 'Sat Nov 24 15:43:45 -0500 2012...   \n",
       "2020-01-03 09:42:22  {'created_at': 'Mon Jul 19 04:38:21 -0400 2010...   \n",
       "2020-01-03 17:08:04  {'created_at': 'Wed Aug 06 15:51:01 -0400 2008...   \n",
       "2020-01-04 08:23:03  {'created_at': 'Sun May 01 11:50:43 -0400 2016...   \n",
       "\n",
       "                            user_id_str user_screen_name    neg    neu  pos  \\\n",
       "dateTime                                                                      \n",
       "2020-01-01 11:10:10           364476368    XavierAbadMdG  0.165  0.835  0.0   \n",
       "2020-01-01 12:08:45           968761746       RossellRos  0.204  0.796  0.0   \n",
       "2020-01-03 09:42:22           168425731         Agenparl  0.158  0.842  0.0   \n",
       "2020-01-03 17:08:04            15754217    ironorehopper  0.137  0.863  0.0   \n",
       "2020-01-04 08:23:03  726801029963108352   Azeria64Azeria  0.224  0.776  0.0   \n",
       "\n",
       "                    compound  \n",
       "dateTime                      \n",
       "2020-01-01 11:10:10  -0.6289  \n",
       "2020-01-01 12:08:45  -0.6289  \n",
       "2020-01-03 09:42:22  -0.4939  \n",
       "2020-01-03 17:08:04  -0.2263  \n",
       "2020-01-04 08:23:03  -0.3818  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.sort_values(by=\"created_at\", inplace=True)\n",
    "\n",
    "if fileType == \".csv\":\n",
    "    cdf[\"created_at\"] = cdf[\"created_at\"].astype(str).str[:-6]\n",
    "else:\n",
    "    None\n",
    "\n",
    "cdf[\"created_at\"] = pd.to_datetime(cdf[\"created_at\"])\n",
    "cdf = cdf.dropna(subset=[\"created_at\"])\n",
    "cdf.index = cdf[\"created_at\"]\n",
    "cdf.index.names = [\"dateTime\"]\n",
    "cdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create two separate data frames and export data\n",
    "Now we get the mean for the sentiment compound score by date and assign the results to a new data frame called \"meanDF\".\n",
    "\n",
    "Then we name the two columns in the new data frame. The names are \"date\" and \"mean\".\n",
    "\n",
    "Next we export the data frame as a \".csv\" file. This \".csv\" will be used in the \"vaderAverageOverTimeResults\" notebook to create the interactive line graph displaying our results.\n",
    "\n",
    "Then we look at the first five rows of the data frame to make sure it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>-0.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>-0.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>-0.278133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>-0.888500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      mean\n",
       "0 2020-01-01 -0.628900\n",
       "1 2020-01-02  0.000000\n",
       "2 2020-01-03 -0.360100\n",
       "3 2020-01-04 -0.278133\n",
       "4 2020-01-05 -0.888500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanDF = cdf.groupby(pd.Grouper(freq = timeLength))[\"compound\"].mean().fillna(0).sort_index().reset_index()\n",
    "meanDF.columns = [\"date\", \"mean\"]\n",
    "meanDF.to_csv(os.path.join(dataClean, \"vaderAvg.csv\"))\n",
    "meanDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the number of tweets per date and assign the results to a new data frame called \"countDF\".\n",
    "\n",
    "Then we name the two columns in the new data frame. The names are \"date\" and \"count\".\n",
    "\n",
    "Next we export the data frame as a \".csv\" file. This \".csv\" will be used in the \"vaderAverageOverTimeResults\" notebook to create the interactive line graph displaying our results.\n",
    "\n",
    "Then we look at the first five rows of the data frame to make sure it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  count\n",
       "0 2020-01-01      2\n",
       "1 2020-01-02      0\n",
       "2 2020-01-03      2\n",
       "3 2020-01-04      3\n",
       "4 2020-01-05      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countDF = cdf.groupby(pd.Grouper(freq = timeLength))[\"compound\"].count().sort_index().reset_index()\n",
    "countDF.columns = [\"date\", \"count\"]\n",
    "countDF.to_csv(os.path.join(dataClean, \"tweetCount.csv\"))\n",
    "countDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## VOILA!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
