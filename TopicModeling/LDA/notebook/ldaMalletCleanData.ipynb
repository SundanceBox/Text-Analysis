{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA) and MALLET\n",
    "\n",
    "The following notebook walks you through doing LDA topic modeling in Python using the Gensim package MALLET wrapper. We then export the LDA model and cleaned data so it can be used in our \"ldaMalletResultsBarGraph\" or \"ldaMalletResultsWC\" notebooks to create tables and graphs (ldaMalletResultsBarGraph), or tables and wordclouds (ldaMalletResultsWC) of your LDA topics for analysis. We do this to improve the reproducability of your results and increase efficiency by eliminating the need to repeat the entire process (which will create a slightly different model) if you decide you need to make changes to the visual output, but do not want the LDA model and topics to change. This notebook is where you would make changes that will impact the topics produced.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "Before we start, you will need to have set up a [Carbonate account](https://kb.iu.edu/d/aolp) in order to access [Research Desktop (ReD)](https://kb.iu.edu/d/apum). You will also need to have access to ReD through the [thinlinc client](https://kb.iu.edu/d/aput). If you have not done any of this, or have only done some of this, but not all, you should go to our [textPrep-Py.ipynb](https://github.com/cyberdh/Text-Analysis/blob/drafts/textPrep-Py.ipynb) before you proceed further. The textPrep-Py notebook provides information and resources on how to get a Carbonate account, how to set up ReD, and how to get started using the Jupyter Notebook on ReD.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CyberDH environment\n",
    "The code in the cell below points to a Python environment specificaly for use with the Python Jupyter Notebooks created by Cyberinfrastructure for Digital Humanities. It allows for the use of the different pakcages in our notebooks and their subsequent data sets.\n",
    "\n",
    "##### Packages\n",
    "- **sys:** Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "#### NOTE: This cell is only for use with Research Desktop. You will get an error if you try to run this cell on your personal device!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,\"/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages\")\n",
    "os.environ[\"NLTK_DATA\"] = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others created by Python users are available for download.\n",
    "\n",
    "In your terminal, packages can be installed by typing `pip install nameofpackage --user`. However, since you are using ReD and our Python environment, you will not need to install any of the packages below to use this notebook. Anytime you need to make use of a package, however, you need to import it so that Python knows to look in these packages for any functions or commands you use. Below is a brief description of the packages we are using in this notebook:  \n",
    "\n",
    "- **re:** Provides regular expression matching operations similar to those found in Perl.\n",
    "- **nltk:** A leading platform for building Python programs to work with human language data.\n",
    "- **glob:** Finds all the pathnames matching a specified pattern according to the rules used by the Unix shell. \n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- **pprint:** Provides a capability to “pretty-print” arbitrary Python data structures in a form which can be used as input to the interpreter.\n",
    "- **collections:** Implements specialized container datatypes providing alternatives to Python's general purpose built-in containers, dict, list, set, and tuple.\n",
    "- **matplotlib:**  Produces publication quality 2D graphics for interactive graphing, scientific publishing, user interface development and web application servers targeting multiple user interfaces and hardcopy output formats.\n",
    "- **pickle:** Implements binary protocols for serializing and de-serializing a Python object structure. \"Pickling\" is the process whereby a Python object hierarchy is converted into a byte stream, and \"unpickling\" is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy.\n",
    "- **itertools:** Standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination. Together, they form an “iterator algebra” making it possible to construct specialized tools succinctly and efficiently in pure Python.\n",
    "- **zipfile:** Allows for handling of zipfiles.\n",
    "- **tarfile:** Makes it possible to read and write tar archives, including those using gzip, bz2 and lzma compression.\n",
    "- **gensim:** Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n",
    "- **spacy:** A library for advanced Natural Language Processing in Python and Cython.\n",
    "- **logging:** Defines functions and classes which implement a flexible event logging system for applications and libraries.\n",
    "- **warnings:** Allows for the manipulation of warning messages in Python.\n",
    "\n",
    "Notice we import some of the packages differently. In some cases we just import the entire package when we say `import XYZ`. For some packages which are small, or, from which we are going to use a lot of the functionality it provides, this is fine. \n",
    "\n",
    "Sometimes when we import the package directly we say `import XYZ as X`. All this does is allow us to type `X` instead of `XYZ` when we use certain functions from the package. So we can now say `X.function()` instead of `XYZ.function()`. This saves time typing and eliminates errors from having to type out longer package names. I could just as easily type `import XYZ as potato` and whenever I use a function from the `XYZ` package I would need to type `potato.function()`. What we import the package as is up to you, but some commonly used packages have abbreviations that are standard amongst Python users such as `import pandas as pd` or `import matplotlib.pyplot as plt`. You do not need to us `pd` or `plt`, however, these are widely used and using something else could confuse other users and is generally considered bad practice. \n",
    "\n",
    "Other times we import only specific elements or functions from a package. This is common with packages that are very large and provide a lot of functionality, but from which we are only using a couple functions or a specific subset of the package that contains the functionality we need. This is seen when we say `from XYZ import ABC`. This is saying I only want the `ABC` function from the `XYZ` package. Sometimes we need to point to the specific location where a function is located within the package. We do this by adding periods in between the directory names, so it would look like `from XYZ.123.A1B2 import LMN`. This says we want the `LMN` function which is located in the `XYZ` package and then the `123` and `A1B2` directory in that package. \n",
    "\n",
    "You can also import more than one function from a package by separating the functions with commas like this `from XYZ import ABC, LMN, QRS`. This imports the `ABC`, `LMN` and `QRS` functions from the `XYZ` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools as it\n",
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Import warning\n",
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give more details regarding error messages and will also ignore deprecation and user warnings. All the deprecation and user warnings in this code are not concerning and will not break the code or cause errors in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File paths\n",
    "Here we are saving as variables different file paths that we need in our code. We do this so that they are easier to call later and so that you can make most of your changes now and not need to make as many changes later. \n",
    "\n",
    "First we use the `os` package above to find our `[HOME]` directory using the `environ` function. This will work for any operating system, so if you decide to try this out on your personal computer instead of ReD, the `homePath` variable will still be the path to your \"home\" directory, so no changes are needed.\n",
    "\n",
    "Next, we combine the `homePath` variable with the folder names that lead to where our data is stored. Note that we do not use any file names yet, just the path to the folder. This is because we may want to read in all the files in the directory, or just one file. There are options below for doing both. We save the path as a variable named `dataHome`.\n",
    "\n",
    "Now we assign paths we may not need, depending on our data. We assign these paths to `csvPath` and `jsonPath`. If we have twitter data in a .csv file, we will need to adjust the path assigned to `csvPath` to point to where our .csv twitter data resides. Just the folder, not the actual file. If our twitter data is in a .json file, then we need to adjust the path assigned to `jsonPath` so it points to the folder our .json twitter data resides. If our data is in .txt format, then do not worry about either of these variables.\n",
    "\n",
    "Next, since we are using MALLET to do our LDA, you need to [download](http://mallet.cs.umass.edu/download.php) the MALLET zipfile, unzip it and provide the path to the extracted folder. We recommend saving the extracted folder in your \"Carbonate\" directory which is also your \"home\" directory. This way you will not need to adjust anything in the last line as it should point to the folder needed to run MALLET. We save this file path as the variable `malletPath`.\n",
    "\n",
    "Then we join the file path assigned to the `homePath` variable to another file path that leads to a folder for our LDA model, LDA dictionary, and other needed data and assign it to the variable `cleanDataPath`.\n",
    "\n",
    "Now, we join the file path assigned to the `cleanDataPath` variable to other file paths to point to what we want to name our output files for our cleaned data (`cleanData`), our LDA dictionary (`cleanDict`), our LDA model (`cleanModel`), and our original data uncleaned and converted to a list (`origData`). If you wish to name the files something else, then change the name in quotes in all of the variables just listed.\n",
    "\n",
    "Lastly, we assign a file path to the `cleanDF` variable. Here we may store a .csv file later in the code depending on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homePath = os.environ[\"HOME\"]\n",
    "dataHome = os.path.join(homePath, \"Text-Analysis-master\", \"data\")\n",
    "csvPath = os.path.join(dataHome, \"twitter\", \"CSV\")\n",
    "jsonPath = os.path.join(dataHome, \"twitter\", \"JSON\")\n",
    "malletPath = os.path.join(dataHome,\"mallet-2.0.8\", \"bin\", \"mallet\") # update this path\n",
    "cleanDataPath = os.path.join(homePath, \"Text-Analysis-master\", \"TopicModeling\", \"LDA\", \"cleanedData\", \"malletModel\")\n",
    "cleanData = os.path.join(cleanDataPath, \"ldaDataClean\")\n",
    "cleanDict = os.path.join(cleanDataPath, \"ldaDict\")\n",
    "cleanModel = os.path.join(cleanDataPath, \"ldaModel\")\n",
    "origData = os.path.join(cleanDataPath, \"ldaDataOrig\")\n",
    "cleanDF = os.path.join(homePath, \"Text-Analysis-master\", \"TopicModeling\", \"LDA\", \"cleanedCSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set needed variables\n",
    "This is where you will make some decisions about your data and set the necessary variables. Much like the file path variables above, we do this so you do not need to make as many changes later.\n",
    "\n",
    "**source**<br>\n",
    "First, we need to decide if we want our code to read all the files in a directory or just a single file. If we want all the files in a directory then we set `source` equal to `\"*\"`. This means 'all' and will be added to the file type later in the code. If you want a single file change `\"*\"` to the file name without the \".txt\" or \".csv\" or \".json\" at the end. So if you have a file named \"myFile.txt\" you would set `source` equal to `\"myFile\"` without the \".txt\".\n",
    "\n",
    "**fileType**<br>\n",
    "Next we assign the file type our data comes in to a variable. At the moment the only options are \".txt\", \".csv\" or \".json\". The \".txt\" format is the most popular format for analysis of a text or corpus, while \".csv\" and \".json\" are the most common formats for twitter data. We assign the format to the `fileType` variable. It should look like this: `fileType = \".txt\"`.\n",
    "\n",
    "**chunkLevel**<br>\n",
    "The `chunkLevel` variable is only for file types of \".txt\" so if you have a \".csv\" or \".json\" you want to assign `None` to `chunkLevel` or it will cause problems in other parts of the code later since we do not keep track of file names for the \".csv\" and \".json\" files. If your data is in \".txt\" format, then you need to determine if you want to chunk your corpus by lines, files, or directories.\n",
    "\n",
    "We do this in case your data is a single \".txt\" file, multiple text files, or multiple directories of text files. The LDA algorithm needs to have multiple chunks to accurately weigh and order words into topics. If you have multiple documents then the documents themselves are the chunks. If you have a single document, then we need to create chunks, and we do this by spliting the document up by line and each line is a separate chunk.\n",
    "\n",
    "If you want to separate by individual files, then set `chunkLevel` equal to `\"files\"`(with the quotes). If you want to separate a line at a time and have each line be it's own entity or 'chunk' then set `chunkLevel` equal to `\"lines\"` (again, with the quotes). If you have multiple directories, each containing multiple .txt files and you want to use the directories as your \"chunks\" then assign `\"direct\"` (with the quotes) to `chunkLevel`. For example, if you have a lot of different authors, and you have a directory for each author containing that particular author's works and you want to use LDA to see which authors tended to cover similar topics or what topics were prevelant amongst the particualr group of authors, you would assign `\"direct\"` to `chunkLevel`. On another note, if you set `source` equal to `\"*\"` then you will want to set `chunkLevel` equal to `\"files\"` or `\"direct\"`. If you set `source` equal to a specific file name, then you will want to set `chunkLevel` equal to `\"lines\"`. \n",
    "\n",
    "**n**<br>\n",
    "The `n` variable is where you assign the number of lines you would like to chunk the document into if you set `chunkLevel` equal to `\"lines\"`. This variable is not used if `chunkLevel` is `\"files\"` or `\"direct\"`.\n",
    "\n",
    "**nltkStop**<br>\n",
    "The `nltkStop` is where you determine if you want to use the built in stopword list provided by the NLTK package. They provide stopword lists in multiple languages. If you wish to use this then set `nltkStop` equal to `True`. If you do not, then set `nltkStop` equal to `False`.\n",
    "\n",
    "**customStop**<br>\n",
    "`customStop` is for if you have a .txt file that contains additional stopwords that you would like to read in and have added to the existing `stopWords` list. You do *NOT* need to use the NLTK stopwords list in order to add your own custom list of stopwords. **NOTE: Your custom stopwords file needs to have one word per line as it reads in a line at a time and the full contents of the line is read in and added to the existing stopWords list.** If you have a list of your own then set `customStop` equal to `True`. If you do not have your own custom stopwords list then set `customStop` equal to `False`.\n",
    "\n",
    "**spacyLem**<br>\n",
    "`spacyLem` is where we decide if we want to use the spaCy package lemmatization function. What is lemmatization? Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, where as stemming does not take the context of the word into account. For example, if we lemmatize the word \"running\" or \"ran\" it will become the word \"run\". If we stem the word \"running\" most stemmers will convert it to \"runn\" only removing the \"ing\" and leaving the second \"n\". Stemming will also change \"police\" and \"policy\" both to \"polic\" and they will be considered the same word by the LDA script. The lemmatizer will leave both words as \"police\" and \"policy\". This is useful and recommended for topic modeling as it allows the algorithm to just consider \"walk\" instead of \"walking\", \"walked\", and \"walk\" and thereby can increase the accuracy of your results. To use the spacy lemmatizer assign `True` to `spacyLem`. If you do not wish to use the lemmatizer assign `False` to `spacyLem`.\n",
    "\n",
    "**coherence**<br>\n",
    "`coherence` is where you decide if you want to calculate the coherence score for not just the default 20 topics, but if you want to calculate the coherence score beginning with 10 topics and going up by 10 until you get to 80 topics. This step takes a LONG time as you are running LDA on your corpus at least eight times and also running the coherence algorithm eight times as well. The idea behind the coherence score is that the higher the score, the more \"coherent\" or \"understandable\" your topics should be. This step is meant to help you determine the number of topics that are most likely to help make sense of your data. Again, this is a computer algorithm and so is not always right about what makes sense to humans, so we have given you the option to run all the coherence scores, or just stick with the default number of topics. If you want to see all the coherence scores assign `True` to `coherence`. If you want just the default 20 topics and do not need to see a coherence score, assign `False` to `coherence`. **NOTE:** If you assign `True` to `coherence` the code default is to automatically apply the number of topics with the highest coherence score to the LDA algorithm. If you want to see the coherence scores, but want to decide on your own which number of topics to apply to the LDA algorithm, you will need to make changes the appropriate cell of code below (don't worry, there is a note letting you know which cell it is when you get to it).\n",
    "\n",
    "**stopLang**<br>\n",
    "Now we choose the language we will be using for the nltk stopwords list. If you need a different language, simply change `\"english\"` (keep the quotes) in the `stopLang` variable to the anglicized name of the language you wish to use (e.g. \"spanish\" instead of \"espanol\" or \"german\" instead of \"deutsch\"). For a list of available stopword languages in nltk add a new code cell and type `print(stopwords.fileids())` and the list of available languages will print out below the cell.\n",
    "\n",
    "**lemLang**<br>\n",
    "Now we choose the language for our lemmatizer. The languages available for spacy include the list below and the abbreviation spacy uses for that language:\n",
    "\n",
    "- **English:** `\"en\"`\n",
    "- **Spanish:** `\"es\"`\n",
    "- **German:** `\"de\"`\n",
    "- **French:** `\"fr\"`\n",
    "- **Italian:** `\"it\"`\n",
    "- **Portuguese:** `\"pt\"`\n",
    "- **Dutch:** `\"nl\"`\n",
    "- **Multi-Language:** `\"xx\"`\n",
    "\n",
    "To choose a language simply type the two letter code following the angliscized language name in the list above. So for Spanish it would be `\"es\"` (with the quotes) and for German `\"de\"` and so on.\n",
    "\n",
    "**encoding, errors**<br>\n",
    "The variable `encoding` is where you determine what type of encoding to use (ascii, ISO-8850-1, utf-8, etc...). We have it set to `\"utf-8\"` at the moment as we have found it is less likely to have any problems. However, errors do occur, but the encoding errors rarely impact our results and it causes the Python code to exit. So instead of dealing with unhelpful errors we ignore the ones dealing with encoding by assigning `\"ignore\"` to the `errors` variable. If you want to see any encoding errors then change `\"ignore\"` to `None` (without quotes).\n",
    "\n",
    "**textColIndex**<br>\n",
    "The `textColIndex` variable is only applicable if our `fileType` is \".csv\" or \".json\". The `textColIndex` variable is where we put the header name of the dataframe column that will contain the content we are interested in from our tweets. Generally the content of the tweets are labeled as \"text\" since this is the label given to the tweet content when it is pulled directly from the Twitter API. For this reason our default value assigned to the `textColIndex` is `\"text\"`. If for some reason the tweet content has a different label or header, and you need to change this, remember to keep the quotes around the new label.\n",
    "\n",
    "**stopWords, docs**<br>\n",
    "The `stopWords =[]` variable is simply an empty list. This is where the words from the nltk stopword list or your custom stopword list or both combined or neither (depending on what you decide) will reside later on. You do not need to do anything to this line of code.\n",
    "\n",
    "The `docs = []` variable also does not need to have anything done to it as it is also an empty list that will be added to later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = \"*\"\n",
    "fileType = \".txt\"\n",
    "chunkLevel = \"files\"\n",
    "n = 100\n",
    "nltkStop = True\n",
    "customStop = True\n",
    "spacyLem = True\n",
    "coherence = True\n",
    "stopLang = \"english\"\n",
    "lemLang = \"en\"\n",
    "encoding = \"utf-8\"\n",
    "errors = \"ignore\"\n",
    "textColIndex = \"text\"\n",
    "stopWords = []\n",
    "docs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "If you assigned `True` to `nltkStop` above then this will add the NLTK stopwords list to the empty list named `stopWords`.\n",
    "\n",
    "You already chose your desired language above, so you do not need to do that now. \n",
    "\n",
    "If you need to add a few more words to the `stopWords` list that are specific to your dataset (such as common names or phrases that may make your results inaccurate), then add those to the `stopWords.extend(['would', 'said', 'says', 'also'])` part of the code in the square brackets with single quotes around each word and separated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "if nltkStop is True:\n",
    "    stopWords.extend(stopwords.words(stopLang))\n",
    "    stopWords.extend(['would', 'said', 'says', 'also'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add own stopword list\n",
    "\n",
    "Here is where your own stopwords list is added if you assigned `True` to `customStop` above. Here you will need to change the folder names and file name to match your folders and file. Remember to put each folder name in quotes and in the correct order always putting the file name including the file extension (.txt) last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if customStop is True:\n",
    "    stopWordsFilepath = os.path.join(homePath, \"Text-Analysis-master\", \"data\", \"earlyModernStopword.txt\")\n",
    "\n",
    "    with open(stopWordsFilepath, \"r\",encoding = encoding) as stopfile:\n",
    "        stopWordsCustom = [x.strip() for x in stopfile.readlines()]\n",
    "\n",
    "    stopWords.extend(stopWordsCustom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzip folders\n",
    "\n",
    "Here we are unzipping files. Since twitter data can be rather large it is often necessary to compress it into a '.zip' file in order to upload it to places such as GitHub. For this reason, we have setup some code to go in and automatically extract all the items in a compressed '.zip' file so you don't have to and so you don't get errors later. If the data is not in a '.zip' file there is no need to worry, it will not give an error if there are no files ending in '.zip' in your directory.\n",
    "\n",
    "We also extract the mallet data here and save it to our data folder for use further down in the code.\n",
    "\n",
    "You should not need to make any changes as we use the same variables containing our file paths as above, so if you need to make adjustments to the file paths, you need to make them there, specifically to the `dataHome` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if fileType == \".csv\":\n",
    "    allZipFiles = glob.glob(os.path.join(csvPath,\"*.zip\"))\n",
    "    for item in allZipFiles:\n",
    "        fileName = os.path.splitext(csvPath)[0]\n",
    "        zipRef = zipfile.ZipFile(item, \"r\")\n",
    "        zipRef.extractall(fileName)\n",
    "        zipRef.close()\n",
    "        os.remove(item)\n",
    "if fileType == \".json\":\n",
    "    allZipFiles = glob.glob(os.path.join(jsonPath,\"*.zip\"))\n",
    "    for item in allZipFiles:\n",
    "        fileName = os.path.splitext(jsonPath)[0]\n",
    "        zipRef = zipfile.ZipFile(item, \"r\")\n",
    "        zipRef.extractall(fileName)\n",
    "        zipRef.close()\n",
    "        os.remove(item)\n",
    "\n",
    "if not glob.glob(os.path.join(dataHome,\"*.tar.gz\")):\n",
    "    None\n",
    "else:\n",
    "    fname = \"mallet-2.0.8.tar.gz\"\n",
    "    if fname.endswith(\".tar.gz\"):\n",
    "        tar = tarfile.open(os.path.join(dataHome, fname), \"r:gz\")\n",
    "        tar.extractall(dataHome)\n",
    "        tar.close()\n",
    "        os.remove(os.path.join(dataHome, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in .txt files\n",
    "The code below reads in text files if you chose fileType `\".txt\"` above. It can do this in three ways. We can read in single file, a single directory, or we can read in multiple directories and it will do these based on what you chose for `source` above. Then it will chunk your data, either by line, document, or by directory, and this will depend on what you chose for `chunkLevel` above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if fileType == \".txt\":\n",
    "    if chunkLevel == \"files\":\n",
    "        paths = glob.glob(os.path.join(dataHome, \"shakespeareFolger\", source + fileType))\n",
    "        for path in paths:\n",
    "            with open(path, \"r\", encoding = encoding, errors = errors) as file:\n",
    "                 # skip hidden file\n",
    "                if path.startswith('.'):\n",
    "                    continue\n",
    "                docs.append(file.read().strip('\\n').splitlines())\n",
    "                \n",
    "    elif chunkLevel == \"lines\":\n",
    "        paths = glob.glob(os.path.join(dataHome, \"shakespeareFolger\", source + fileType))\n",
    "        for path in paths:\n",
    "            with open(path, \"r\", encoding = encoding, errors = errors) as file:\n",
    "                 # skip hidden file\n",
    "                if path.startswith('.'):\n",
    "                    continue\n",
    "                for line in file:\n",
    "                    stripLine = line.strip()\n",
    "                    if len(stripLine) == 0:\n",
    "                        continue\n",
    "                    docs.append(stripLine.split())\n",
    "    elif chunkLevel == \"direct\":\n",
    "        folderTextCSV = \"folderText.csv\"\n",
    "        paths = []\n",
    "        txt = []\n",
    "        dataPath = os.path.join(dataHome, \"starTrek\")\n",
    "        for folder in sorted(os.listdir(dataPath)):\n",
    "            if not os.path.isdir(os.path.join(dataPath, folder)):\n",
    "                continue\n",
    "            for file in sorted(os.listdir(os.path.join(dataPath, folder))):\n",
    "                paths.append(((dataPath, folder, file)))\n",
    "        df = pd.DataFrame(paths, columns = [\"Root\", \"Folder\", \"File\"])\n",
    "        df[\"Paths\"] = df[\"Root\"].astype(str) +\"/\" + df[\"Folder\"].astype(str) + \"/\" + df[\"File\"].astype(str)\n",
    "        for path in df[\"Paths\"]:\n",
    "            if not path.endswith(\".txt\"):\n",
    "                continue\n",
    "            with open(path, \"r\", encoding = encoding, errors = errors) as f:\n",
    "                t = f.read().strip().split()\n",
    "                txt.append(t)\n",
    "        df[\"Text\"] = pd.Series(txt)\n",
    "        df[\"Text\"] = [\"\".join(map(str, l)) for l in df[\"Text\"].astype(str)]\n",
    "        d = {'Text':'merge'}\n",
    "        dfText = df.groupby(['Folder'])[\"Text\"].apply(lambda x: ' '.join(x)).reset_index()\n",
    "        \n",
    "        docs.extend(dfText[\"Text\"].tolist())\n",
    "        dfText.to_csv(os.path.join(cleanDF, folderTextCSV))\n",
    "    else:\n",
    "        None\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in .csv and .json files\n",
    "\n",
    "If you chose `\".csv\"` as your `fileType` up above, then the first `if` statement in the code below reads in \".csv\" files and saves the contents to a dataframe using the Pandas package. It will read in either an entire directory or a single \".csv\" file depending on what you chose for `source` above. \n",
    "\n",
    "Once we have read in the \".csv\" file using the Pandas `read_csv` function, we need to concatenate the \".csv\" files if there are multiple. Because of this it is important that your \".csv\" files have an identical column count and each column has identical header names or you will get errors. If you have a single \".csv\" file then you should be fine for this step. We assign this process to the variable `cdf` so we can use it later.\n",
    "\n",
    "Now we convert our `cdf` to a pandas dataframe. This allows for easier manipulation of the data in the next line.\n",
    "\n",
    "Finally, we pull in the column containing the data we are interested in which we assigned to the variable `textColIndex` earlier and turn it into a list assigned to the variable `tweets`.\n",
    "\n",
    "If you chose `\".json\"` for your fileType, then the second `if` statement will read in \".json\" files and save the content to a dataframe using the Pandas package much like the \".csv\" file process described above. The only difference is that we use the Pandas function `read_json` instead of `read_csv`. Everything else is exactly the same as what is described above in the \".csv\" section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if fileType == \".csv\":\n",
    "    allFiles = glob.glob(os.path.join(csvPath, source + fileType))     \n",
    "    df = (pd.read_csv(f, engine = \"python\") for f in allFiles)\n",
    "    cdf = pd.concat(df, ignore_index=True)\n",
    "    cdf = pd.DataFrame(cdf, dtype = 'str')\n",
    "    tweets = cdf[textColIndex].values.tolist()\n",
    "if fileType == \".json\":\n",
    "    allFiles = glob.glob(os.path.join(jsonPath, source + fileType))     \n",
    "    df = (pd.read_json(f, encoding = encoding, lines = True) for f in allFiles)\n",
    "    cdf = pd.concat(df, ignore_index=True)\n",
    "    cdf = pd.DataFrame(cdf, dtype = 'str')\n",
    "    tweets = cdf[textColIndex].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data variable\n",
    "\n",
    "Now we need to change our variable containing our data (either docs or tweets from above) to the variable `data` since this is the variable used going forward and it saves you from having to switch between `tweets` and `docs` later in the code. If you read in \".csv\" or \".json\" files then your data is saved in the `tweets` list and if you read in \".txt\" files then it is in the `docs` list. This code says if the length of the `docs` list is greater than 0 then assign `docs` to the variable data. However, if the length of `tweets` is greater than 0 then assign `tweets` to the variable `data`.\n",
    "\n",
    "If your data was in `docs` and was from a single document, then you may want to chunk the data by lines. The \"if...else\" statement says if `chunkLevel` is `\"files\"` or `\"direct\"` then `data = docs`. If `chunkLevel` is `\"lines\"` then we chunk the document by `n` lines with `n` being the number we assigned to that variable up above.\n",
    "\n",
    "If your data was in `tweets` then it most likely needs some additional cleaning. So the next chunk of code removes URLS and new line characters from the `data` variable if the length of `tweets` is greater than 0.\n",
    "\n",
    "The last line prints out the length of our collection. If you are reading in a single document this will print the number of chunks created by considering every `n` lines a chunk. If you chunked by files then it will print out the number of files, if you chunked by directory than the number of directories. If you are reading in tweets than it will print out the number of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "if len(docs) > 0:\n",
    "    if chunkLevel == \"files\" or \"direct\":\n",
    "        data = docs\n",
    "    elif chunkLevel == \"lines\":\n",
    "        data = []\n",
    "        for i in list(it.zip_longest(*(iter(docs),)*n)):\n",
    "            data.append(i)\n",
    "    else:\n",
    "        None\n",
    "elif len(tweets) > 0:\n",
    "    data = tweets\n",
    "    # Remove Urls\n",
    "    data = [re.sub(r'http\\S+', '', sent) for sent in data]\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "else:\n",
    "    None\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "This block of code separates each chunk of text into a list of individual words. In the process it also lower cases all the words and removes punctuation. If you wish to keep the punctuation change `deacc = True` to `deacc = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "def sentToWords(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "dataWords = list(sentToWords(data))\n",
    "\n",
    "print(len(dataWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Bigrams and Trigrams\n",
    "\n",
    "This code will most likely not need to be adjusted. It creates a model of bigrams and trigrams in your dataset that occur frequently and then connects them with an underscore so the LDA algorithm will later consider them as one word. This is a good idea for items like 'new york' or 'new zealand' or 'Ho Chi Minh'. If we do not combine these frequently occuring phrases then 'new' and 'york' will be considered independently and give us less accurate results.\n",
    "\n",
    "However, you may wish to adjust two settings depending on your data. The first is `min_count` and we assign a number to a variable `minCount` that will be used for `min_count` in the code. The other setting you might want to adjust is `threshold` and for this we assign a number to a variable `tHold` that will be used for `threshold` in the code.\n",
    "\n",
    "Right now we have a `min_count` of 5 and a `threshold` of 100. The `min_count` is simply the minimum number of times the bigram or trigram needs to occur in order to be combined with an underscore. The `threshold` is a score that the bigram or trigram needs to exceed in order to be combined with an underscore. The score is determined by using this formula: (bigram_count - min_count)\\*vocab_count/(wordA_count \\* wordB_count). So let's say we have the bigram \"good_lord\" and it appears 30 times in a text of 10,000 words where \"good\" appears 60 times total and \"lord\" appears 40. With our `min_count` set to 5 we get the following: (30 - 5)\\*10000/(60 \\* 40) = 104.167 which means since our `threshold` is set to 100 \"good_lord\" will be combined with an underscore and made into a bigram. If the resulting score is above your `threshold` then the ngram is considered important enough to combine with an underscore and will be viewed as one word for the LDA scoring later. Therefore, if you increase the `threshold`, you will get fewer bigrams and trigrams. If our threshold was set to 110, then \"good\" and \"lord\" would not be combined into \"good_lord\".\n",
    "\n",
    "The Phraser function takes the model you built with the Phrases function and cuts down memory consumption of Phrases, by discarding model state not strictly needed for the bigram detection task.\n",
    "\n",
    "Lastly, we take a look at the ngrams created from the first item in our dataset only, so the results are for only one chunk, not the whole dataset. We do this by counting the number of words that contain an underscore as this is used to connect the words in the ngram together. **NOTE:** The output is only to test if the ngrams work so you will probably see ngrams containing stopwords. We will create a few functions next and then apply them to remove stopwords, create bigrams, and lemmatize the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'poor_tom': 13,\n",
      "         'foul_fiend': 11,\n",
      "         'hundred_knights': 6,\n",
      "         'gav_st': 3,\n",
      "         'alack_alack': 3,\n",
      "         'take_heed': 2,\n",
      "         'good_morrow': 2,\n",
      "         'how_fares': 2,\n",
      "         'wast_born': 1,\n",
      "         'both_sides': 1,\n",
      "         'an_ass': 1,\n",
      "         'ha_ha_ha': 1,\n",
      "         'hundred_pound': 1,\n",
      "         'ha_ha': 1,\n",
      "         'cut_off': 1,\n",
      "         'heigh_ho': 1,\n",
      "         'fie_fie_fie': 1,\n",
      "         'looking_glass': 1})\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "minCount = 5\n",
    "tHold = 100\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = Phrases(dataWords, min_count=minCount, threshold=tHold) # higher threshold fewer phrases.\n",
    "trigram = Phrases(bigram[dataWords], threshold=tHold)  \n",
    "\n",
    "# Removes model state from Phrases thereby reducing memory use.\n",
    "bigramMod = Phraser(bigram)\n",
    "trigramMod = Phraser(trigram)\n",
    "\n",
    "# See bigram/trigram example\n",
    "testNgram = trigramMod[bigramMod[dataWords[0]]]\n",
    "char = \"_\"\n",
    "nGrams = [s for s in testNgram if char in s]\n",
    "            \n",
    "pprint(Counter(nGrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "We need to create a function in order to stem and tokenize our data. Any time you see `def` that means we are **DE**claring a **F**unction. The `def` is usually followed by the name of the function being created and then in parentheses are the parameters required by the function. After the parentheses is a colon, which closes the declaration, then a bunch of code below which is indented. The indented code is the program statement or statements to be executed. Once you have created your function all you need to do in order to run it is call the function by name and make sure you have included all the required parameters in the parentheses. This allows you to call the function without having to write out all the code in the function every time you wish to perform that task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions\n",
    "\n",
    "Below are functions we are creating that perform certain tasks. First we are creating a function to remove the stopwords that are in our stopword list we created previously. Then we create functions to apply our bigram and trigram code from above. \n",
    "\n",
    "Lastly, if you set `spacyLem` equal to `True` above then we will create the `lemmatization` function. If you set it equal to `False` then it will not create the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def removeStopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopWords] for doc in texts]\n",
    "\n",
    "def makeBigrams(texts):\n",
    "    return [bigramMod[doc] for doc in texts]\n",
    "\n",
    "def makeTrigrams(texts):\n",
    "    return [trigramMod[bigramMod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "if spacyLem is True:\n",
    "    def lemmatization(texts):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        textsOut = []\n",
    "        lemmaPOS = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            textsOut.append([token.lemma_ for token in doc if token.lemma_ != '-PRON-'])\n",
    "            lemmaPOS.append([token.text and token.lemma_ and token.pos_ for token in doc])\n",
    "        return textsOut\n",
    "        print(lemmaPOS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the functions. There are really only two parts where you may need to make changes and they are in the lines `dataWordsNgrams = makeBigrams(dataWordsNostops)` and `nlp = spacy.load(lemLang, disable=\\['parser', 'ner'\\])`.\n",
    "\n",
    "The `dataWordsNgrams` variable is where you will change between either using the `makeBigrams` or `makeTrigrams` functions above. If you only want bigrams, then keep the code as it is. If you want both bigrams and trigrams to be considered in your topic modeling, then change the `makeBigrams` part to `makeTrigrams` and it will now calculate both bigrams and trigrams. \n",
    "\n",
    "\n",
    "Adjustments to the other line that might need changes mentioned above may only be necessary if you previously set `spacyLem` equal to `True`. Even if you set it to `True` you may still not need to make changes. The line of code you may want to change is `nlp = spacy.load('lemLang', disable=\\['parser', 'ner'\\])` and is where you can disable the parser and named entity recognizer (ner). \n",
    "\n",
    "If you wish for your words to be parsed simply remove `'parser'` from the `disable=` bracket. Same for `ner`. If you wish to use both the parser and ner then just remove the `, disable=\\['parser', 'ner'\\]` entirely (including the preceding comma), but leave the closing parantheses. The reason we disable to 'parser' and 'ner' is because they slow down the lemmatization process and are not necessary to lemmatize our dataset.\n",
    "\n",
    "Lastly, we print out the ngrams we find in the first chunk (document or line) of our data. Notice there are no trigrams included. This is because we applied only the `makeBigrams` function from above. If we had applied the `makeTrigrams` function we would have both bigrams and trigrams. Feel free to change this in the code as described above. If we set `spacyLem` equal to `True` then we will get the first 10 words, their lemmatized form (which sometimes is identical to the word being lemmatized), with their parts of speech tagging from the `lemmatization` function above. Below this is a list of the lemmatized bigrams from the first chunk of our data. If we set it to `False` then we will get bigrams from the first chunk that have not been lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['thought', 'thought', 'PROPN'], ['king', 'king', 'PROPN'], ['affect', 'affect', 'VERB'], ['duke', 'duke', 'PROPN'], ['albany', 'albany', 'PROPN'], ['cornwall', 'cornwall', 'PROPN'], ['division', 'division', 'PROPN'], ['kingdom', 'kingdom', 'NOUN'], ['appear', 'appear', 'VERB'], ['duke', 'duke', 'PROPN']]\n",
      "Counter({'poor_tom': 13, 'foul_fiend': 11, 'gav_st': 3, 'alack_alack': 3, 'ha_ha': 2, 'good_morrow': 2, 'fie_fie': 2, 'wast_born': 1, 'heigh_ho': 1, 'looking_glass': 1})\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "dataWordsNostops = removeStopwords(dataWords)\n",
    "\n",
    "# Form Bigrams\n",
    "dataWordsNgrams = makeBigrams(dataWordsNostops)\n",
    "\n",
    "if spacyLem is True:\n",
    "    # Initialize spacy language model, eliminating the parser and ner components\n",
    "    nlp = spacy.load(lemLang, disable=['parser', 'ner'])\n",
    "    \n",
    "    # Do lemmatization tagging\n",
    "    \n",
    "    dataLemmatized = lemmatization(dataWordsNgrams)\n",
    "    lemmaPOS = []\n",
    "    for sent in dataLemmatized:\n",
    "        lemmaNLP = nlp(\" \".join(sent))\n",
    "        for token in lemmaNLP:\n",
    "            lemmaPOS.append([token.text, token.lemma_, token.pos_])\n",
    "    print(lemmaPOS[:10])\n",
    "    \n",
    "\n",
    "    # Find ngrams and count number of times they occur\n",
    "    dataNgrams = [s for s in dataLemmatized[0] if char in s]\n",
    "    \n",
    "else:\n",
    "    dataNgrams = [s for s in dataWordsNgrams[0] if char in s]\n",
    "print(Counter(dataNgrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dictionary and Corpus needed for Topic Modeling\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them.\n",
    "\n",
    "First, we need to determine if we are using lemmatized data or not. To do this we are using another `if` statement. Here, `if` we set `spacyLem` equal to `True` then we create the id2word dictionary based off of the lemmatized version of the data. However, if we did not, (denoted by `else`) then we will create the id2word dictionary based on the non-lemmatized data.\n",
    "\n",
    "Gensim creates a unique id for each word in the document. For example, (0, 1) implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if spacyLem is True:\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(dataLemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = dataLemmatized\n",
    "else:\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(dataWordsNgrams)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = dataWordsNgrams\n",
    "    \n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abuse'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you can see a human-readable form of the corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('abate', 1), ('abatement', 1), ('abhor', 1), ('abhorred', 1), ('abjure', 1), ('able', 1), ('abode', 1), ('abominable', 1), ('abroad', 2), ('absolute', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "hReadable = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "for i in hReadable:\n",
    "    print(i[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIND OPTIMAL NUMBER OF TOPICS\n",
    "\n",
    "If you set `coherence` to `False` above, then this cell is skipped. \n",
    "\n",
    "However, if you assigned `True` to `coherence` above, then we will try and find the optimal number of topics. To do this we build many LDA models with different values of number of topics and pick the one that gives the highest coherence value. We do this by creating the function `computeCoherenceValues`. You will most likley not want to make changes to the function itself. \n",
    "\n",
    "You may want to make changes to the variables that precede the function, namely `start`, `lmt`, and `steps`. The `start` variable is where you assign the number of topics you want to start with to determine the number of topics with the highest coherence score. Currently 10 is assigned to `start` the `start` parameter is inclusive, so it includes the number we assign to that parameter. The `lmt` variable is where you assign the number of topics at which you want to stop testing coherence scores. Notice that `lmt` is set to 81, not 80. This is because the `limit` parameter in the `computeCoherenceValues` function is exclusive, meaning it includes everything before that number, but not the number itself. The `steps` variable is where we assign the increments by which we go from `start` to `lmt`. Currently it is set to 10, so the algorithms will be applied every 10 topics from 10 to 80 with the current settings. Therefore, we should see coherence scores for 10, 20, 30, 40, 50, 60, 70, and 80 topics. \n",
    "\n",
    "Note that you want to make sure the assignments to `start`, `lmt`, and `steps` all add up. If I keep the numbers assigned to `start` and `lmt`, but change `steps` to 20, then the function will run MALLET and compute coherence scores for 10, 30, 50, and 70 topics, but will not run for 80 topics as it tries to go to 90, but stops because it has exceeded the 81 limit. However, the assigned `steps` do not allow it to run the algorithms for 80 topics, so it ends with 70.\n",
    "\n",
    "Feel free to adjust these variables to suit your corpus, just make sure the `steps` you count by make sense with assigned `start` and `lmt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if coherence is True:\n",
    "    #Variables\n",
    "    start = 10\n",
    "    lmt = 81\n",
    "    steps = 10\n",
    "    def computeCoherenceValues(dictionary, corpus, texts, limit, start=start, step=steps):\n",
    "        \"\"\"\n",
    "        Compute c_v coherence for various number of topics\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dictionary : Gensim dictionary\n",
    "        corpus : Gensim corpus\n",
    "        texts : List of input texts\n",
    "        limit : Max num of topics\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        modelList : List of LDA topic models\n",
    "        coherenceValues : Coherence values corresponding to the LDA model with respective number of topics\n",
    "        \"\"\"\n",
    "        coherenceValues = []\n",
    "        modelList = []\n",
    "        \n",
    "        for numTopics in range(start, limit, step):\n",
    "            model = gensim.models.wrappers.LdaMallet(malletPath, corpus=corpus, num_topics=numTopics, id2word=id2word, iterations = 1000, workers = 1, prefix=cleanDataPath, optimize_interval = 10, random_seed = 42)\n",
    "            modelList.append(model)\n",
    "            coherenceModel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherenceValues.append(coherenceModel.get_coherence())\n",
    "\n",
    "        return modelList, coherenceValues\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set `coherence` equal to `False` then this cell is skipped.\n",
    "\n",
    "If otherwise...\n",
    "\n",
    "The `computeCoherenceValues()` trains multiple LDA models and their corresponding coherence scores. In addition, if `True` was assigned to `spacyLem` then the algorithms are run on the lemmatized version of the corpus, if `False` was assigned to `spacyLem` then the algorithms are run on the non-lemmatized version of the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "if coherence is True:\n",
    "    if spacyLem is True:\n",
    "        modelList, coherenceValues = computeCoherenceValues(dictionary=id2word, corpus=corpus, texts=dataLemmatized, start=start, limit=lmt, step=steps)\n",
    "    else:\n",
    "        modelList, coherenceValues = computeCoherenceValues(dictionary=id2word, corpus=corpus, texts=dataWordsNgrams, start=start, limit=lmt, step=steps)\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set `coherence` equal to `True` we will want to visualize the coherence scores for each number of topics to help us decide how many topics we should use going forward.\n",
    "\n",
    "Choosing a number of topics that marks the end of a sharp increase in topic coherence scores usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics. If you see the same keywords being repeated in multiple topics, it’s probably a sign that the number of topics is too large. To choose the number of topics that might be best do not just use the highest coherence score. There are other factors that need to be considered. If the coherence score just seems to keep climbing higher and higher along with the number of topics, it might be best to find the number where it appears to stop and flatten out a bit before continuing to ascend. You do not want to run the risk of having too many or too few topics.\n",
    "\n",
    "However, further down we have the default number of topics to be the number of topics with the highest coherence score if `True` was assigned to `coherence`. If your coherence score results lead you to believe that the highest score is not the \"optimal\" number of topics, then you will want to change the number of topics to the number you think best exemplifies the recommendations for choosing the number of topics in the paragraph above when you get to that part of the code.\n",
    "\n",
    "If you set `coherence` equal to `False` then this cell is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0ouF94mZN6uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if coherence is True:\n",
    "    # Show graph\n",
    "    limit=lmt; start=start; step=steps;\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherenceValues)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherenceValues\"), loc='best')\n",
    "    plt.show()\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `coherence` was set to `False` then this cell is skipped.\n",
    "\n",
    "If otherwise...\n",
    "\n",
    "Now that we have a visual, it may help to go back and look at the actual numbers. The code below lists the coherence score for each number of topics we have selected as well as the number of topics which produced the highest coherance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics = 10  has Coherence Value of 0.3489\n",
      "Num Topics = 20  has Coherence Value of 0.3808\n",
      "Num Topics = 30  has Coherence Value of 0.3937\n",
      "Num Topics = 40  has Coherence Value of 0.3772\n",
      "Num Topics = 50  has Coherence Value of 0.4259\n",
      "Num Topics = 60  has Coherence Value of 0.4018\n",
      "Num Topics = 70  has Coherence Value of 0.3963\n",
      "Num Topics = 80  has Coherence Value of 0.3929\n",
      " \n",
      "Optimal Number of Topics is 50\n"
     ]
    }
   ],
   "source": [
    "if coherence is True:\n",
    "    # Print the coherence scores\n",
    "    cval = coherenceValues\n",
    "    maxCoVal = cval.index(max(cval))\n",
    "    mcv = round(maxCoVal, 4)\n",
    "    \n",
    "    for m, cv in zip(x, coherenceValues):\n",
    "        print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "    print(\" \")\n",
    "    print(\"Optimal Number of Topics is\", x[mcv])\n",
    "    \n",
    "        \n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING MALLET\n",
    "First we choose some important settings for our LDA model. \n",
    "If we assigned `True` to `coherence` then we want to make changes to the variables after `if coherence is True:`, but before `else:`. If we assigned `False` to the `coherence` variable, then we want to make our changes to the variables after `else:` in the cell below.\n",
    "\n",
    "If you assigned `True` to `coherence` then the number of topics with the highest coherence score is assigned to the `nTopics` variable. **NOTE: If you feel that a different number of topics is optimal, then change the default** `x[mcv]` **to the number of topics that works best for your corpus.** If you assigned `False` to the `coherence` variable the `nTopics` variable is assigned 20 as the default number of topics. Feel free to adjust this number as needed for your particular corpus.\n",
    "\n",
    "The `workers` variable is where we determine the number of nodes beings used. For ReD this number needs to stay at 1 as you are only allowed to use one node at a time. If you are using this on your own system, then do not exceed the number of cores on your computer.\n",
    "\n",
    "`nIter` is where you choose the number of training passes or iterations you want to make over the data. Here it is better to error on the side of the number being high. However, you want to balance this with any time constraints you might have. More iterations take more time. Note that the default is 1000, however, there are instances where this parameter has been set as low as 10. Feel free to play with this number as well to find an appropriate balance.\n",
    "\n",
    "`optInt` is where we set the number of iterations that take place before the algorithm checks what the optimal hyperparameters are. The hyperparameters are the alpha and beta levels. Alpha represents document-topic density and Beta represents topic-word density. If you raise the value of alpha, then the documents are composed of more topics and if you lower the value of alpha the documents are composed of less topics. If you raise the Beta number, the topics are composed of more words from the corpus, if you lower the Beta value the topics are composed of fewer words from the corpus. Currently, every 10 iterations the algorithm checks which alpha and Beta scores are most optimal. Feel free to change this and see the difference in your results.\n",
    "\n",
    "`seed` is where you set the seed. The seed helps with reproducability so if someone else runs the code on your dataset with these settings they should get the same answer. The seed basically tells the algorithm to start in the same spot, otherwise a random number generator chooses a different starting location each time. **Note:** the topics will be the same, but possibly in a different order even when you set the seed, so if you run it and get a set of words for topic 5, then if you run it again with the exact same settings on the exact same dataset what was topic 5 might now be topic 12, but the topic will contain the same words ordered by weight/importance. \n",
    "\n",
    "Gensim provides a wrapper to implement Mallet’s LDA from within Gensim itself. If you are running this on ReD and have saved the Text-Analysis-master folder in your home directory, then the default file path assigned to the `malletPath` variable should point to the version of MALLET that came with this repo. \n",
    "\n",
    "If you are running this on your own computer, then if you also saved this repo to your home directory the path assigned to the `malletPath` variable should point to MALLET. If you saved the repo somewhere other than your home directory then as long as you have [downloaded](http://mallet.cs.umass.edu/download.php) the MALLET zipfile, extracted it and provided the path to the extracted mallet folder in the `malletPath` variable in the cell where we assign file paths to variables towards the top, then you should be good.  \n",
    "\n",
    "Adjustments should be made according to your specific corpus.\n",
    "\n",
    "After the variables are two lines of code.\n",
    "\n",
    "The first line of code runs the MALLET algorithm using our desired number of topics, number of workers, number of iterations, and set seed.\n",
    "\n",
    "The final line of code says that we want to print the first three topics with the top ten words in each topic based on their weight. We do this to check that the algorithm is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(21,\n",
      "  '0.053*\"rome\" + 0.043*\"martius\" + 0.042*\"people\" + 0.027*\"voice\" + '\n",
      "  '0.022*\"city\" + 0.021*\"home\" + 0.020*\"tribune\" + 0.020*\"country\" + '\n",
      "  '0.020*\"coriolanus\" + 0.018*\"aufidius\"'),\n",
      " (49,\n",
      "  '0.044*\"jew\" + 0.032*\"antonio\" + 0.026*\"bassanio\" + 0.026*\"choose\" + '\n",
      "  '0.022*\"bond\" + 0.019*\"father\" + 0.018*\"lancelet\" + 0.016*\"lorenzo\" + '\n",
      "  '0.014*\"judge\" + 0.014*\"ring\"'),\n",
      " (11,\n",
      "  '0.084*\"romeo\" + 0.037*\"tybalt\" + 0.029*\"juliet\" + 0.026*\"nurse\" + '\n",
      "  '0.025*\"night\" + 0.023*\"love\" + 0.023*\"death\" + 0.020*\"art\" + '\n",
      "  '0.017*\"montague\" + 0.015*\"friar\"')]\n"
     ]
    }
   ],
   "source": [
    "if coherence is True:\n",
    "    #Variables\n",
    "    nTopics = x[mcv]\n",
    "    workers = 1\n",
    "    nIter = 1000\n",
    "    optInt = 10\n",
    "    seed = 42\n",
    "    \n",
    "    # Select the model and print the topics\n",
    "    optimalModel = LdaMallet(malletPath, corpus=corpus, num_topics=nTopics, id2word=id2word, iterations = nIter, workers = workers, prefix=cleanDataPath, optimize_interval = optInt, random_seed = seed, topic_threshold=0)\n",
    "    pprint(optimalModel.print_topics(num_words=10)[:3])\n",
    "else:\n",
    "    # Variables\n",
    "    nTopics = 20\n",
    "    workers = 1\n",
    "    nIter = 1000\n",
    "    optInt = 10\n",
    "    seed = 42\n",
    "\n",
    "    optimalModel = LdaMallet(malletPath, corpus=corpus, num_topics=nTopics, id2word=id2word, workers = workers, prefix = cleanDataPath, iterations = nIter, optimize_interval = optInt, random_seed = seed,topic_threshold=0)\n",
    "    pprint(optimalModel.print_topics(num_words=10)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save models, dictionary, and data\n",
    "\n",
    "Now we save our LDA model, our dictionary of unique words, our original data as a list of words, and a cleaned version of our data as a list of words to the folder and by the file names we chose up above in an earlier cell. \n",
    "\n",
    "To create tables and graphs that will help bring more clarity to your LDA topics use the \"ldaMalletResults\" notebook that is part of this text analysis repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimalModel.save(cleanModel)\n",
    "id2word.save(cleanDict)\n",
    "with open(origData, \"wb\") as sd:\n",
    "    pickle.dump(data, sd)\n",
    "with open(cleanData, \"wb\") as phrases:\n",
    "    pickle.dump(texts, phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ackowledgements: This algorithm was adapted from the blog \"Machine Learning Plus\". Reference: Machine Learning Plus. Topic Modeling with Gensim (Python). Retrieved from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ on November 5, 2018."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
