{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA) and MALLET\n",
    "\n",
    "The following notebook walks you through doing LDA topic modeling in Python using the Gensim package MALLET wrapper. We then export the LDA model and cleaned data so it can be used in our \"ldaMalletResultsBarGraph\" or \"ldaMalletResultsWC\" notebooks to create tables and graphs (ldaMalletResultsBarGraph), or tables and wordclouds (ldaMalletResultsWC) of your LDA topics for analysis. We do this to improve the reproducability of your results and increase efficiency by eliminating the need to repeat the entire process (which will create a slightly different model) if you decide you need to make changes to the visual output, but do not want the LDA model and topics to change. This notebook is where you would make changes that will impact the topics produced.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "Before we start, you will need to have set up a [Carbonate account](https://kb.iu.edu/d/aolp) in order to access [Research Desktop (ReD)](https://kb.iu.edu/d/apum). You will also need to have access to ReD through the [thinlinc client](https://kb.iu.edu/d/aput). If you have not done any of this, or have only done some of this, but not all, you should go to our [textPrep-Py.ipynb](https://github.com/cyberdh/Text-Analysis/blob/drafts/textPrep-Py.ipynb) before you proceed further. The textPrep-Py notebook provides information and resources on how to get a Carbonate account, how to set up ReD, and how to get started using the Jupyter Notebook on ReD.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CyberDH environment\n",
    "The code in the cell below points to a Python environment specificaly for use with the Python Jupyter Notebooks created by Cyberinfrastructure for Digital Humanities. It allows for the use of the different pakcages in our notebooks and their subsequent data sets.\n",
    "\n",
    "##### Packages\n",
    "- **sys:** Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "#### NOTE: This cell is only for use with Research Desktop. You will get an error if you try to run this cell on your personal device!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,\"/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages\")\n",
    "os.environ[\"NLTK_DATA\"] = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others created by Python users are available for download.\n",
    "\n",
    "In your terminal, packages can be installed by typing `pip install nameofpackage --user`. However, since you are using ReD and our Python environment, you will not need to install any of the packages below to use this notebook. Anytime you need to make use of a package, however, you need to import it so that Python knows to look in these packages for any functions or commands you use. Below is a brief description of the packages we are using in this notebook:  \n",
    "\n",
    "- **re:** Provides regular expression matching operations similar to those found in Perl.\n",
    "- **nltk:** A leading platform for building Python programs to work with human language data.\n",
    "- **glob:** Finds all the pathnames matching a specified pattern according to the rules used by the Unix shell. \n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- **pprint:** Provides a capability to “pretty-print” arbitrary Python data structures in a form which can be used as input to the interpreter.\n",
    "- **collections:** Implements specialized container datatypes providing alternatives to Python's general purpose built-in containers, dict, list, set, and tuple.\n",
    "- **matplotlib:**  Produces publication quality 2D graphics for interactive graphing, scientific publishing, user interface development and web application servers targeting multiple user interfaces and hardcopy output formats.\n",
    "- **pickle:** Implements binary protocols for serializing and de-serializing a Python object structure. \"Pickling\" is the process whereby a Python object hierarchy is converted into a byte stream, and \"unpickling\" is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy.\n",
    "- **itertools:** Standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination. Together, they form an “iterator algebra” making it possible to construct specialized tools succinctly and efficiently in pure Python.\n",
    "- **zipfile:** Allows for handling of zipfiles.\n",
    "- **gensim:** Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n",
    "- **spacy:** A library for advanced Natural Language Processing in Python and Cython.\n",
    "- **logging:** Defines functions and classes which implement a flexible event logging system for applications and libraries.\n",
    "- **warnings:** Allows for the manipulation of warning messages in Python.\n",
    "\n",
    "Notice we import some of the packages differently. In some cases we just import the entire package when we say `import XYZ`. For some packages which are small, or, from which we are going to use a lot of the functionality it provides, this is fine. \n",
    "\n",
    "Sometimes when we import the package directly we say `import XYZ as X`. All this does is allow us to type `X` instead of `XYZ` when we use certain functions from the package. So we can now say `X.function()` instead of `XYZ.function()`. This saves time typing and eliminates errors from having to type out longer package names. I could just as easily type `import XYZ as potato` and whenever I use a function from the `XYZ` package I would need to type `potato.function()`. What we import the package as is up to you, but some commonly used packages have abbreviations that are standard amongst Python users such as `import pandas as pd` or `import matplotlib.pyplot as plt`. You do not need to us `pd` or `plt`, however, these are widely used and using something else could confuse other users and is generally considered bad practice. \n",
    "\n",
    "Other times we import only specific elements or functions from a package. This is common with packages that are very large and provide a lot of functionality, but from which we are only using a couple functions or a specific subset of the package that contains the functionality we need. This is seen when we say `from XYZ import ABC`. This is saying I only want the `ABC` function from the `XYZ` package. Sometimes we need to point to the specific location where a function is located within the package. We do this by adding periods in between the directory names, so it would look like `from XYZ.123.A1B2 import LMN`. This says we want the `LMN` function which is located in the `XYZ` package and then the `123` and `A1B2` directory in that package. \n",
    "\n",
    "You can also import more than one function from a package by separating the functions with commas like this `from XYZ import ABC, LMN, QRS`. This imports the `ABC`, `LMN` and `QRS` functions from the `XYZ` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import itertools as it\n",
    "import zipfile\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Import warning\n",
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will give more details regarding error messages and will also ignore deprecation and user warnings. All the deprecation and user warnings in this code are not concerning and will not break the code or cause errors in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File paths\n",
    "Here we are saving as variables different file paths that we need in our code. We do this so that they are easier to call later and so that you can make most of your changes now and not need to make as many changes later. \n",
    "\n",
    "First we use the `os` package above to find our `[HOME]` directory using the `environ` function. This will work for any operating system, so if you decide to try this out on your personal computer instead of ReD, the `homePath` variable will still be the path to your \"home\" directory, so no changes are needed.\n",
    "\n",
    "Next, we combine the `homePath` variable with the folder names that lead to where our data is stored. Note that we do not use any file names yet, just the path to the folder. This is because we may want to read in all the files in the directory, or just one file. There are options below for doing both. We save the path as a variable named `dataHome`.\n",
    "\n",
    "Next, since we are using MALLET to do our LDA, you need to [download](http://mallet.cs.umass.edu/download.php) the MALLET zipfile, unzip it and provide the path to the extracted folder. We recommend saving the extracted folder in your \"Carbonate\" directory which is also your \"home\" directory. This way you will not need to adjust anything in the last line as it should point to the folder needed to run MALLET. We save this file path as the variable `malletPath`.\n",
    "\n",
    "Then we join the file path assigned to the `homePath` variable to another file path that leads to a folder for our LDA model, LDA dictionary, and other needed data and assign it to the variable `cleanDataPath`.\n",
    "\n",
    "Lastly, we join the file path assigned to the `cleanDataPath` variable to other file paths to point to what we want to name our output files for our cleaned data (`cleanData`), our LDA dictionary (`cleanDict`), our LDA model (`cleanModel`), and our original data uncleaned and converted to a list (`origData`). If you wish to name the files something else, then change the name in quotes in all of the variables just listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homePath = os.environ[\"HOME\"]\n",
    "dataHome = os.path.join(homePath, \"Text-Analysis-master\", \"data\")\n",
    "malletPath = os.path.join(dataHome,\"mallet-2.0.8\", \"bin\", \"mallet\") # update this path\n",
    "cleanDataPath = os.path.join(homePath, \"Text-Analysis-master\", \"TopicModeling\", \"LDA\", \"cleanedData\")\n",
    "cleanData = os.path.join(cleanDataPath, \"ldaDataClean\")\n",
    "cleanDict = os.path.join(cleanDataPath, \"ldaDict\")\n",
    "cleanModel = os.path.join(cleanDataPath, \"ldaModel\")\n",
    "origData = os.path.join(cleanDataPath, \"ldaDataOrig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set needed variables\n",
    "This is where you will make some decisions about your data and set the necessary variables. Much like the file path variables above, we do this so you do not need to make as many changes later.\n",
    "\n",
    "**source**<br>\n",
    "First, we need to decide if we want our code to read all the files in a directory or just a single file. If we want all the files in a directory then we set `source` equal to `\"*\"`. This means 'all' and will be added to the file type later in the code. If you want a single file change `\"*\"` to the file name without the \".txt\" or \".csv\" or \".json\" at the end. So if you have a file named \"myFile.txt\" you would set `source` equal to `\"myFile\"` without the \".txt\".\n",
    "\n",
    "**fileType**<br>\n",
    "Next we assign the file type our data comes in to a variable. At the moment the only options are \".txt\", \".csv\" or \".json\". The \".txt\" format is the most popular format for analysis of a text or corpus, while \".csv\" and \".json\" are the most common formats for twitter data. We assign the format to the `fileType` variable. It should look like this: `fileType = \".txt\"`.\n",
    "\n",
    "**docLevel**<br>\n",
    "The `docLevel` variable is only for file types of \".txt\" so if you have a \".csv\" or \".json\" you want to set it to **False** or it will cause problems in other parts of the code later since we do not keep track of file names for the \".csv\" and \".json\" files. If your data is in \".txt\" format, then you need to determine if you want to chunk your corpus by line or by document.\n",
    "\n",
    "We do this in case your data is a single \".txt\" file. The LDA algorithm needs to have multiple chunks to accurately weigh and order words into topics. If you have multiple documents then the documents themselves are the chunks. If you have a single document, then we need to create chunks, and we do this by spliting the document up by line and each line is a separate chunk.\n",
    "\n",
    "If you want to separate by document, then set docLevel equal to **True**. If you want to separate a line at a time and have each line be it's own entity or 'chunk' then set `docLevel` equal to **False**. If you set `source` equal to `\"*\"` then you will want to set `docLevel` equal to **True**. If you set `source` equal to a specific file name, then you will want to set `docLevel` equal to **False**.\n",
    "\n",
    "**n**<br>\n",
    "The `n` variable is where you assign the number of lines you would like to chunk the document into if you set `docLevel` equal to **False**. This variable is not used if `docLevel` is **True**.\n",
    "\n",
    "**nltkStop**<br>\n",
    "The `nltkStop` is where you determine if you want to use the built in stopword list provided by the NLTK package. They provide stopword lists in multiple languages. If you wish to use this then set `nltkStop` equal to **True**. If you do not, then set `nltkStop` equal to **False**.\n",
    "\n",
    "**customStop**<br>\n",
    "`customStop` is for if you have a .txt file that contains additional stopwords that you would like to read in and have added to the existing `stopWords` list. You do *NOT* need to use the NLTK stopwords list in order to add your own custom list of stopwords. **NOTE: Your custom stopwords file needs to have one word per line as it reads in a line at a time and the full contents of the line is read in and added to the existing stopWords list.** If you have a list of your own then set `customStop` equal to **True**. If you do not have your own custom stopwords list then set `customStop` equal to **False**.\n",
    "\n",
    "**spacyLem**<br>\n",
    "`spacyLem` is where we decide if we want to use the spaCy package lemmatization function. What is lemmatization? Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, where as stemming does not take the context of the word into account. For example, if we lemmatize the word \"running\" or \"ran\" it will become the word \"run\". If we stem the word \"running\" most stemmers will convert it to \"runn\" only removing the \"ing\" and leaving the second \"n\". Stemming will also change \"police\" and \"policy\" both to \"polic\" and they will be considered the same word by the LDA script. The lemmatizer will leave both words as \"police\" and \"policy\". This is useful and recommended for topic modeling as it allows the algorithm to just consider \"walk\" instead of \"walking\", \"walked\", and \"walk\" and thereby can increase the accuracy of your results. To use the spacy lemmatizer set `spacyLem` equal to **True**. If you do not wish to use the lemmatizer set `spacyLem` equal to **False**.\n",
    "\n",
    "**coherence**<br>\n",
    "`coherence` is where you decide if you want to calculate the coherence score for not just the default 20 topics, but if you want to calculate the coherence score beginning with 20 topics and going up by 10 until you get to 80 topics. This step takes a LONG time as you are running LDA on your corpus at least seven times and also running the coherence algorithm seven times as well. The idea behind the coherence score is that the higher the score, the more \"coherent\" or \"understandable\" your topics should be. This step is meant to help you determine the number of topics that are most likely to help make sense of your data. Again, this is a computer algorithm and so is not always right about what makes sense to humans, so we have given you the option to run all the coherence scores, or just stick with the default number of topics. If you want to see all the coherence scores set `coherence` equal to **True**. If you want just the default 20 topics and do not need to see a coherence score, set `coherence` equal to **False**.  \n",
    "\n",
    "**stopLang**<br>\n",
    "Now we choose the language we will be using for the nltk stopwords list. If you need a different language, simply change `'english'` (keep the quotes) in the `stopLang` variable to the anglicized name of the language you wish to use (e.g. 'spanish' instead of 'espanol' or 'german' instead of 'deutsch').\n",
    "\n",
    "**lemLang**<br>\n",
    "Now we choose the language for our lemmatizer. The languages available for spacy include the list below and the abbreviation spacy uses for that language:\n",
    "\n",
    "- **English:** en\n",
    "- **Spanish:** es\n",
    "- **German:** de\n",
    "- **French:** fr\n",
    "- **Italian:** it\n",
    "- **Portuguese:** pt\n",
    "- **Dutch:** nl\n",
    "- **Multi-Language:** xx\n",
    "\n",
    "To choose a language simply type the two letter code following the angliscized language name in the list above. So for Spanish it would be `'es'` (with the quotes) and for German `'de'` and so on.\n",
    "\n",
    "**encoding, errors**<br>\n",
    "The variable `encoding` is where you determine what type of encoding to use (ascii, ISO-8850-1, utf-8, etc...). We have it set to utf-8 at the moment as we have found it is less likely to have any problems. However, errors do occur, but the encoding errors rarely impact our results and it causes the Python code to exit. So instead of dealing with unhelpful errors we ignore the ones dealing with encoding by assigning `'ignore'` to the `errors` variable. If you want to see any encoding errors then change `'ignore'` to `None` without the quotes.\n",
    "\n",
    "**textColIndex**<br>\n",
    "The `textColIndex` variable is only applicable if our `fileType` is \".csv\" or \".json\". The `textColIndex` variable is where we put the header name of the dataframe column that will contain the content we are interested in from our tweets. Generally the content of the tweets are labeled as \"text\" since this is the label given to the tweet content when it is pulled directly from the Twitter API. For this reason our default value assigned to the `textColIndex` is `\"text\"`. If for some reason the tweet content has a different label or header, and you need to change this, remember to keep the quotes around the new label.\n",
    "\n",
    "**stopWords, docs**<br>\n",
    "The `stopWords =[]` variable is simply an empty list. This is where the words from the nltk stopword list or your custom stopword list or both combined or neither (depending on what you decide) will reside later on. You do not need to do anything to this line of code.\n",
    "\n",
    "The `docs = []` variable also does not need to have anything done to it as it is also an empty list that will be added to later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = \"*\"\n",
    "fileType = \".txt\"\n",
    "docLevel = True\n",
    "n = 100\n",
    "nltkStop = True\n",
    "customStop = True\n",
    "spacyLem = True\n",
    "coherence = False\n",
    "stopLang = 'english'\n",
    "lemLang = 'en'\n",
    "encoding = \"utf-8\"\n",
    "errors = \"ignore\"\n",
    "textColIndex = \"text\"\n",
    "stopWords = []\n",
    "docs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "If you set `nltkStop` equal to **True** above then this will add the NLTK stopwords list to the empty list named `stopWords`.\n",
    "\n",
    "You already chose your desired language above, so you do not need to do that now. \n",
    "\n",
    "If you need to add a few more words to the `stopWords` list that are specific to your dataset (such as common names or phrases that may make your results inaccurate), then add those to the `stopWords.extend(['would', 'said', 'says', 'also'])` part of the code in the square brackets with single quotes around each word and separated by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "if nltkStop is True:\n",
    "    stopWords.extend(stopwords.words(stopLang))\n",
    "    stopWords.extend(['would', 'said', 'says', 'also'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add own stopword list\n",
    "\n",
    "Here is where your own stopwords list is added if you selected **True** in `customStop` above. Here you will need to change the folder names and file name to match your folders and file. Remember to put each folder name in quotes and in the correct order always putting the file name including the file extension (.txt) last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if customStop is True:\n",
    "    stopWordsFilepath = os.path.join(homePath, \"Text-Analysis-master\", \"data\", \"earlyModernStopword.txt\")\n",
    "\n",
    "    with open(stopWordsFilepath, \"r\",encoding = encoding) as stopfile:\n",
    "        stopWordsCustom = [x.strip() for x in stopfile.readlines()]\n",
    "\n",
    "    stopWords.extend(stopWordsCustom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzip folders\n",
    "\n",
    "Here we are unzipping files. Since twitter data can be rather large it is often necessary to compress it into a '.zip' file in order to upload it to places such as GitHub. For this reason, we have setup some code to go in and automatically extract all the items in a compressed '.zip' file so you don't have to and so you don't get errors later. If the data is not in a '.zip' file there is no need to worry, it will not give an error if there are no files ending in '.zip' in your directory.\n",
    "\n",
    "You should not need to make any changes as we use the same variables containing our file paths as above, so if you need to make adjustments to the file paths, you need to make them there, specifically to the `dataHome` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if fileType == \".csv\":\n",
    "    dataRoot = os.path.join(dataHome, \"twitter\",\"CSV\")\n",
    "    direct = dataRoot\n",
    "    allZipFiles = glob.glob(os.path.join(dataRoot,\"*.zip\"))\n",
    "    for item in allZipFiles:\n",
    "        fileName = os.path.splitext(direct)[0]\n",
    "        zipRef = zipfile.ZipFile(item, \"r\")\n",
    "        zipRef.extractall(fileName)\n",
    "        zipRef.close()\n",
    "        os.remove(item)\n",
    "if fileType == \".json\":\n",
    "    dataRoot = os.path.join(dataHome, \"twitter\",\"JSON\")\n",
    "    direct = dataRoot\n",
    "    allZipFiles = glob.glob(os.path.join(dataHome, \"twitter\",\"JSON\",\"*.zip\"))\n",
    "    for item in allZipFiles:\n",
    "        fileName = os.path.splitext(direct)[0]\n",
    "        zipRef = zipfile.ZipFile(item, \"r\")\n",
    "        zipRef.extractall(fileName)\n",
    "        zipRef.close()\n",
    "        os.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in .txt files\n",
    "The code below reads in text files if you chose fileType `\".txt\"` above. It can do this in two ways. We can read in an entire directory, or we can read in a single file and it will do those based on what you chose for `source` above. Then it will chunk your data, either by document or by line, and this will depend on what you chose for `docLevel` above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if fileType == \".txt\":\n",
    "    paths = glob.glob(os.path.join(dataHome, \"shakespeareFolger\", source + fileType))\n",
    "    for path in paths:\n",
    "        with open(path, \"r\", encoding = encoding, errors = errors) as file:\n",
    "             # skip hidden file\n",
    "            if path.startswith('.'):\n",
    "                continue\n",
    "            if docLevel is True:\n",
    "                docs.append(file.read().strip('\\n').splitlines())\n",
    "            else:\n",
    "                for line in file:\n",
    "                    stripLine = line.strip()\n",
    "                    if len(stripLine) == 0:\n",
    "                        continue\n",
    "                    docs.append(stripLine.split())\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in .csv and .json files\n",
    "\n",
    "If you chose `\".csv\"` as your `fileType` up above, then the first `if` statement in the code below reads in \".csv\" files and saves the contents to a dataframe using the Pandas package. It will read in either an entire directory or a single \".csv\" file depending on what you chose for `source` above. \n",
    "\n",
    "Once we have read in the \".csv\" file using the Pandas `read_csv` function, we need to concatenate the \".csv\" files if there are multiple. Because of this it is important that your \".csv\" files have an identical column count and each column has identical header names or you will get errors. If you have a single \".csv\" file then you should be fine for this step. We assign this process to the variable `cdf` so we can use it later.\n",
    "\n",
    "Now we convert our `cdf` to a pandas dataframe. This allows for easier manipulation of the data in the next line.\n",
    "\n",
    "Finally, we pull in the column containing the data we are interested in which we assigned to the variable `textColIndex` earlier and turn it into a list assigned to the variable `tweets`.\n",
    "\n",
    "If you chose `\".json\"` for your fileType, then the second `if` statement will read in \".json\" files and save the content to a dataframe using the Pandas package much like the \".csv\" file process described above. The only difference is that we use the Pandas function `read_json` instead of `read_csv`. Everything else is exactly the same as what is described above in the \".csv\" section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if fileType == \".csv\":\n",
    "    allFiles = glob.glob(os.path.join(dataHome, \"twitter\", \"CSV\", source + fileType))     \n",
    "    df = (pd.read_csv(f, engine = \"python\") for f in allFiles)\n",
    "    cdf = pd.concat(df, ignore_index=True)\n",
    "    cdf = pd.DataFrame(cdf, dtype = 'str')\n",
    "    tweets = cdf[textColIndex].values.tolist()\n",
    "if fileType == \".json\":\n",
    "    allFiles = glob.glob(os.path.join(dataHome, \"twitter\", \"JSON\", source + fileType))     \n",
    "    df = (pd.read_json(f, encoding = encoding) for f in allFiles)\n",
    "    cdf = pd.concat(df, ignore_index=True)\n",
    "    cdf = pd.DataFrame(cdf, dtype = 'str')\n",
    "    tweets = cdf[textColIndex].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data variable\n",
    "\n",
    "Now we need to change our variable containing our data (either docs or tweets from above) to the variable `data` since this is the variable used going forward and it saves you from having to switch between `tweets` and `docs` later in the code. If you read in \".csv\" or \".json\" files then your data is saved in the `tweets` list and if you read in \".txt\" files then it is in the `docs` list. This code says if the length of the `docs` list is greater than 0 then assign `docs` to the variable data. If the length of `tweets` greater than 0 then assign `tweets` to the variable `data`.\n",
    "\n",
    "If your data was in `docs` and was from a single document, then you may want to chunk the data by lines. The \"if...else\" statement says if `docLevel` is **True** then `data = docs`. If `docLevel` is **False** then we chunk the document by `n` lines with `n` being the number we assigned to that variable up above.\n",
    "\n",
    "If your data was in `tweets` then it most likely needs some additional cleaning. So the next chunk of code removes URLS and new line characters from the `data` variable if the length of `tweets` is greater than 0.\n",
    "\n",
    "The last line prints out the first chunk of data in our collection, in this case the first few lines of the first item in our list of lists. If you are reading in a single document this will print the first line of your data. If you are reading in a line at a time this will print out each word for the entire text (either a single document or mutiple documents depending on your choices above) on it's own individual line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "if len(docs) > 0:\n",
    "    if docLevel is True:\n",
    "        data = docs\n",
    "    else:\n",
    "        data = []\n",
    "        for i in list(it.zip_longest(*(iter(docs),)*n)):\n",
    "            data.append(i)\n",
    "else:\n",
    "    if len(tweets) > 0:\n",
    "        data = tweets\n",
    "        # Remove Urls\n",
    "        data = [re.sub(r'http\\S+', '', sent) for sent in data]\n",
    "        # Remove new line characters\n",
    "        data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "This block of code separates each chunk of text into a list of individual words. In the process it also lower cases all the words and removes punctuation. If you wish to keep the punctuation change `deacc = True` to `deacc = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "def sentToWords(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "dataWords = list(sentToWords(data))\n",
    "\n",
    "print(len(dataWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Bigrams and Trigrams\n",
    "\n",
    "This code will most likely not need to be adjusted. It creates a model of bigrams and trigrams in your dataset that occur frequently and then connects them with an underscore so the LDA algorithm will later consider them as one word. This is a good idea for items like 'new york' or 'new zealand' or 'Ho Chi Minh'. If we do not combine these frequently occuring phrases then 'new' and 'york' will be considered independently and give us less accurate results.\n",
    "\n",
    "However, you may wish to adjust two settings depending on your data. The first is `min_count` and we assign a number to a variable `minCount` that will be used for `min_count` in the code. The other setting you might want to adjust is `threshold` and for this we assign a number to a variable `tHold` that will be used for `threshold` in the code.\n",
    "\n",
    "Right now we have a `min_count` of 5 and a `threshold` of 100. The `min_count` is simply the minimum number of times the bigram or trigram needs to occur in order to be combined with an underscore. The `threshold` is a score that the bigram or trigram needs to exceed in order to be combined with an underscore. The score is determined by using this formula: (bigram_count - min_count)\\*vocab_count/(wordA_count \\* wordB_count). So let's say we have the bigram \"good_lord\" and it appears 30 times in a text of 10,000 words where \"good\" appears 60 times total and \"lord\" appears 40. With our `min_count` set to 5 we get the following: (30 - 5)\\*10000/(60 \\* 40) = 104.167 which means since our `threshold` is set to 100 \"good_lord\" will be combined with an underscore and made into a bigram. If the resulting score is above your `threshold` then the ngram is considered important enough to combine with an underscore and will be viewed as one word for the LDA scoring later. Therefore, if you increase the `threshold`, you will get fewer bigrams and trigrams. If our threshold was set to 110, then \"good\" and \"lord\" would not be combined into \"good_lord\".\n",
    "\n",
    "The Phraser function takes the model you built with the Phrases function and cuts down memory consumption of Phrases, by discarding model state not strictly needed for the bigram detection task.\n",
    "\n",
    "Lastly, we take a look at the ngrams created from the first item in our dataset only, so the results are for only one chunk, not the whole dataset. We do this by counting the number of words that contain an underscore as this is used to connect the words in the ngram together. **NOTE:** The output is only to test if the ngrams work so you will probably see ngrams containing stopwords. We will create a few functions next and then apply them to remove stopwords, create bigrams, and lemmatize the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'poor_tom': 13,\n",
      "         'foul_fiend': 11,\n",
      "         'hundred_knights': 6,\n",
      "         'gav_st': 3,\n",
      "         'alack_alack': 3,\n",
      "         'take_heed': 2,\n",
      "         'good_morrow': 2,\n",
      "         'how_fares': 2,\n",
      "         'wast_born': 1,\n",
      "         'both_sides': 1,\n",
      "         'an_ass': 1,\n",
      "         'ha_ha_ha': 1,\n",
      "         'hundred_pound': 1,\n",
      "         'ha_ha': 1,\n",
      "         'cut_off': 1,\n",
      "         'heigh_ho': 1,\n",
      "         'fie_fie_fie': 1,\n",
      "         'looking_glass': 1})\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "minCount = 5\n",
    "tHold = 100\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = Phrases(dataWords, min_count=minCount, threshold=tHold) # higher threshold fewer phrases.\n",
    "trigram = Phrases(bigram[dataWords], threshold=tHold)  \n",
    "\n",
    "# Removes model state from Phrases thereby reducing memory use.\n",
    "bigramMod = Phraser(bigram)\n",
    "trigramMod = Phraser(trigram)\n",
    "\n",
    "# See bigram/trigram example\n",
    "testNgram = trigramMod[bigramMod[dataWords[0]]]\n",
    "char = \"_\"\n",
    "nGrams = [s for s in testNgram if char in s]\n",
    "            \n",
    "pprint(Counter(nGrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "We need to create a function in order to stem and tokenize our data. Any time you see `def` that means we are **DE**claring a **F**unction. The `def` is usually followed by the name of the function being created and then in parentheses are the parameters required by the function. After the parentheses is a colon, which closes the declaration, then a bunch of code below which is indented. The indented code is the program statement or statements to be executed. Once you have created your function all you need to do in order to run it is call the function by name and make sure you have included all the required parameters in the parentheses. This allows you to call the function without having to write out all the code in the function every time you wish to perform that task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions\n",
    "\n",
    "Below are functions we are creating that perform certain tasks. First we are creating a function to remove the stopwords that are in our stopword list we created previously. Then we create functions to apply our bigram and trigram code from above. \n",
    "\n",
    "Lastly, if you set `spacyLem` equal to **True** above then we will create the `lemmatization` function. If you set it equal to **False** then it will not create the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def removeStopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopWords] for doc in texts]\n",
    "\n",
    "def makeBigrams(texts):\n",
    "    return [bigramMod[doc] for doc in texts]\n",
    "\n",
    "def makeTrigrams(texts):\n",
    "    return [trigramMod[bigramMod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "if spacyLem is True:\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        textsOut = []\n",
    "        lemmaPOS = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            textsOut.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "            lemmaPOS.append([token.text and token.lemma_ and token.pos_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return textsOut\n",
    "        print(lemmaPOS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the functions. There are really only two parts where you may need to make changes and they are in the lines `dataWordsNgrams = makeBigrams(dataWordsNostops)` and `nlp = spacy.load(lemLang, disable=\\['parser', 'ner'\\])`.\n",
    "\n",
    "The `dataWordsNgrams` variable is where you will change between either using the `makeBigrams` or `makeTrigrams` functions above. If you only want bigrams, then keep the code as it is. If you want both bigrams and trigrams to be considered in your topic modeling, then change the `makeBigrams` part to `makeTrigrams` and it will now calculate both bigrams and trigrams. \n",
    "\n",
    "\n",
    "Adjustments to the other line that might need changes mentioned above may only be necessary if you previously set `spacyLem` equal to **True**. Even if you set it to **True** you may still not need to make changes. The line of code you may want to change is `nlp = spacy.load('lemLang', disable=\\['parser', 'ner'\\])` and is where you can disable the parser and named entity recognizer (ner). \n",
    "\n",
    "If you wish for your words to be parsed simply remove `'parser'` from the `disable=` bracket. Same for `ner`. If you wish to use both the parser and ner then just remove the `, disable=\\['parser', 'ner'\\]` entirely (including the preceding comma), but leave the closing parantheses. The reason we disable to 'parser' and 'ner' is because they slow down the lemmatization process and are not necessary to lemmatize our dataset.\n",
    "\n",
    "Lastly, we print out the ngrams we find in the first chunk (document or line) of our data. Notice there are no trigrams included. This is because we applied only the `makeBigrams` function from above. If we had applied the `makeTrigrams` function we would have both bigrams and trigrams. Feel free to change this in the code as described above. If we set `spacyLem` equal to **True** then we will get the first 10 words, their lemmatized form (which sometimes is identical to the word being lemmatized), with their parts of speech tagging from the `lemmatization` function above. Below this is a list of the lemmatized bigrams from the first chunk of our data. If we set it to **false** then we will get bigrams from the first chunk that have not been lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['think', 'think', 'VERB'], ['king', 'king', 'NOUN'], ['affect', 'affect', 'VERB'], ['duke', 'duke', 'ADJ'], ['albany', 'albany', 'NOUN'], ['cornwall', 'cornwall', 'NOUN'], ['division', 'division', 'NOUN'], ['kingdom', 'kingdom', 'NOUN'], ['appear', 'appear', 'VERB'], ['duke', 'duke', 'ADJ']]\n",
      "Counter({'foul_fiend': 11, 'alack_alack': 3, 'gav_st': 2, 'good_morrow': 2, 'wast_born': 1, 'looking_glass': 1})\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "dataWordsNostops = removeStopwords(dataWords)\n",
    "\n",
    "# Form Bigrams\n",
    "dataWordsNgrams = makeBigrams(dataWordsNostops)\n",
    "\n",
    "if spacyLem is True:\n",
    "    # Initialize spacy language model, eliminating the parser and ner components\n",
    "    nlp = spacy.load(lemLang, disable=['parser', 'ner'])\n",
    "    \n",
    "    # Do lemmatization tagging only noun, adj, vb, adv\n",
    "    allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    dataLemmatized = lemmatization(dataWordsNgrams, allowed_postags=allowed_postags)\n",
    "    lemmaPOS = []\n",
    "    for sent in dataLemmatized:\n",
    "        lemmaNLP = nlp(\" \".join(sent))\n",
    "        for token in lemmaNLP:\n",
    "            lemmaPOS.append([token.text, token.lemma_, token.pos_])\n",
    "    print(lemmaPOS[:10])\n",
    "    \n",
    "\n",
    "    # Find ngrams and count number of times they occur\n",
    "    dataNgrams = [s for s in dataLemmatized[0] if char in s]\n",
    "    \n",
    "else:\n",
    "    dataNgrams = [s for s in dataWordsNgrams[0] if char in s]\n",
    "print(Counter(dataNgrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dictionary and Corpus needed for Topic Modeling\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them.\n",
    "\n",
    "First, we need to determine if we are using lemmatized data or not. To do this we are using another `if` statement. Here, `if` we set `spacyLem` equal to **True** then we create the id2word dictionary based off of the lemmatized version of the data. However, if we did not, (denoted by `else`) then we will create the id2word dictionary based on the non-lemmatized data.\n",
    "\n",
    "Gensim creates a unique id for each word in the document. For example, (0, 1) implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "\n",
    "This is used as the input by the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if spacyLem is True:\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(dataLemmatized)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = dataLemmatized\n",
    "else:\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(dataWordsNgrams)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = dataWordsNgrams\n",
    "    \n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abuse'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you can see a human-readable form of the corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-PRON-', 1), ('abate', 1), ('abatement', 1), ('abhor', 2), ('abjure', 1), ('able', 1), ('abode', 1), ('abominable', 1), ('abroad', 2), ('absolute', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "hReadable = [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "for i in hReadable:\n",
    "    print(i[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING MALLET\n",
    "First we choose some important settings for our LDA model. \n",
    "\n",
    "The `nTopics` variable is where we choose the number of topics we want for our data. This number will vary depending on your data, so feel free to adjust this as needed. You had the option in a cell towards the top to choose to see coherence scores. If you set `coherence` to **True** then you will see coherence scores for various topic numbers below and you can adjust the `nTopics` to match the number of topics with the highest `coherence` score below. If you set `coherence` to **False** then the code below will not compute coherence scores and you can keep the default 20 topics, or play with the number of topics until you feel the results are the most coherent.\n",
    "\n",
    "The `workers` variable is where we determine the number of nodes beings used. For ReD this number needs to stay at 1 as you are only allowed to use one node at a time. If you are using this on your own system, then do not exceed the number of cores on your computer.\n",
    "\n",
    "`nIter` is where you choose the number of training passes or iterations you want to make over the data. Here it is better to error on the side of the number being high. However, you want to balance this with any time constraints you might have. More iterations take more time. Note that the default is 1000, however, I have seen this parameter set as low as 10. Feel free to play with this number as well to find an appropriate balance. \n",
    "\n",
    "`seed` is where you set the seed. The seed helps with reproducability so if someone else runs the code on your dataset with these settings they should get the same answer. **Note:** the topics will be the same, but possibly in a different order even when you set the seed, so if you run it and get a set of words for topic 5, then if you run it again with the exact same settings on the exact same dataset what was topic 5 might now be topic 12, but the topic will contain the same words ordered by weight/importance. \n",
    "\n",
    "Gensim provides a wrapper to implement Mallet’s LDA from within Gensim itself. As long as you have [downloaded](http://mallet.cs.umass.edu/download.php) the MALLET zipfile, extracted it and provided the path to the extracted mallet folder in the `malletPath` variable in the cell where we assign file paths to variables towards the top, then you should be good.  \n",
    "\n",
    "Adjustments should be made according to your specific data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "nTopics = 20\n",
    "workers = 1\n",
    "nIter = 1000\n",
    "seed = 42\n",
    "\n",
    "ldamallet = LdaMallet(malletPath, corpus=corpus, num_topics=nTopics, id2word=id2word, workers = workers, iterations = nIter, random_seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set `coherence` to **False**, then this cell will print the first three topics, and move on. The results show the topic number, the ten highest weighted (or important) keywords, and the weight score of those words.\n",
    "\n",
    "If you set `coherence` to **True** previously, then this will compute a coherence score in addition to the first three topics.  \n",
    "\n",
    "The weight score may seem very low, however, when you consider that every word in your data makes up every topic then the words seem more important.\n",
    "\n",
    "The coherence score is an algorithm that determines how 'human understandable' or 'coherent' your collection of topics are. We will do more with the coherence score further down in the notebook (if you set `coherence` to **True**). For now, you should not need to make any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('romeo', 0.02722926626480981),\n",
      "   ('rome', 0.026189981292870507),\n",
      "   ('son', 0.018707129494907503),\n",
      "   ('lord', 0.016836416545416753),\n",
      "   ('emperor', 0.01642070255664103),\n",
      "   ('brother', 0.01621284556225317),\n",
      "   ('sweet', 0.01579713157347745),\n",
      "   ('hand', 0.015589274579089585),\n",
      "   ('death', 0.014965703595926003),\n",
      "   ('roman', 0.012887133652047392)]),\n",
      " (15,\n",
      "  [('love', 0.03578030890333353),\n",
      "   ('sweet', 0.02468841314330014),\n",
      "   ('lord', 0.018307591388872325),\n",
      "   ('wit', 0.018307591388872325),\n",
      "   ('fair', 0.014908462043055638),\n",
      "   ('faith', 0.012702009660683404),\n",
      "   ('hand', 0.01240384041982229),\n",
      "   ('word', 0.012165305027133401),\n",
      "   ('hold', 0.012046037330788956),\n",
      "   ('fool', 0.01073409267100006)]),\n",
      " (14,\n",
      "  [('eye', 0.021317011397214015),\n",
      "   ('play', 0.018995356690586745),\n",
      "   ('hamlet', 0.016673701983959476),\n",
      "   ('monster', 0.014774166314900802),\n",
      "   ('lord', 0.014352047277332207),\n",
      "   ('follow', 0.013929928239763613),\n",
      "   ('night', 0.012030392570704939),\n",
      "   ('moon', 0.010552975939214858),\n",
      "   ('sweet', 0.009919797382861967),\n",
      "   ('love', 0.009497678345293373)])]\n"
     ]
    }
   ],
   "source": [
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False)[:3])\n",
    "if coherence is True:\n",
    "    if spacyLem is True:\n",
    "        # Compute Coherence Score\n",
    "        coherenceModelLdamallet = CoherenceModel(model=ldamallet, texts=dataLemmatized, dictionary=id2word, coherence='c_v')\n",
    "        coherenceLdamallet = coherenceModelLdamallet.get_coherence()\n",
    "    else:\n",
    "        # Compute Coherence Score\n",
    "        coherenceModelLdamallet = CoherenceModel(model=ldamallet, texts=dataWordsNgrams, dictionary=id2word, coherence='c_v')\n",
    "        coherenceLdamallet = coherenceModelLdamallet.get_coherence()\n",
    "    \n",
    "    print('\\nCoherence Score: ', coherenceLdamallet)\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIND OPTIMAL NUMBER OF TOPICS\n",
    "\n",
    "Next we will try and find the optimal number of topics. To do this we build many LDA models with different values of number of topics and pick the one that gives the highest coherence value. We do this by creating the function `computeCoherenceValues`. You will most likley not want to make changes to the function. Any possible changes will come in the next cell of code. If you set `coherence` to **False** above, then this cell is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if coherence is True:\n",
    "    def computeCoherenceValues(dictionary, corpus, texts, limit, start=20, step=10):\n",
    "        \"\"\"\n",
    "        Compute c_v coherence for various number of topics\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dictionary : Gensim dictionary\n",
    "        corpus : Gensim corpus\n",
    "        texts : List of input texts\n",
    "        limit : Max num of topics\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        modelList : List of LDA topic models\n",
    "        coherenceValues : Coherence values corresponding to the LDA model with respective number of topics\n",
    "        \"\"\"\n",
    "        coherenceValues = []\n",
    "        modelList = []\n",
    "        for numTopics in range(start, limit, step):\n",
    "            model = gensim.models.wrappers.LdaMallet(malletPath, corpus=corpus, num_topics=numTopics, id2word=id2word)\n",
    "            modelList.append(model)\n",
    "            coherenceModel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherenceValues.append(coherenceModel.get_coherence())\n",
    "\n",
    "        return modelList, coherenceValues\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set `coherence` equal to **False** then this cell is skipped.\n",
    "\n",
    "If otherwise...\n",
    "\n",
    "The `computeCoherenceValues()` trains multiple LDA models and their corresponding coherence scores. Right now we have it set to start with 20 topics (`nt` variable, stands for \"number of topics\") and increase the number of topics by 10 (`steps` variable) every time and stopping at 80 topics (`lmt` variable). Notice that `lmt` is set to 81, not 80. This is because the `limit` parameter in the `computeCoherenceValues` function is exclusive, meaning it includes everything before that number, but not the number itself. However, the `start` parameter is inclusive, so it includes the number we assign to that parameter. If we wanted to start at 30 topics and go up by 20 topics each time and stop at 90 topics as the max we would change the `nTopics`, `lmt`, and `steps` numbers to `nTopics=30`, `lmt=91`, `steps=20`. This is the only part of the code you may want to adjust depending on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "if coherence is True:\n",
    "    nt = 20\n",
    "    lmt = 81\n",
    "    steps = 10\n",
    "    if spacyLem is True:\n",
    "        modelList, coherenceValues = computeCoherenceValues(dictionary=id2word, corpus=corpus, texts=dataLemmatized, start=nt, limit=lmt, step=steps)\n",
    "    else:\n",
    "        modelList, coherenceValues = computeCoherenceValues(dictionary=id2word, corpus=corpus, texts=dataWordsNgrams, start=nt, limit=lmt, step=steps)\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set `coherence` equal to **True** we will want to visualize the coherence scores for each number of topics to help us decide how many topics we should use going forward. Here we need to make sure the limit, start, and step are the same as what we have in the previous code cell so that we can accurately see the coherence scores for each topic.\n",
    "\n",
    "Choosing a number of topics that marks the end of a sharp increase in topic coherence scores usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics. If you see the same keywords being repeated in multiple topics, it’s probably a sign that the number of topics is too large. To choose the number of topics that might be best do not just use the highest coherence score. There are other factors that need to be considered. If the coherence score just seems to keep climbing higher and higher along with the number of topics, it might be best to find the number where it appears to stop and flatten out a bit before continuing to ascend. You do not want to run the risk of having too many or too few topics.\n",
    "\n",
    "If you set `coherence` equal to **False** then this cell is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if coherence is True:\n",
    "    # Show graph\n",
    "    limit=lmt; start=nt; step=steps;\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherenceValues)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherenceValues\"), loc='best')\n",
    "    plt.show()\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `coherence` was set to **False** then this cell is skipped.\n",
    "\n",
    "If otherwise...\n",
    "\n",
    "Now that we have a visual, it may help to go back and look at the actual numbers. The code below lists the coherence score for each number of topics we have selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if coherence is True:\n",
    "    # Print the coherence scores\n",
    "    for m, cv in zip(x, coherenceValues):\n",
    "        print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose how many topics we want to work with going forward. If you set `coherence` equal to **False** then this number will be the number we assigned to the `nTopics` variable earlier. \n",
    "\n",
    "If you set `coherence` equal to true, then you will need to decide the number based on the coherence options from above. To do this we count Pythonically (so starting with 0) and in the first line we choose the number of topics we want based on the order of where it falls in the list from the previous cell. So if we wish to keep using 20 topics, in the first line of code (after the `if` statement) I would put `modelNum = 0` which assigns the topic number in the first position of the list above to the variable `modelNum`. If I wanted 60 topics I would change the 0 to a 4. Pick whichever number of topics you think provides the best results for your data. We have it set to 20 (or number 0 in the list).\n",
    "\n",
    "The second line of code says we want to see the topics and the weighted words associated with them as well as the weight of each word.\n",
    "\n",
    "The final line of code says that we only want to see the first three topics with the top ten words in each topic based on their weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.027*\"romeo\" + 0.026*\"rome\" + 0.019*\"son\" + 0.017*\"lord\" + 0.016*\"emperor\" '\n",
      "  '+ 0.016*\"brother\" + 0.016*\"sweet\" + 0.016*\"hand\" + 0.015*\"death\" + '\n",
      "  '0.013*\"roman\"'),\n",
      " (1,\n",
      "  '0.051*\"good\" + 0.025*\"great\" + 0.022*\"hear\" + 0.021*\"leave\" + 0.020*\"bring\" '\n",
      "  '+ 0.018*\"speak\" + 0.016*\"pray\" + 0.016*\"bear\" + 0.016*\"honour\" + '\n",
      "  '0.012*\"poor\"'),\n",
      " (2,\n",
      "  '0.031*\"lord\" + 0.031*\"brother\" + 0.025*\"die\" + 0.021*\"duke\" + 0.015*\"death\" '\n",
      "  '+ 0.014*\"claudio\" + 0.014*\"place\" + 0.013*\"life\" + 0.012*\"law\" + '\n",
      "  '0.012*\"angelo\"')]\n"
     ]
    }
   ],
   "source": [
    "if coherence is True:\n",
    "    modelNum = 0\n",
    "    # Select the model and print the topics\n",
    "    optimalModel = modelList[modelNum]\n",
    "    modelTopics = optimalModel.show_topics(formatted=False)\n",
    "    pprint(optimalModel.print_topics(num_words=10)[:3])\n",
    "else:\n",
    "    optimalModel = ldamallet\n",
    "    modelTopics = optimalModel.show_topics(formatted=False)\n",
    "    pprint(optimalModel.print_topics(num_words=10)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save models, dictionary, and data\n",
    "\n",
    "Now we save our LDA model, our dictionary of unique words, our original data as a list of words, and a cleaned version of our data as a list of words to the folder and by the file names we chose up above in an earlier cell. \n",
    "\n",
    "To create tables and graphs that will help bring more clarity to your LDA topics use the \"ldaMalletResults\" notebook that is part of this text analysis repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimalModel.save(cleanModel)\n",
    "id2word.save(cleanDict)\n",
    "with open(origData, \"wb\") as sd:\n",
    "    pickle.dump(data, sd)\n",
    "with open(cleanData, \"wb\") as phrases:\n",
    "    pickle.dump(texts, phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ackowledgements: This algorithm was adapted from the blog \"Machine Learning Plus\". Reference: Machine Learning Plus. Topic Modeling with Gensim (Python). Retrieved from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ on November 5, 2018."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
