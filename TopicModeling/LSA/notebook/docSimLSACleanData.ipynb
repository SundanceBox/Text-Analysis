{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Document Similarity with Latent Semantic Analysis (LSA)\n",
    "\n",
    "The following notebook walks you through doing LSA document similarity in Python. We then output the document similarity matrix as a .csv file which can be manipulated to highlight similarity between documents. You then have the option of using our \"docSimLSAHeatmap\" notebook to create a heatmap of cosine similarity scores between documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "Before we start, you will need to have set up a [Carbonate account](https://kb.iu.edu/d/aolp) in order to access [Research Desktop (ReD)](https://kb.iu.edu/d/apum). You will also need to have access to ReD through the [thinlinc client](https://kb.iu.edu/d/aput). If you have not done any of this, or have only done some of this, but not all, you should go to our [textPrep-Py.ipynb](https://github.com/cyberdh/Text-Analysis/blob/drafts/textPrep-Py.ipynb) before you proceed further. The textPrep-Py notebook provides information and resources on how to get a Carbonate account, how to set up ReD, and how to get started using the Jupyter Notebook on ReD.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CyberDH environment\n",
    "The code in the cell below points to a Python environment specificaly for use with the Python Jupyter Notebooks created by Cyberinfrastructure for Digital Humanities. It allows for the use of the different pakcages in our notebooks and their subsequent data sets.\n",
    "\n",
    "##### Packages\n",
    "- **sys:** Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "#### NOTE: This cell is only for use with Research Desktop. You will get an error if you try to run this cell on your personal device!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,\"/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages\")\n",
    "os.environ[\"NLTK_DATA\"] = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary packages for notebook \n",
    "\n",
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others created by Python users are available for download. Make sure to have the following packages installed before beginning so that they can be accessed while running the scripts.\n",
    "\n",
    "In your terminal, packages can be installed by simply typing `pip install nameofpackage --user`. However, since you are using ReD and our Python environment, you will not need to install any of the packages below to use this notebook. Anytime you need to make use of a package, however, you need to import it so that Python knows to look in these packages for any functions or commands you use. Below is a brief description of the packages we are using in this notebook:  \n",
    "\n",
    "- **sklearn:** Simple and efficient tools for data mining and data analysis built on NumPy, SciPy, and matplotlib.\n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- **warnings:** Allows for the manipulation of warning messages in Python.\n",
    "- **numpy:** a general-purpose array-processing package designed to efficiently manipulate large multi-dimensional arrays of arbitrary records without sacrificing too much speed for small multi-dimensional arrays. \n",
    "- **string:** Contains a number of functions to process standard Python strings.\n",
    "- **nltk:** A leading platform for building Python programs to work with human language data.\n",
    "\n",
    "Notice we import some of the packages differently. In some cases we just import the entire package when we say `import XYZ`. For some packages which are small, or, from which we are going to use a lot of the functionality it provides, this is fine. \n",
    "\n",
    "Sometimes when we import the package directly we say `import XYZ as X`. All this does is allow us to type `X` instead of `XYZ` when we use certain functions from the package. So we can now say `X.function()` instead of `XYZ.function()`. This saves time typing and eliminates errors from having to type out longer package names. I could just as easily type `import XYZ as potato` and whenever I use a function from the `XYZ` package I would need to type `potato.function()`. What we import the package as is up to you, but some commonly used packages have abbreviations that are standard amongst Python users such as `import pandas as pd` or `import matplotlib.pyplot as plt`. You do not need to us `pd` or `plt`, however, these are widely used and using something else could confuse other users and is generally considered bad practice. \n",
    "\n",
    "Other times we import only specific elements or functions from a package. This is common with packages that are very large and provide a lot of functionality, but from which we are only using a couple functions or a specific subset of the package that contains the functionality we need. This is seen when we say `from XYZ import ABC`. This is saying I only want the `ABC` function from the `XYZ` package. Sometimes we need to point to the specific location where a function is located within the package. We do this by adding periods in between the directory names, so it would look like `from XYZ.123.A1B2 import LMN`. This says we want the `LMN` function which is located in the `XYZ` package and then the `123` and `A1B2` directory in that package. \n",
    "\n",
    "You can also import more than one function from a package by separating the functions with commas like this `from XYZ import ABC, LMN, QRS`. This imports the `ABC`, `LMN` and `QRS` functions from the `XYZ` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will ignore deprecation and future warnings. All the warnings in this code are not concerning and will not break the code or cause errors in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Suppress warnings from pandas library\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
    "                        module=\"pandas\", lineno=570)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
    "                        module = \"sklearn\", lineno = 1059)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
    "                        module = \"sklearn\", lineno = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File paths\n",
    "Here we are saving as variables different file paths that we need in our code. We do this so that they are easier to call later and so that you can make most of your changes now and not need to make as many changes later. \n",
    "\n",
    "First we use the `os` package above to find our `[\"HOME\"]` directory using the `environ` function. This will work for any operating system, so if you decide to try this out on your personal computer instead of ReD, the `homePath` variable will still be the path to your 'home' directory, so no changes are needed.\n",
    "\n",
    "Next, we combine the `homePath` variable with the folder names that lead to where our data is stored. Note that we do not use any file names yet, just the path to the folder. This is because we are comparing documents to one another, so we need to read in an entire directory. You will want to change the folder names to match your folder names in your file path.\n",
    "\n",
    "Now we add the `homePath` variable to other folder names that lead to a folder where we will want to save our document similarity matrix. You again will want to change the folder names in the path to match your own folder names. We save this file path as the variable `cleanedData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homePath = os.environ[\"HOME\"]\n",
    "dataHome = os.path.join(homePath, \"Text-Analysis-master\", \"data\", \"shakespeareDated\")\n",
    "cleanedData = os.path.join(homePath, \"Text-Analysis-master\", \"TopicModeling\", \"LSA\", \"cleanedData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set needed variables\n",
    "\n",
    "Now we are setting needed variables that will help determine what the code will do farther down. Just like the file path variables, this is done so you have to make fewer changes later and also to make the changes easier to find by putting them in one place.\n",
    "\n",
    "If you want to use the stopword list that comes with the nltk package then set `nltkStop` equal to **True**. If you do not wish to use the nltk stopword list then set `nltkStop` equal to **False**.\n",
    "\n",
    "If you have created your own custom stopword list and wish to use that, then set `customStop` equal to **True**. If you do not have your own custom stopword list then set `customStop` equal to **False**.\n",
    "\n",
    "**NOTE: You can use both the nltk and custom stopword lists or you can use neither or just one or the other. You do NOT need to set them both to True or both to False. Use whatever works best for you.**\n",
    "\n",
    "Next we decide if we want to stem our words. Stemming words will remove certain endings of words leaving you with the root of the word. So \"reads\" becomes \"read\" as does \"reading\" and \"reader\". This will probably increase the similarity of documents as they will then share more words in common. If you want to stem the words in your dataset the assign `True` to the variable `stem`. If you do not wish to stem your words then assign `False` to the variable `stem`.\n",
    "\n",
    "Then we decide if we want all the words in our dataset lowercased. This will change \"Love\" to \"love\" so that it is recognized as the same word for similarity purposes. However, there are some cases where the use of capitalization may be important to determining similarity, so we have the option to lowercase or not. If you want to lowercase all the words in your dataset assign `True` to the variable `lower`. If you do not wish to lowercase all the words in your dataset then assign `False` to the variable `lower`.\n",
    "\n",
    "Now we choose the language we will be using for the nltk stopwords list and the nltk stemmer. If you need a different language, simply change 'english' (keep the quotes) in the `language` variable to the anglicized name of the language you wish to use (e.g. 'spanish' instead of 'espanol' or 'german' instead of 'deutsch').\n",
    "\n",
    "The variable `encoding` is where you determine what type of encoding to use (ascii, ISO-8850-1, utf-8, etc...). We have it set to utf-8 at the moment as we have found it is less likely to have any problems. However, errors do occur, but the encoding errors rarely impact our results and it causes the Python code to exit. So instead of dealing with unhelpful errors we ignore the ones dealing with encoding by assigning `'ignore'` to the `errors` variable. If you want to see any encoding errors then change `'ignore'` to `None` without the quotes.\n",
    "\n",
    "The `stopWords =[]` variable is simply an empty list. This is where the words from the nltk stopword list or your custom stopword list or both combined or neither (depending on what you decide) will reside later on. You do not need to do anything to this line of code.\n",
    "\n",
    "The `tokenDict = {}` variable is an empty dictionary. This is where your documents will reside later. The file name for the document will be the key and the content of the document will be the value. This will be explained in more detail later. For now, you do not need to do anything to this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltkStop = True\n",
    "customStop = True\n",
    "stem = True\n",
    "lowerCase = True\n",
    "language = 'english'\n",
    "encoding = 'utf-8'\n",
    "errors = 'ignore'\n",
    "stopWords = []\n",
    "tokenDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "If you set `nltkStop` equal to **True** above then this will add the NLTK stopwords list to the empty list named `stopWords`.\n",
    "\n",
    "You should have already chosen your desired language above, but if you wish to add any words to the stopWords list then add the word(s) you want as a stop word in the `stopWords.extend(['words', 'you', 'want', 'to', 'add'])` part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if nltkStop is True:\n",
    "    # NLTK Stop words\n",
    "    stopWords = stopwords.words(language)\n",
    "\n",
    "    stopWords.extend(['would', 'said', 'says', 'also'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add own stopword list\n",
    "\n",
    "Here is where your own stopwords list is added if you set `customStop` equal to **True** above. Here you will need to change the folder names and file name to match your folders and file. Remember to put each folder name in quotes and in the correct order always putting the file name including the file extension (.txt) last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if customStop is True:\n",
    "    stopWordsFilepath = os.path.join(homePath, \"Text-Analysis-master\", \"data\", \"earlyModernStopword.txt\")\n",
    "\n",
    "    with open(stopWordsFilepath, \"r\",encoding = encoding, errors = errors) as f:\n",
    "        stopWordsList = [x.strip() for x in f.readlines()]\n",
    "\n",
    "    stopWords.extend(stopWordsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary and stemmer\n",
    "Here we save the word stemmer we are using as the variable `stemmer`. \n",
    "\n",
    "The stemmer will truncate endings of words that end in 'ing' or 's' or 'es' et cetera. This will help ensure that words such as 'reading' and 'read' are considered as the same word so that the concept of 'to read' is given equal consideration. You should have already chosen your desired language up above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if stem is True:\n",
    "    stemmer = SnowballStemmer(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "We need to create a function in order to stem and tokenize our data. Any time you see `def` that means we are **DE**claring a **F**unction. The `def` is usually followed by the name of the function being created and then in parentheses are the parameters required by the function. After the parentheses is a colon, which closes the declaration, then a bunch of code below which is indented. The indented code is the program statement or statements to be executed. Once you have created your function all you need to do in order to run it is call the function by name and make sure you have included all the required parameters in the parentheses. This allows you to call the function without having to write out all the code in the function every time you wish to perform that task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and tokenization functions\n",
    "Here we create two functions. The first is where we will use the `stemmer` variable above to create a function that will stem the words in our documents when applied.\n",
    "\n",
    "The second tokenizes our documents which means it splits it into individual words. This function uses the `stemTokens` function as part of it so later in the code we will only need to apply the `tokenize` function. \n",
    "\n",
    "You should not need to make any changes to this block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if stem is True:\n",
    "    def stemTokens(tokens, stemmer):\n",
    "        stemmed = []\n",
    "        for item in tokens:\n",
    "            stemmed.append(stemmer.stem(item))\n",
    "        return stemmed\n",
    "\n",
    "    def tokenize(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stems = stemTokens(tokens, stemmer)\n",
    "        return stems\n",
    "\n",
    "else:\n",
    "    def tokenize(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in documents\n",
    "Now we read in our documents and also perform some text cleaning. This code lower cases all the words as well as removes punctuation. Then it adds the file names and cleaned content of each file to our previously empty `tokenDict` dictionary above. You should not need to make any changes to this code.\n",
    "\n",
    "A dictionary is similar to a list except it has what are called 'keys' and 'values'. This basically allows us to label our data. In this case we will be making the file names of our documents the 'keys' and the content of the file the 'values' so that each document name correlates to the content of that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for subdir, dirs, files in os.walk(dataHome):\n",
    "    for file in files:\n",
    "        if file.startswith('.'):\n",
    "                continue\n",
    "        filePath = subdir + os.path.sep + file\n",
    "        with open(filePath, 'r', encoding = encoding, errors = errors) as textFile:\n",
    "            text = textFile.read()\n",
    "            if lowerCase is True:\n",
    "                lowers = text.lower()\n",
    "                noPunctuation = lowers.translate(str.maketrans('','', string.punctuation))\n",
    "                tokenDict[file] = noPunctuation\n",
    "            else:\n",
    "                noPunctuation = text.translate(str.maketrans('','', string.punctuation))\n",
    "                tokenDict[file] = noPunctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check and see if our dictionary now has our data. We are asking to see the first 10 keys of our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1610Cymbeline.txt', '1596MerchantOfVenice.txt', '1604AllsWellThatEndsWell.txt', '1591KingHenry6_3.txt', '1599JuliusCaesar.txt', '1600TroilusAndCressida.txt', '1605TimonOfAthens.txt', '1592KingRichard3.txt', '1591KingHenry6_2.txt', '1603MeasureForMeasure.txt']\n"
     ]
    }
   ],
   "source": [
    "print(list(tokenDict.keys())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer\n",
    "\n",
    "Here we weight the importance of each word in the document. This is done using Term Frequency-Inverse Document Frequency (Tfidf). This considers how important a word is based on the frequency in the whole corpus as well as in individual documents. This allows for words that might not have a high frequency in an entire collection, but do have a high frequency in one or two documents when compared to other words to still be given a higher level of importance throughout the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = tokenize, stop_words=stopWords)\n",
    "dtm = vectorizer.fit_transform(tokenDict.values())\n",
    "testDF = pd.DataFrame(dtm.toarray(), index=tokenDict.keys(), columns = vectorizer.get_feature_names())\n",
    "testDF = testDF.sort_index(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at our data frame where the rows are the documents and the columns are the terms. To adjust which part of the data frame you see in the output change the numbers in the square brackets. To see the 10th thru the 20th row and the 100th to 110th column the numbers in the brackets should read `[10:20, 100:110]`. So the first set of numbers is for what rows you wish to see and the second set dictate the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>massacr</th>\n",
       "      <th>massi</th>\n",
       "      <th>mast</th>\n",
       "      <th>master</th>\n",
       "      <th>mastercord</th>\n",
       "      <th>masterdom</th>\n",
       "      <th>mastergunn</th>\n",
       "      <th>masterleav</th>\n",
       "      <th>masterless</th>\n",
       "      <th>masterpiec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1589TwoGentlemenOfVerona.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590TamingOfTheShrew.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591KingHenry6_1.txt</th>\n",
       "      <td>0.015566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591KingHenry6_2.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591KingHenry6_3.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006464</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591TitusAndronicus.txt</th>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592KingRichard3.txt</th>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>0.012907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594ComedyOfErrors.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.146888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594LovesLaboursLost.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595KingRichard2.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               massacr  massi      mast    master  mastercord  \\\n",
       "1589TwoGentlemenOfVerona.txt  0.000000    0.0  0.000000  0.079115         0.0   \n",
       "1590TamingOfTheShrew.txt      0.000000    0.0  0.000000  0.104583         0.0   \n",
       "1591KingHenry6_1.txt          0.015566    0.0  0.000000  0.012101         0.0   \n",
       "1591KingHenry6_2.txt          0.000000    0.0  0.000000  0.051486         0.0   \n",
       "1591KingHenry6_3.txt          0.000000    0.0  0.006464  0.010712         0.0   \n",
       "1591TitusAndronicus.txt       0.009424    0.0  0.000000  0.006279         0.0   \n",
       "1592KingRichard3.txt          0.008609    0.0  0.003462  0.012907         0.0   \n",
       "1594ComedyOfErrors.txt        0.000000    0.0  0.013904  0.146888         0.0   \n",
       "1594LovesLaboursLost.txt      0.000000    0.0  0.000000  0.059978         0.0   \n",
       "1595KingRichard2.txt          0.000000    0.0  0.000000  0.003632         0.0   \n",
       "\n",
       "                              masterdom  mastergunn  masterleav  masterless  \\\n",
       "1589TwoGentlemenOfVerona.txt        0.0    0.000000         0.0         0.0   \n",
       "1590TamingOfTheShrew.txt            0.0    0.000000         0.0         0.0   \n",
       "1591KingHenry6_1.txt                0.0    0.006773         0.0         0.0   \n",
       "1591KingHenry6_2.txt                0.0    0.000000         0.0         0.0   \n",
       "1591KingHenry6_3.txt                0.0    0.000000         0.0         0.0   \n",
       "1591TitusAndronicus.txt             0.0    0.000000         0.0         0.0   \n",
       "1592KingRichard3.txt                0.0    0.000000         0.0         0.0   \n",
       "1594ComedyOfErrors.txt              0.0    0.000000         0.0         0.0   \n",
       "1594LovesLaboursLost.txt            0.0    0.000000         0.0         0.0   \n",
       "1595KingRichard2.txt                0.0    0.000000         0.0         0.0   \n",
       "\n",
       "                              masterpiec  \n",
       "1589TwoGentlemenOfVerona.txt         0.0  \n",
       "1590TamingOfTheShrew.txt             0.0  \n",
       "1591KingHenry6_1.txt                 0.0  \n",
       "1591KingHenry6_2.txt                 0.0  \n",
       "1591KingHenry6_3.txt                 0.0  \n",
       "1591TitusAndronicus.txt              0.0  \n",
       "1592KingRichard3.txt                 0.0  \n",
       "1594ComedyOfErrors.txt               0.0  \n",
       "1594LovesLaboursLost.txt             0.0  \n",
       "1595KingRichard2.txt                 0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.iloc[:10, 7640:7650]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code outputs the first 10 words that make up the columns once we have broken our corpus down into a Tfidf matrix. To output all of the words remove the square brackets and their contents in the `print(vectGFN[:20])` line of code. To change the number of words printed change `20` in the same line to the number of words you wish to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2d', '2s', '4d', '5s', '6d', '8d', 'aaron', 'abandon', 'abas', 'abash', 'abat', 'abatfowl', 'abbess', 'abbey', 'abbot', 'abbrevi', 'abc', 'abe', 'abel', 'abergavenni']\n"
     ]
    }
   ],
   "source": [
    "# Get words that correspond to each column\n",
    "vectGFN = vectorizer.get_feature_names()\n",
    "print(vectGFN[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run SVD and Cosine Similarity\n",
    "\n",
    "Here we run our Tfidf matrix created above through Singular Value Decomposition and then calculate the Cosine Similarity of the documents to one another. \n",
    "\n",
    "Singular Value Decomposition condenses our Tfidf matrix down a bit to make it easier to process. Here we also set the number of dimensions (`n_components`) , how many times we iterate over the corpus (`n_iter`), and then set the seed (`random_state`) so that the results are reproducable. At the moment the `random_state` is set to 42 which sets the seed for the random number generator, but feel free to adjust the number to get a slightly different output. Just make sure you keep the seed the same once you find one you like for reproducibility.\n",
    "\n",
    "Cosine similarity is where we measure how similar the documents are to one another. The result is a number between -1 and 1 with 1 being a perfect match (which we will get when the document is compared to itself) and -1 being completely different which we might get if we have a document of all numbers and one of all words with no numbers at all. Usually, even documents that are about unrelated topics share some common words and so are not completely dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components = 100, n_iter = 1000, random_state = 42)\n",
    "dtmLsa = lsa.fit_transform(dtm)\n",
    "cosineSim = cosine_similarity(dtmLsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save as .csv\n",
    "\n",
    "Now we save the results as a .csv file. First we name the output .csv file so it matches our data. We do this in the first line of the cell.\n",
    "\n",
    "Then we create the data frame and say we want the rows and columns to be labeled with the file names. Then we sort the columns in alphanumeric order by column header, then we sort the rows alphanumericaly by row label.\n",
    "\n",
    "Finally, we export the dataframe as a .csv file.\n",
    "\n",
    "You can manipulate the .csv file in excel or some other spreadsheet software, or you can use it in the \"docSimLSAHeatmap\" notebook that is part of this text analysis repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvFileName = \"docSimilarityMatrix.csv\"\n",
    "\n",
    "df = pd.DataFrame(cosineSim, index = tokenDict.keys(), columns=tokenDict.keys())\n",
    "dfS = df[sorted(df)]\n",
    "sortedDf = dfS.sort_index(axis = 0)\n",
    "np.fill_diagonal(sortedDf.values, np.nan)\n",
    "sortedDf.to_csv(os.path.join(cleanedData, csvFileName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was adapted from https://www.datascienceassn.org/sites/default/files/users/user1/lsa_presentation_final.pdf at Colorado University, Boulder. Accessed on 02/01/2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
