{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Document Similarity with Latent Semantic Analysis (LSA)\n",
    "\n",
    "The following notebook walks you through doing LSA document similarity in Python. We then output the document similarity matrix as a .csv file which can be manipulated to highlight similarity between documents. You then have the option of using our \"docSimLSAHeatmap\" notebook to create a heatmap of cosine similarity scores between documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Before we begin\n",
    "Before we start, you will need to have set up a [Carbonate account](https://kb.iu.edu/d/aolp) in order to access [Research Desktop (ReD)](https://kb.iu.edu/d/apum). You will also need to have access to ReD through the [thinlinc client](https://kb.iu.edu/d/aput). If you have not done any of this, or have only done some of this, but not all, you should go to our [textPrep-Py.ipynb](https://github.com/cyberdh/Text-Analysis/blob/drafts/textPrep-Py.ipynb) before you proceed further. The textPrep-Py notebook provides information and resources on how to get a Carbonate account, how to set up ReD, and how to get started using the Jupyter Notebook on ReD.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CyberDH environment\n",
    "The code in the cell below points to a Python environment specificaly for use with the Python Jupyter Notebooks created by Cyberinfrastructure for Digital Humanities. It allows for the use of the different pakcages in our notebooks and their subsequent data sets.\n",
    "\n",
    "##### Packages\n",
    "- **sys:** Provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "- **os:** Provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "#### NOTE: This cell is only for use with Research Desktop. You will get an error if you try to run this cell on your personal device!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,\"/N/u/cyberdh/Carbonate/dhPyEnviron/lib/python3.6/site-packages\")\n",
    "os.environ[\"NLTK_DATA\"] = \"/N/u/cyberdh/Carbonate/dhPyEnviron/nltk_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include necessary packages for notebook \n",
    "\n",
    "Python's extensibility comes in large part from packages. Packages are groups of functions, data, and algorithms that allow users to easily carry out processes without recreating the wheel. Some packages are included in the basic installation of Python, others created by Python users are available for download. Make sure to have the following packages installed before beginning so that they can be accessed while running the scripts.\n",
    "\n",
    "In your terminal, packages can be installed by simply typing `pip install nameofpackage --user`. However, since you are using ReD and our Python environment, you will not need to install any of the packages below to use this notebook. Anytime you need to make use of a package, however, you need to import it so that Python knows to look in these packages for any functions or commands you use. Below is a brief description of the packages we are using in this notebook:  \n",
    "\n",
    "- **sklearn:** Simple and efficient tools for data mining and data analysis built on NumPy, SciPy, and matplotlib.\n",
    "- **pandas:** An open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "- **warnings:** Allows for the manipulation of warning messages in Python.\n",
    "- **numpy:** a general-purpose array-processing package designed to efficiently manipulate large multi-dimensional arrays of arbitrary records without sacrificing too much speed for small multi-dimensional arrays. \n",
    "- **string:** Contains a number of functions to process standard Python strings.\n",
    "- **nltk:** A leading platform for building Python programs to work with human language data.\n",
    "- **spacy:** A library for advanced Natural Language Processing in Python and Cython.\n",
    "\n",
    "Notice we import some of the packages differently. In some cases we just import the entire package when we say `import XYZ`. For some packages which are small, or, from which we are going to use a lot of the functionality it provides, this is fine. \n",
    "\n",
    "Sometimes when we import the package directly we say `import XYZ as X`. All this does is allow us to type `X` instead of `XYZ` when we use certain functions from the package. So we can now say `X.function()` instead of `XYZ.function()`. This saves time typing and eliminates errors from having to type out longer package names. I could just as easily type `import XYZ as potato` and whenever I use a function from the `XYZ` package I would need to type `potato.function()`. What we import the package as is up to you, but some commonly used packages have abbreviations that are standard amongst Python users such as `import pandas as pd` or `import matplotlib.pyplot as plt`. You do not need to us `pd` or `plt`, however, these are widely used and using something else could confuse other users and is generally considered bad practice. \n",
    "\n",
    "Other times we import only specific elements or functions from a package. This is common with packages that are very large and provide a lot of functionality, but from which we are only using a couple functions or a specific subset of the package that contains the functionality we need. This is seen when we say `from XYZ import ABC`. This is saying I only want the `ABC` function from the `XYZ` package. Sometimes we need to point to the specific location where a function is located within the package. We do this by adding periods in between the directory names, so it would look like `from XYZ.123.A1B2 import LMN`. This says we want the `LMN` function which is located in the `XYZ` package and then the `123` and `A1B2` directory in that package. \n",
    "\n",
    "You can also import more than one function from a package by separating the functions with commas like this `from XYZ import ABC, LMN, QRS`. This imports the `ABC`, `LMN` and `QRS` functions from the `XYZ` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will ignore deprecation and future warnings. All the warnings in this code are not concerning and will not break the code or cause errors in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Suppress warnings from pandas library\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
    "                        module=\"pandas\", lineno=570)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
    "                        module = \"sklearn\", lineno = 1059)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning,\n",
    "                        module = \"sklearn\", lineno = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File paths\n",
    "Here we are saving as variables different file paths that we need in our code. We do this so that they are easier to call later and so that you can make most of your changes now and not need to make as many changes later. \n",
    "\n",
    "First we use the `os` package above to find our `[\"HOME\"]` directory using the `environ` function. This will work for any operating system, so if you decide to try this out on your personal computer instead of ReD, the `homePath` variable will still be the path to your 'home' directory, so no changes are needed.\n",
    "\n",
    "Next, we combine the `homePath` variable with the folder names that lead to where our data is stored. Note that we do not use any file names yet, just the path to the folder. This is because we are comparing documents to one another, so we need to read in an entire directory. You will want to change the folder names to match your folder names in your file path.\n",
    "\n",
    "Now we add the `homePath` variable to other folder names that lead to a folder where we will want to save our document similarity matrix. You again will want to change the folder names in the path to match your own folder names. We save this file path as the variable `cleanedData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "homePath = os.environ[\"HOME\"]\n",
    "dataHome = os.path.join(homePath, \"Text-Analysis-master\", \"data\", \"shakespeareDated\")\n",
    "cleanedData = os.path.join(homePath, \"Text-Analysis-master\", \"TopicModeling\", \"LSA\", \"cleanedData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set needed variables\n",
    "\n",
    "Now we assign values to variables that will inform various parts of our code. Just like the file path variables, this is done so you have to make fewer changes later and also to make the changes easier to find by putting them in one place.\n",
    "\n",
    "- **nltkStop:** If you want to use the stopword list that comes with the nltk package then set `nltkStop` equal to `True`. If you do not wish to use the nltk stopword list then set `nltkStop` equal to `False`.\n",
    "\n",
    "- **customStop:** If you have created your own custom stopword list and wish to use that, then set `customStop` equal to `True`. If you do not have your own custom stopword list then set `customStop` equal to `False`.\n",
    "\n",
    "**NOTE: You can use both the nltk and custom stopword lists or you can use neither or just one or the other. You do NOT need to set them both to True or both to False. Use whatever works best for you.**\n",
    "\n",
    "- **lem:** Next we decide if we want to lemmatize our words. Lemmatizing words will turn certain words to the root of the word. So \"are\" and \"is\" become \"be\" and \"runs\" and \"running\" become \"run\". This will probably increase the similarity of documents as they will then share more words in common. If you want to lemmatize the words in your dataset then assign `True` to the variable `lem`. If you do not wish to lemmatize your words then assign `False` to the variable `lem`.\n",
    "\n",
    "- **lowerCase:** Then we decide if we want all the words in our dataset lowercased. This will change \"Love\" to \"love\" so that it is recognized as the same word for similarity purposes. However, there are some cases where the use of capitalization may be important to determining similarity, so we have the option to lowercase or not. If you want to lowercase all the words in your dataset assign `True` to the variable `lower`. If you do not wish to lowercase all the words in your dataset then assign `False` to the variable `lower`.\n",
    "\n",
    "- **removeDigits:** Now we decide if we want to remove numbers from out text. Again, removing numbers will increase the similarity of texts as page numbers and other integers that may not be exactly alike will be removed. However, there are instances where numbers are thematically important, and they need to be kept. Here is where you make that decision. If you wish to remove all numbers then assign `True` to the `removeDigits` variable. If you wish to retain all numbers then assign `False` to the `removeDigits` variable.\n",
    "\n",
    "- **language:** Now we choose the language we will be using for the nltk stopwords list. If you need a different language, simply change 'english' (keep the quotes) in the `language` variable to the anglicized name of the language you wish to use (e.g. 'spanish' instead of 'espanol' or 'german' instead of 'deutsch').\n",
    "\n",
    "- **lemLang:** Now we choose the language for our lemmatizer. The languages available for spacy include the list below and the abbreviation spacy uses for that language. To choose a language simply type the two letter code following the angliscized language name in the list. So for Spanish it would be `'es'` (with the quotes) and for German `'de'` and so on.\n",
    "\n",
    "- **English:** `'en'`\n",
    "- **Spanish:** `'es'`\n",
    "- **German:** `'de'`\n",
    "- **French:** `'fr'`\n",
    "- **Italian:** `'it'`\n",
    "- **Portuguese:** `'pt'`\n",
    "- **Dutch:** `'nl'`\n",
    "- **Multi-Language:** `'xx'`\n",
    "\n",
    "- **encoding/errors:** The variable `encoding` is where you determine what type of encoding to use (ascii, ISO-8850-1, utf-8, etc...). We have it set to utf-8 at the moment as we have found it is less likely to have any problems. However, errors do occur, but the encoding errors rarely impact our results and it causes the Python code to exit. So instead of dealing with unhelpful errors we ignore the ones dealing with encoding by assigning `'ignore'` to the `errors` variable. If you want to see any encoding errors then change `'ignore'` to `None` without the quotes.\n",
    "\n",
    "- **singleDocs:** If your data exists as a single file for each document and is in one directory, then assign `True` to the `singleDocs` variable. If each of your \"documents\" is actually multiple directories of multiple files and each directory needs to be concsiderd as a separate \"document\", then assign `False` to the `singleDocs` variable.\n",
    "\n",
    "- **nComp:** The LSA algorithm used by the sklearn Python package has a required parameter called `n_components` for \"number of components\" (the purpose of this parameter will be explained later). There is a part of the code further down in this notebook that will help determine what the ideal number of components is for your corpus to produce the most accurate result. This will also make the process take longer as it is an added complicated step. If you want to perform this part of the code and try to determine the best number of components, then assigne `True` to the `nComp` variable. If you do not wish to perform this added step then assign `False` to the `nComp` variable. The default number of components will be the number of \"documents\" in your corpus if you assign `False` to `nComp`.\n",
    "\n",
    "- **stopWords = []:** The `stopWords = []` variable is simply an empty list. This is where the words from the nltk stopword list or your custom stopword list or both combined or neither (depending on what you decide) will reside later on. You do not need to do anything to this line of code.\n",
    "\n",
    "- **tokenDict = {}:** The `tokenDict = {}` variable is an empty dictionary. This is where your documents will reside later. The file or folder name (depending on your choices above) for the document will be the key and the content of the document will be the value. This will be explained in more detail later. For now, you do not need to do anything to this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltkStop = True\n",
    "customStop = True\n",
    "lem = True\n",
    "lowerCase = True\n",
    "removeDigits = True\n",
    "language = 'english'\n",
    "lemLang = \"en\"\n",
    "encoding = 'utf-8'\n",
    "errors = 'ignore'\n",
    "singleDocs = True\n",
    "nComp = True\n",
    "stopWords = []\n",
    "tokenDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "If you set `nltkStop` equal to **True** above then this will add the NLTK stopwords list to the empty list named `stopWords`.\n",
    "\n",
    "You should have already chosen your desired language above, but if you wish to add any words to the stopWords list then add the word(s) you want as a stop word in the `stopWords.extend(['words', 'you', 'want', 'to', 'add'])` part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if nltkStop is True:\n",
    "    # NLTK Stop words\n",
    "    stopWords = stopwords.words(language)\n",
    "\n",
    "    stopWords.extend(['would', 'said', 'says', 'also'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add own stopword list\n",
    "\n",
    "Here is where your own stopwords list is added if you set `customStop` equal to **True** above. Here you will need to change the folder names and file name to match your folders and file. Remember to put each folder name in quotes and in the correct order always putting the file name including the file extension (.txt) last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if customStop is True:\n",
    "    stopWordsFilepath = os.path.join(homePath, \"Text-Analysis-master\", \"data\", \"earlyModernStopword.txt\")\n",
    "\n",
    "    with open(stopWordsFilepath, \"r\",encoding = encoding, errors = errors) as f:\n",
    "        stopWordsList = [x.strip() for x in f.readlines()]\n",
    "\n",
    "    stopWords.extend(stopWordsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "We need to create a function in order to stem and tokenize our data. Any time you see `def` that means we are **DE**claring a **F**unction. The `def` is usually followed by the name of the function being created and then in parentheses are the parameters required by the function. After the parentheses is a colon, which closes the declaration, then a bunch of code below which is indented. The indented code is the program statement or statements to be executed. Once you have created your function all you need to do in order to run it is call the function by name and make sure you have included all the required parameters in the parentheses. This allows you to call the function without having to write out all the code in the function every time you wish to perform that task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization functions\n",
    "Here we have several \"if...else\" statements. If we set `lem` to `True*` above we create two functions. One to ignore the \"\\n\" markers in our text and the next to tokenize and lemmatize our text. Both of these functions require the language dictionary being used to lemmatize the text. Therefore, within the `if lem is True:` statement, we have several \"if...else\" statements to specify which language you assigned to the `lemLang` variable above. \n",
    "\n",
    "However, if we assigned `False` to `lem` above then we only create one function that tokenizes our text, no need to specify a language.\n",
    "\n",
    "You should not need to make any changes to this block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if lem is True:\n",
    "    nlp = spacy.load(lemLang, diasble=[\"parser\",\"ner\"])\n",
    "    nlp.max_length=2500000\n",
    "    def tokenFilter(token):\n",
    "        return not (token.is_space)\n",
    "    \n",
    "    def tokenize(text):\n",
    "        for doc in nlp.pipe([text]):\n",
    "            tokens = [token.lemma_ for token in doc if tokenFilter(token)]\n",
    "        return tokens\n",
    "\n",
    "else:\n",
    "    def tokenize(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in documents\n",
    "Now we read in our documents and also perform some text cleaning. This code lower cases all the words as well as removes punctuation and digits, depending what you assigned for the `lowerCase` and `removeDigits` variables above. Then it adds the file names and cleaned content of each file to our previously empty `tokenDict` dictionary above. You should not need to make any changes to this code.\n",
    "\n",
    "A dictionary is similar to a list except it has what are called 'keys' and 'values'. This basically allows us to label our data. In this case we will be making the file or folder names of our documents the 'keys' and the content of the file(s) the 'values' so that each document name correlates to the content of that document.\n",
    "\n",
    "The `if singleDocs is True:` statement says that if we assigned `True` to the `singleDocs` variable above, then the code below the statement will be run, which reads in files as if each each file is a single document. If we did NOT assign `True` to `singleDocs` (`else`), then the code below `else` will be run instead, which reads in the files for each directory and treats each directory as a single volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if singleDocs is True:\n",
    "    for subdir, dirs, files in os.walk(dataHome):\n",
    "        for file in files:\n",
    "            if file.startswith('.'):\n",
    "                    continue\n",
    "            filePath = subdir + os.path.sep + file\n",
    "            with open(filePath, 'r', encoding = encoding, errors = errors) as textFile:\n",
    "                text = textFile.read()\n",
    "                if lowerCase and removeDigits is True:\n",
    "                    lowers = text.lower()\n",
    "                    noPunctuation = lowers.translate(str.maketrans('','', string.punctuation))\n",
    "                    noDigits = noPunctuation.translate(str.maketrans('','', string.digits))\n",
    "                    tokenDict[file] = noDigits\n",
    "                elif lowerCase == True and removeDigits == False:\n",
    "                    lowers = text.lower()\n",
    "                    noPunctuation = lowers.translate(str.maketrans('','', string.punctuation))\n",
    "                    tokenDict[file] = noPunctuation\n",
    "                elif lowerCase == False and removeDigits == True:\n",
    "                    noPunctuation = text.translate(str.maketrans('','', string.punctuation))\n",
    "                    noDigits = noPunctuation.translate(str.maketrans('','', string.digits))\n",
    "                    tokenDict[file] = noDigits\n",
    "                else:\n",
    "                    noPunctuation = text.translate(str.maketrans('','', string.punctuation))\n",
    "                    tokenDict[file] = noPunctuation\n",
    "else:\n",
    "    data = []\n",
    "    text = []\n",
    "    for folder in sorted(os.listdir(dataHome)):\n",
    "        if not os.path.isdir(os.path.join(dataHome, folder)):\n",
    "            continue\n",
    "        for file in sorted(os.listdir(os.path.join(dataHome, folder))):\n",
    "            data.append(((dataHome,folder,file)))\n",
    "    df = pd.DataFrame(data, columns = [\"Root\", \"Folder\", \"File\"])\n",
    "    df[\"Paths\"] = df[\"Root\"].astype(str) + \"/\" + df[\"Folder\"].astype(str) + \"/\" + df[\"File\"].astype(str)\n",
    "    for path in df[\"Paths\"]:\n",
    "        if not path.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(path, \"r\", encoding=encoding, errors = errors) as f:\n",
    "            t = f.read().strip().split()\n",
    "            if lowerCase and removeDigits is True:\n",
    "                lowers = ' '.join(t).lower()\n",
    "                noPunctuation = lowers.translate(str.maketrans('','', string.punctuation))\n",
    "                noDigits = noPunctuation.translate(str.maketrans('','', string.digits))\n",
    "                text.append(noDigits)\n",
    "            elif lowerCase == True and removeDigits == False:\n",
    "                lowers = ' '.join(t).lower()\n",
    "                noPunctuation = lowers.translate(str.maketrans('','', string.punctuation))\n",
    "                text.append(noPunctuation)\n",
    "            elif lowerCase == False and removeDigits == True:\n",
    "                noPunctuation = ' '.join(t).translate(str.maketrans('','', string.punctuation))\n",
    "                noDigits = noPunctuation.translate(str.maketrans('','', string.digits))\n",
    "                text.append(noDigits)\n",
    "            else:\n",
    "                noPunctuation = text.translate(str.maketrans('','', string.punctuation))\n",
    "                text.append(noPunctuation)\n",
    "    df[\"Text\"] = pd.Series(text)\n",
    "    df[\"Text\"] = [\"\".join(map(str, l)) for l in df[\"Text\"].astype(str)]\n",
    "    d = {'Text':'merge'}\n",
    "    dfText = df.groupby([\"Folder\"])[\"Text\"].apply(lambda x: ' '.join(x)).reset_index()\n",
    "    \n",
    "    tokenDict = dict(zip(dfText[\"Folder\"], dfText[\"Text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check and see if our dictionary now has our data. We are asking to see the first 10 keys of our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1610Cymbeline.txt', '1596MerchantOfVenice.txt', '1604AllsWellThatEndsWell.txt', '1591KingHenry6_3.txt', '1599JuliusCaesar.txt', '1600TroilusAndCressida.txt', '1605TimonOfAthens.txt', '1592KingRichard3.txt', '1591KingHenry6_2.txt', '1603MeasureForMeasure.txt']\n"
     ]
    }
   ],
   "source": [
    "print(list(tokenDict.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf Vectorizer\n",
    "\n",
    "Here we weight the importance of each word in the document. This is done using Term Frequency-Inverse Document Frequency (Tfidf). This considers how important a word is based on the frequency in the whole corpus as well as in individual documents. This allows for words that might not have a high frequency in an entire collection, but do have a high frequency in one or two documents when compared to other words to still be given a higher level of importance throughout the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = tokenize, stop_words = stopWords)\n",
    "dtm = vectorizer.fit_transform(tokenDict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code outputs the first 10 words that make up the columns once we have broken our corpus down into a Tfidf matrix. To output all of the words remove the square brackets and their contents in the `print(vectGFN[:20])` line of code. To change the number of words printed change `20` in the same line to the number of words you wish to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'aaron', 'abandon', 'abase', 'abash', 'abate', 'abated', 'abatement', 'abatfowling', 'abbess', 'abbey', 'abbot', 'abbreviate', 'abc', 'abe', 'abed', 'abel', 'abergavenny', 'abet', 'abhor']\n"
     ]
    }
   ],
   "source": [
    "# Get words that correspond to each column\n",
    "vectGFN = vectorizer.get_feature_names()\n",
    "print(vectGFN[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assigned `True` to the `nComp` variable above, this cell will create a function that determines the number of components that first gives us an explained variance ratio of our choice (currently 0.95 in code cell directly below this one). The math is complicated, but we are reducing our tfidf matrix to a more manageable size, but we need to know how many components we can reduce our matrix to and still keep a certain percentage of our data (again, currently set to 0.95 or 95% of our data in the code cell after this one). If we assigned `False` to the `nComp` variable, this cell will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if nComp is True:\n",
    "    tsvd = TruncatedSVD(n_components=dtm.shape[1]-1)\n",
    "    tsvd.fit(dtm)\n",
    "    tsvdVarRatios = tsvd.explained_variance_ratio_\n",
    "\n",
    "    def selectNcomponents(var_ratio, goal_var: float) -> int:\n",
    "        total_variance = 0.0\n",
    "        n_components = 0\n",
    "        for explained_variance in var_ratio:\n",
    "            total_variance += explained_variance\n",
    "            n_components += 1\n",
    "            if total_variance >= goal_var:\n",
    "                break\n",
    "        return n_components\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assigned `True` to the `nComp` variable we will now apply the function we created in the cell above. This will now incremently increase the number of components we will reduce our matrix to until we get a 95% or higher variance ratio, meaning we are still keeping 95% or more of our data. If we assigned `False` to `nComp` then this cell will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "if nComp is True:\n",
    "    nc = selectNcomponents(tsvdVarRatios, 0.95)\n",
    "    print(nc)\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run SVD and Cosine Similarity\n",
    "\n",
    "Here we run our Tfidf matrix created above through Singular Value Decomposition and then calculate the Cosine Similarity of the documents to one another. \n",
    "\n",
    "Singular Value Decomposition condenses our Tfidf matrix down a bit to make it easier to process. Here we also set the number of dimensions (`n_components`) , how many times we iterate over the corpus (`n_iter`), and then set the seed (`random_state`) so that the results are reproducable. At the moment the `random_state` is set to 42 which sets the seed for the random number generator, but feel free to adjust the number to get a slightly different output. Just make sure you keep the seed the same once you find one you like for reproducibility.\n",
    "\n",
    "Cosine similarity is where we measure how similar the documents are to one another. The result is a number between -1 and 1 with 1 being a perfect match (which we will get when the document is compared to itself) and -1 being completely different which we might get if we have a document of all numbers and one of all words with no numbers at all. Usually, even documents that are about unrelated topics share some common words and so are not completely dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if nComp is True:\n",
    "    lsa = TruncatedSVD(n_components = nc, n_iter = 1000, random_state = 42)\n",
    "    dtmLsa = lsa.fit_transform(dtm)\n",
    "    cosineSim = cosine_similarity(dtmLsa)\n",
    "else:\n",
    "    lsa = TruncatedSVD(n_components = dtm.shape[0], n_iter = 1000, random_state = 42)\n",
    "    dtmLsa = lsa.fit_transform(dtm)\n",
    "    cosineSim = cosine_similarity(dtmLsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save as .csv\n",
    "\n",
    "Now we save the results as a .csv file. First we name the output .csv file so it matches our data. We do this in the first line of the cell.\n",
    "\n",
    "Then we create the data frame and say we want the rows and columns to be labeled with the file names. Then we sort the columns in alphanumeric order by column header, then we sort the rows alphanumericaly by row label.\n",
    "\n",
    "Finally, we export the dataframe as a .csv file.\n",
    "\n",
    "You can manipulate the .csv file in excel or some other spreadsheet software, or you can use it in the \"docSimLSAHeatmap\" notebook that is part of this text analysis repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvFileName = \"docSimilarityMatrix.csv\"\n",
    "\n",
    "df = pd.DataFrame(cosineSim, index = tokenDict.keys(), columns=tokenDict.keys())\n",
    "dfS = df[sorted(df)]\n",
    "sortedDf = dfS.sort_index(axis = 0)\n",
    "np.fill_diagonal(sortedDf.values, np.nan)\n",
    "sortedDf.to_csv(os.path.join(cleanedData, csvFileName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was adapted from https://www.datascienceassn.org/sites/default/files/users/user1/lsa_presentation_final.pdf at Colorado University, Boulder. Accessed on 02/01/2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
